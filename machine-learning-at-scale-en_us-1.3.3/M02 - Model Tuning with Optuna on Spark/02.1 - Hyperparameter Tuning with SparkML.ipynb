{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f585c8-f270-4ce8-ad1f-64b61bb1c924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3899206a-2076-4263-a1d1-eb04a4ebb3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo: Hyperparameter Tuning with SparkML\n",
    "\n",
    "In this demo, you will learn how to use **Optuna**, a powerful hyperparameter optimization (HPO) framework, to tune machine learning models in Databricks utilizing **Spark MLlib**.\n",
    "\n",
    "We will demonstrate how to implement **Optuna**  using a **Random Forest Regressor** from SparkML, covering:\n",
    "- **Defining search spaces** for HPO.\n",
    "- **Creating objective functions** tailored to the framework.\n",
    "- **Optimizing hyperparameters** using two execution strategies:\n",
    "  - **Single-node multithreading** for local tuning.\n",
    "  - **Distributed Spark execution** for large-scale training.\n",
    "\n",
    "Additionally, we will track and log the results using **MLflow**, enabling efficient management and monitoring of the tuning process.\n",
    "\n",
    "### **Distributed Machine Learning in Databricks**\n",
    "Distributing the workload for Hyperparameter tuning with Spark can be broken down into two key components:\n",
    "\n",
    "1. **Model Training Level:**  \n",
    "   - Utilize **PySpark DataFrames** for distributed data processing.\n",
    "   - Leverage **Spark ML algorithms**, which are inherently scalable.\n",
    "\n",
    "2. **Optimization Level:**  \n",
    "   - Use **driver-based orchestration frameworks** (e.g., Optuna) to manage hyperparameter tuning while leveraging **Spark MLlib** for distributed model training.\n",
    "   - While model training runs in parallel across the Spark cluster, **hyperparameter tuning is orchestrated on a single machine** (the driver node). Optuna can execute multiple trials **in parallel using threads**, but this still occurs within the driver and is not distributed across the cluster.\n",
    "\n",
    "\n",
    "### **ðŸš¨A Warning Concerning HyperOpt on Databricks**\n",
    "The open-source version of Hyperopt is no longer being maintained and will be removed in the DBR ML versions 17.0+. *This notebook is currently running on a version that supports Hyperopt.* Databricks recommends using Optuna for single-node optimization or RayTune for a similar experience.\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this demo, you will be able to:\n",
    "\n",
    "- **Understand the Hyperparameter Tuning Approach**\n",
    "   - **Optuna** for single-node orchestration of hyperparameter tuning with parallel execution across threads.\n",
    "   - **Spark MLlib** for distributed model training, combined with driver-based hyperparameter orchestration (e.g., via Optuna).\n",
    "\n",
    "- **Perform Hyperparameter Tuning using Optuna**\n",
    "   - Define an **objective function** tailored to your model.\n",
    "   - Configure **a search space** for hyperparameter optimization.\n",
    "   - Optimize hyperparameters using **single-node execution**.\n",
    "\n",
    "- **Understand Hyperoptâ€™s Usage with Spark MLlib (Optionally)**\n",
    "   - Learn how Hyperopt can be used for **sequential Bayesian optimization** with Spark MLlib.\n",
    "   - Identify the **limitations and trade-offs** of using Hyperopt in distributed environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a95882-29f2-4d8a-bebb-32181011cd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15aba840-b6ea-4a9c-9f00-c91ae0ee13f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0fe6d48-bc71-4ff1-8936-f61f3cb43309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U optuna optuna-integration mlflow\n",
    "%pip install --upgrade ray[tune]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c84c430-1321-44b4-81a0-e9d8994f031c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a0a985-1d46-468e-9997-f2dc316575a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937e9115-91c4-4048-b3df-c9ef4b2e7afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3330fb-defe-4eb3-9a14-7ddfb026560d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.wine_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d688800-f96d-45ee-954d-8d67f9e2bfe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data and Perform Train-Test Split\n",
    "\n",
    "In this step, we will load the dataset from the **Delta table** `wine_quality_features`, which is stored in **Unity Catalog** under:  \n",
    "`{DA.catalog_name}.{DA.schema_name}.wine_quality_features`\n",
    "\n",
    "### **Instructions:**\n",
    "1. **Load the dataset** from the Delta table using `spark.read.table()`.\n",
    "2. **Split the dataset** into **training (80%) and testing (20%)** sets to evaluate the model's performance.\n",
    "   - Since we are using **PySpark DataFrames**, we will use `.randomSplit()` for the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1392a74-aa50-45f5-8d9f-9b586c772d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").table(f\"{DA.catalog_name}.{DA.schema_name}.wine_quality_features\")\n",
    "# Split the dataset into training and test sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d424a4-533f-454d-bd5a-71261619ef56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eb6245-6e15-4966-b0fd-2253df5e3a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: HPO with Optuna and Distributed Training of a Spark Model\n",
    "In this part, we will use **Optuna for hyperparameter optimization** while training a Spark ML model in **parallel**.\n",
    "\n",
    "### How This Works:\n",
    "- **Optuna runs on a single machine** to manage hyperparameter tuning, where it suggests configurations for each trial and records their performance.\n",
    "- **Model training can be distributed**, ensuring the ability to scale out and speed up for large datasets and complex models.\n",
    "- **Each Optuna trial runs a new model training job** on the Spark cluster, allowing it to evaluate different hyperparameter configurations efficiently.\n",
    "\n",
    "This approach allows us to leverage **distributed computing for training** while keeping **hyperparameter optimization lightweight and efficient** on a single node across multiple threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243402b2-9343-42b0-834c-992044ccce5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the Objective Function for Optuna\n",
    "The first step will be to define the **objective function** for Optuna. This is the function that Optuna will minimize by optimizing hyperparameters like the number of trees (`numTrees`) and the depth of the tree (`maxDepth`). In our case, the objective function is the `Root Mean Squared Error` (RMSE) since our model is a random forest regressor. However, we will use a distributed training approach within this function by running the training on Spark workers.\n",
    "\n",
    "**Instructions:**\n",
    "- **Initialize TPESampler Configuration**. In this example we will use Bayesian optimization along with a Gaussian prior to help stabilize the Parzen estimator (known as the Tree-structured Parzen Estimator algorithm).  \n",
    "- **Initialize hyperparameters** using Optuna's `trial.suggest_int()` function. This function samples integers between `low` and `high` for the hyperparameter `<hyperparameter_name>` when calling `trial.suggest_int('<hyperparameter_name>', low, high)`. \n",
    "- **Train the model** using **Spark's distributed cluster** by running the `RandomForestRegressor` model on Spark workers.\n",
    "- **Evaluate the model** using the **RMSE** metric (`rmse`), and return it as the value to *minimize* during the optimization. Note, we will tell Optuna to minimize the returned **RMSE** value when we create an Optuna study later. This happens outside the definition of the objective function. \n",
    "\n",
    "Refer to the documentation for:\n",
    "* [optuna.samplers](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html) for the choice of samplers\n",
    "* [optuna.trial.Trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html) for a full list of functions supported to define a hyperparameter search space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffa6623-1763-4401-8dae-a82077d1408c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "optuna_sampler = optuna.samplers.TPESampler(\n",
    "  consider_prior=True, #Enhance the stability of Parzen estimator by imposing a Gaussian prior when True\n",
    "  n_startup_trials=3, #The random sampling is used instead of the TPE algorithm until the given number of trials finish in the same study.\n",
    "  seed=123 # Seed for random number generator.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dc6764-6830-4d6e-8c51-345b61772c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "class ObjectiveOptuna:\n",
    "    \"\"\"\n",
    "    Objective function class for Optuna hyperparameter tuning with SparkML models.\n",
    "    \n",
    "    Instead of loading the dataset in each trial execution, this class receives \n",
    "    the training and test datasets during initialization, improving efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_df, test_df, label_column=\"label\"):\n",
    "        \"\"\"\n",
    "        Initializes the objective function with training and test datasets.\n",
    "\n",
    "        Args:\n",
    "            train_df (DataFrame): Spark DataFrame containing features and label for training.\n",
    "            test_df (DataFrame): Spark DataFrame containing features and label for evaluation.\n",
    "            label_column (str): Name of the label column in the dataset. Default is \"label\".\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.label_column = label_column\n",
    "    \n",
    "    def objective_sparkmodel_distributed_Optuna(self, trial):\n",
    "        \"\"\"\n",
    "        Optuna objective function for tuning regression models using SparkML. Possible models are: Linear Regression, Random Forest, and Gradient-Boosted Trees.\n",
    "\n",
    "        Args:\n",
    "            trial (optuna.trial.Trial): An Optuna trial object to suggest hyperparameters.\n",
    "\n",
    "        Returns:\n",
    "            float: Root Mean Squared Error (RMSE) to minimize.\n",
    "        \"\"\"\n",
    "\n",
    "        # Select model type\n",
    "        model_name = trial.suggest_categorical(\"model\", [\"LinearRegression\", \"RandomForest\", \"GBTRegressor\"])\n",
    "\n",
    "        if model_name == \"LinearRegression\":\n",
    "            # Hyperparameter tuning for Linear Regression\n",
    "            model = LinearRegression(\n",
    "                featuresCol=\"features\",\n",
    "                labelCol=self.label_column,\n",
    "                regParam=trial.suggest_float(\"reg_param\", 0.0, 1.0),\n",
    "                elasticNetParam=trial.suggest_float(\"elastic_net_param\", 0.0, 1.0)\n",
    "            )\n",
    "\n",
    "        elif model_name == \"RandomForest\":\n",
    "            # Hyperparameter tuning for Random Forest\n",
    "            model = RandomForestRegressor(\n",
    "                featuresCol=\"features\",\n",
    "                labelCol=self.label_column,\n",
    "                numTrees=trial.suggest_int(\"num_trees\", 2, 5, log=True),\n",
    "                maxDepth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                minInstancesPerNode=trial.suggest_int(\"min_instances_per_node\", 1, 10)\n",
    "            )\n",
    "\n",
    "        elif model_name == \"GBTRegressor\":\n",
    "            # Hyperparameter tuning for Gradient-Boosted Trees\n",
    "            model = GBTRegressor(\n",
    "                featuresCol=\"features\",\n",
    "                labelCol=self.label_column,\n",
    "                maxDepth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                maxIter=trial.suggest_int(\"n_estimators\", 2, 5, log=True),\n",
    "                stepSize=trial.suggest_float(\"learning_rate\", 0.01, 0.5)\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        trained_model = model.fit(self.train_df)\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = trained_model.transform(self.test_df)\n",
    "\n",
    "        # Evaluate performance using RMSE\n",
    "        rmse = RegressionEvaluator(\n",
    "            labelCol=self.label_column,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"rmse\"\n",
    "        ).evaluate(predictions)\n",
    "\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8160d177-dba2-4ebc-9c09-8adab8f3c000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimize The Spark ML model on Single-Machine Optuna and Log Results with MLflow\n",
    "In this step, we will utilize `MLflow` to track the optimization process by adding out-of-the box logging provided by Optuna trials using `MLflowCallBack()`. Once we have our logging parameters configured, there are two additional steps to take care of before moving onto the run. \n",
    "\n",
    "1. Initialize Optuna's `optuna.create_study()`. A *study* is corresponds to the optimization task, which is a set of trials and a trial is a process of evaluating an *objective function*.\n",
    "1. Tell Optuna how we want to optimize with `optimize()`. \n",
    "\n",
    "Each trial will be logged to MLflow, including the hyperparameters tested and the corresponding `RMSE` values. Optuna will handle the optimization, while training continues to be distributed across Spark workers.\n",
    "\n",
    "**Instructions:**\n",
    "- **Set up MLflow** to track the experiments using `MLflowCallBack()`.\n",
    "- **Define the storage location** with the variable `storage_url`. In this demonstration, we will be using the driver node to persist our study information, allowing for distributed optimization. \n",
    "- **Setup an Optuna study** with `optuna.study()`. \n",
    "- **Optimize hyperparameters** using Optuna's `study.optimize()` method.\n",
    "- **Log results to MLflow**, including the best hyperparameters and RMSE.\n",
    "- **End the MLflow run** to ensure that all information is saved.\n",
    "\n",
    "*Note on parallelization: The value of `n_jobs` within the `optimization()` function is the number of parallel jobs. If this argument is set to -1 (as we have done below), then the number of parallel jobs is set to the number of CPU cores (the default value for this demonstration is 4 cores).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b145de-5698-4d5a-af11-d707faba0c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "experiment_name_spark = os.path.join(\n",
    "    os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()),\n",
    "    \"02a - Model Tuning with Optuna_spark\"\n",
    ")\n",
    "mlflow.set_experiment(experiment_name_spark)\n",
    "experiment_id_spark = mlflow.get_experiment_by_name(experiment_name_spark).experiment_id\n",
    "\n",
    "def optuna_hpo_fn(n_trials: int, experiment_id: str, optuna_sampler) -> optuna.study.Study:\n",
    "    \"\"\"\n",
    "    Runs hyperparameter optimization using Optuna with MLflow logging.\n",
    "\n",
    "    Args:\n",
    "        n_trials (int): Number of trials for optimization.\n",
    "        experiment_id (str): MLflow experiment ID for logging.\n",
    "        optuna_sampler (optuna.samplers.BaseSampler): Optuna sampler for search strategy.\n",
    "\n",
    "    Returns:\n",
    "        optuna.study.Study: The Optuna study object with optimization results.\n",
    "    \"\"\"\n",
    "\n",
    "    # MLflow callback to log results\n",
    "    mlflow_callback_spark = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        metric_name=\"RMSE\",\n",
    "        create_experiment=False,\n",
    "        mlflow_kwargs={\"experiment_id\": experiment_id}\n",
    "    )\n",
    "\n",
    "    # Define the objective function\n",
    "    objective_function = ObjectiveOptuna(train_df, test_df, label_column=\"quality\").objective_sparkmodel_distributed_Optuna\n",
    "\n",
    "    # Create or load an Optuna study\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"sparkmodel_optuna_distributed_hpo\",\n",
    "        sampler=optuna_sampler,\n",
    "        load_if_exists=True,\n",
    "        direction=\"minimize\"\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective_function,\n",
    "        n_trials=n_trials,\n",
    "        n_jobs=-1,  # Parallel execution\n",
    "        callbacks=[mlflow_callback_spark]\n",
    "    )\n",
    "\n",
    "    # Extract best trial results\n",
    "    best_trial = study.best_trial\n",
    "    best_rmse = best_trial.value  # RMSE metric\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Best Trial Number: {best_trial.number}\")\n",
    "    print(f\"Best Hyperparameters: {best_trial.params}\")\n",
    "    print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "    # Log the best results manually in MLflow\n",
    "    with mlflow.start_run(run_name=\"best_trial_results\"):\n",
    "        mlflow.log_params(best_trial.params)\n",
    "        mlflow.log_metric(\"Best RMSE\", best_rmse)\n",
    "\n",
    "    return study  # Return study for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97522699-5609-4a8c-9131-f1a6768f4ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Execute the Single Node Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5e62e8-5ad0-49be-86ab-1888af4d452c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable MLflow autologging to prevent unwanted logging of model artifacts\n",
    "mlflow.autolog(log_models=False, disable=True)\n",
    "\n",
    "# Invoke Optuna training function on the driver node\n",
    "single_node_study = optuna_hpo_fn(\n",
    "    n_trials=10,\n",
    "    experiment_id=experiment_id_spark,\n",
    "    optuna_sampler=optuna_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d40464-ec61-4395-8887-942449157557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explanation: Distributing Hyperparameter Tuning and Model Training\n",
    "\n",
    "The previous cells implemented **distributed hyperparameter tuning and training** using **Optuna**, **MLflow**, and **Spark MLlib**. \n",
    "\n",
    "#### **Key Characteristics of This Setup**\n",
    "| Aspect | Current Implementation |\n",
    "|--------|------------------------|\n",
    "| **Hyperparameter Tuning** | Runs on a **single machine** (Optuna executes locally, even if multiple trials run in parallel). |\n",
    "| **Parallel Execution** | Trials are parallelized **within a single machine** and training happens in a **distributed fashion** across multiple threads. |\n",
    "| **Database Storage** | Uses **default in memory storage** for Optuna trials, limiting multi-machine and multi-process execution. |\n",
    "| **Experiment Logging** | MLflow logs hyperparameters and RMSE for each trial. |\n",
    "\n",
    "#### **How to Fully Distribute Hyperparameter Tuning**\n",
    "While this implementation already distributes model training, Optuna's default execution with `n_jobs` utilizes multithreading on a single node, which, due to Python's Global Interpreter Lock, allows for concurrency but not true parallelism in CPU-bound tasks. To achieve true parallelization, Optuna can be configured to use multiprocessing, either on a single node or across multiple nodes, by setting up an appropriate backend such as a relational database. To fully distribute the hyperparameter search across multiple machines:\n",
    "\n",
    "**Use a centralized database**:\n",
    "   - Within create_study include storage such as `storage=\"sqlite:////local_disk0/optuna_distributed_model.db\"` for Multi-processing parallelization with single node or client/server Relational Databases like PostgreSQL or MySQL **ex:** for Multi-processing parallelization with multiple nodes\n",
    "     ```\n",
    "     storage=\"mysql://root@localhost/example\"\n",
    "     ```\n",
    "   - This allows multiple workers to share and execute trials.\n",
    "   - **Requirement:** Launch a MySQL instance (can be on AWS RDS, Azure Database for MySQL, GCP Cloud SQL, or an on-prem server).  [See Optuna Documentation](https://optuna.readthedocs.io/en/latest/faq.html#how-can-i-parallelize-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd04bbc3-47b6-4a5a-b532-d61ce4dcf9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Approaches for HPO with SparkML on Databricks\n",
    "\n",
    "Hyperparameter tuning in a **Databricks environment** can be challenging due to **SparkContext limitations** and **process forking issues** in managed clusters. Below are **three recommended approaches** to effectively perform hyperparameter tuning while avoiding common pitfalls.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges with Hyperparameter Tuning in Spark and Python**\n",
    "1. **Serialization Issues**:  \n",
    "   - Passing **Spark objects** (e.g., Spark DataFrame, SparkSession, SparkContext) into a **distributed function** (Hyperopt or a Spark UDF) can cause failures due to **pickling restrictions**.\n",
    "\n",
    "2. **Single SparkContext Per Notebook**:  \n",
    "   - Databricks runs a **single Spark driver** (the notebook environment) with one **Spark session**.  \n",
    "   - Workers **cannot** create new Spark sessions (`SparkSession.getOrCreate()`) without **proper master settings**.\n",
    "\n",
    "3. **Rayâ€™s Process Forking Issue**:  \n",
    "   - Even in **local mode**, Ray spawns separate processes **per trial**.  \n",
    "   - These processes **do not inherit** the Spark master URL or Spark session.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Recommended Approaches**\n",
    "\n",
    "### **Option 1: Use Sparkâ€™s Built-in Hyperparameter Tuning Tools**\n",
    "#### Best for: **Native Spark ML hyperparameter tuning**\n",
    "- **How it Works**:  \n",
    "  - Leverage **Spark MLâ€™s** `CrossValidator` or `TrainValidationSplit` to perform distributed hyperparameter tuning.  \n",
    "  - Spark handles **parallelism natively**.\n",
    "\n",
    "- **Pros**:\n",
    "  - Fully **compatible** with Databricks.  \n",
    "  - Runs in **distributed mode**, leveraging Spark Executors.  \n",
    "  - Avoids **SparkContext serialization issues**.  \n",
    "\n",
    "- **Cons**:\n",
    "  - Limited to **grid search or random search** (without custom logic).  \n",
    "  - No advanced Bayesian Optimization (unless implemented manually).  \n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Using Hyperopt with Spark MLlib (Optional)\n",
    "\n",
    "Hyperopt can be used for hyperparameter tuning with **Spark MLlib models**, but it is subject to important limitations. This approach may be helpful in certain cases but is **not recommended** for scalable or parallel execution.\n",
    "\n",
    "#### What You Can Do\n",
    "- Use Hyperoptâ€™s `fmin()` function with the default `Trials` object (not `SparkTrials`).\n",
    "- Each hyperparameter trial runs **sequentially** on the **driver node**, which can then initiate **distributed training** using Spark MLlib.\n",
    "- This setup allows for **Bayesian Optimization**, offering a more efficient search strategy compared to random or grid search.\n",
    "\n",
    "#### What You Cannot Do\n",
    "- You **cannot** use `SparkTrials` with Spark MLlib. `SparkTrials` is intended for distributing trials across a Spark cluster for **single-node machine learning libraries** like scikit-learn.\n",
    "- Using `SparkTrials` with Spark MLlib will lead to SparkContext-related errors due to the incompatible execution model. For example:\n",
    "```\n",
    "PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER]\n",
    "Could not serialize object: SparkContext can only be used on the driver, not in code that runs on workers.\n",
    "```\n",
    "**References**\n",
    "- [Sample Notebook: Hyperopt with Spark MLlib](https://assets.docs.databricks.com/_extras/notebooks/source/hyperopt-spark-ml.html)\n",
    "- [Databricks Documentation: Hyperopt for Distributed ML](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/automl-hyperparam-tuning/hyperopt-distributed-ml?utm_source=chatgpt.com)\n",
    "- [Hyperopt Documentation: SparkTrials with scikit-learn](https://hyperopt.github.io/hyperopt/scaleout/spark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491bafbe-a100-4ba0-a13c-fb5311da8f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Option 1: Use Spark MLâ€™s Built-in Hyperparameter Tuning Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc72c91-d778-405c-b1e3-a45726c266d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "label_column = \"quality\"\n",
    "\n",
    "# MLflow Experiment Setup\n",
    "experiment_name_spark_cv = os.path.join(\n",
    "    os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()),\n",
    "    \"02c - Model Tuning with spark cv\"\n",
    ")\n",
    "mlflow.set_experiment(experiment_name_spark_cv)\n",
    "experiment_id_spark_cv = mlflow.get_experiment_by_name(experiment_name_spark_cv).experiment_id\n",
    "\n",
    "# Ensure feature vectorization\n",
    "if \"features\" not in train_df.columns:\n",
    "    feature_cols = [col for col in train_df.columns if col != label_column]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_df_transformed = assembler.transform(train_df).select(\"features\", label_column).na.drop()\n",
    "else:\n",
    "    feature_cols = [col for col in train_df.columns if col != label_column]\n",
    "    train_df_transformed = train_df\n",
    "\n",
    "# Define RandomForestRegressor and hyperparameter grid\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=label_column, seed=42)\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [5, 10, 20])    # Number of trees\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10])    # Max tree depth\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Set up CrossValidator\n",
    "evaluator = RegressionEvaluator(labelCol=label_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # 3-fold cross-validation\n",
    "    parallelism=4  # Parallel execution\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=\"spark_cv_rf\", experiment_id=experiment_id_spark_cv):\n",
    "    try:\n",
    "        # Start training\n",
    "        start_time = time.time()\n",
    "        cv_model = cv.fit(train_df_transformed)\n",
    "        training_duration = time.time() - start_time\n",
    "        mlflow.log_metric(\"training_duration_s\", training_duration)\n",
    "\n",
    "        # Retrieve best model and evaluate\n",
    "        best_model = cv_model.bestModel\n",
    "        train_predictions = best_model.transform(train_df_transformed)\n",
    "        train_rmse = evaluator.evaluate(train_predictions)\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "        print(f\"Best Model RMSE on training folds: {train_rmse:.4f}\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_predictions = best_model.transform(test_df)\n",
    "        test_rmse = evaluator.evaluate(test_predictions)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "        # Log best hyperparameters\n",
    "        best_num_trees = best_model.getNumTrees\n",
    "        best_max_depth = best_model.getOrDefault(\"maxDepth\")\n",
    "        mlflow.log_param(\"best_numTrees\", best_num_trees)\n",
    "        mlflow.log_param(\"best_maxDepth\", best_max_depth)\n",
    "\n",
    "        print(f\"Best hyperparameters â†’ numTrees={best_num_trees}, maxDepth={best_max_depth}\")\n",
    "\n",
    "        # Log feature importances\n",
    "        if hasattr(best_model, \"featureImportances\"):\n",
    "            importances = best_model.featureImportances\n",
    "            feat_imp_map = {col: val for col, val in zip(feature_cols, importances.toArray())}\n",
    "            mlflow.log_text(str(feat_imp_map), \"feature_importances.txt\")\n",
    "            print(\"Feature Importances:\", feat_imp_map)\n",
    "\n",
    "        # Log all hyperparameter results\n",
    "        avg_metrics = cv_model.avgMetrics\n",
    "        print(\"\\nHyperparameter Combinations and Avg RMSE:\")\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(f\"{'numTrees':<12}{'maxDepth':<12}{'avg_rmse':<10}\")\n",
    "        print(\"-------------------------------------------------------\")\n",
    "\n",
    "        for i, param_map in enumerate(param_grid):\n",
    "            avg_rmse = avg_metrics[i] if i < len(avg_metrics) else \"N/A\"  # Handle index errors safely\n",
    "            num_trees_val = param_map.get(rf.numTrees, \"N/A\")\n",
    "            max_depth_val = param_map.get(rf.maxDepth, \"N/A\")\n",
    "            print(f\"{num_trees_val:<12}{max_depth_val:<12}{avg_rmse:<10.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cross-validation: {e}\")\n",
    "\n",
    "# End MLflow Run\n",
    "mlflow.end_run()\n",
    "print(\"Cross-validation complete. Check MLflow UI for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4bc828-68dd-4ef4-a665-f3f99fc2f2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this demo, we explored how to optimize machine learning models in **Databricks** using **Optuna** and **HyperOpt** with **Spark ML**. We demonstrated how these frameworks handle hyperparameter tuning at both the **model training level** and **optimization level**, leveraging distributed computing for scalability.\n",
    "\n",
    "We compared multiple strategies for hyperparameter tuning:\n",
    "- **Single-machine tuning** using Optuna for efficient local execution.\n",
    "- **Optional use of Hyperopt** with `Trials` (not `SparkTrials`) for tuning **Spark MLlib models** sequentially on the driver.\n",
    "- **End-to-end SparkML tuning** using `CrossValidator` for native Spark-based optimization.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Parallelization strategies** significantly impact model training efficiency and resource utilization.\n",
    "- **Databricks provides multiple options** for hyperparameter tuning, allowing flexibility in balancing **scalability vs. compute cost**.\n",
    "- **MLflow enables seamless experiment tracking**, making it easier to compare results across different tuning frameworks.\n",
    "\n",
    "By leveraging these frameworks effectively, you can enhance model performance, streamline experimentation, and scale machine learning workflows efficiently within Databricks.\n",
    "\n",
    "### Next Steps\n",
    "In the next demonstration, we will see how to use Ray Tune for hyperparameter optimization leveraging a single node and our understanding of Optuna from this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5935ae-2c1b-478a-9868-eb1f2d82fe5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02.1 - Hyperparameter Tuning with SparkML",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
