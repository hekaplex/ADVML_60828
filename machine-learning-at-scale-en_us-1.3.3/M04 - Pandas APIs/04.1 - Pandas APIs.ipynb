{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9106e1b9-129e-4a10-bf88-c9ec49f03d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f7f971-7306-4ec5-979b-6136c9c115d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo: Pandas APIs  \n",
    "In this demo, we will explore the key operations of Pandas, Spark, and Pandas API (Koalas) DataFrames, focusing on their performance, conversions, and using UDFs and functions for distributed processing.\n",
    "\n",
    "**Learning Objectives:**  \n",
    "_By the end of this demo, you will be able to:_\n",
    "1. **Compare the performance** of Pandas, Spark, and Pandas API (Koalas) DataFrames for numerical operations.\n",
    "2. **Convert between** Pandas, Spark, and Pandas API DataFrames to leverage their unique capabilities.\n",
    "3. **Apply Pandas UDFs and Functions** to a Spark DataFrame for distributed inference using pre-trained models.\n",
    "4. **Train group-specific machine learning models** using Pandas Function APIs for customized modeling.\n",
    "5. **Perform group-specific inference** by loading models from MLflow and running predictions across groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c55f5a92-d443-4b86-8286-28982cfa03cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc34dd0-0cdb-4660-bf9b-7f5312061615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c9681f-5a18-4c20-b34b-dfed386190a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37988421-5332-4d96-ae5c-08b52e7d9c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas pyspark koalas\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c94f981-31b0-4e3a-bd52-fbe17b1542d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1143e3db-db4a-421b-b293-fd5ffcfef598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6872b08-083e-406f-bb14-e47372c34c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b466b0f-9256-4e0f-9475-4ce83b29dc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f77374-82c6-402e-9778-9101da709915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading the Airbnb Dataset\n",
    "\n",
    "In this section, we will load the **Airbnb listings dataset**. The dataset contains important features like price, neighborhood, and property details. We will load this data into a **Spark DataFrame** and then explore how to convert it into both a Pandas and a Pandas API on Spark DataFrames to understand their capabilities and performance.\n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "1. **Load the dataset into a Spark DataFrame**: We'll use Spark's `read.csv` method to load the Airbnb dataset from a Delta table, which contains cleaned data prepared for machine learning tasks.\n",
    "2. **Inspect the data**: After loading, we will display the first few rows of the dataset to ensure it has been loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82640ad4-95e0-4c7e-9e68-0fecad6ea33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more details about working with Spark DataFrames, visit the [PySpark DataFrame Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31774642-ab6c-4bda-b065-86bbcf481544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Load the Airbnb dataset into a Spark DataFrame\n",
    "data_path = f\"{DA.paths.datasets.airbnb}/sf-listings/airbnb-cleaned-mlflow.csv\"\n",
    "airbnb_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "airbnb_df_large = airbnb_df\n",
    "# Let's make a larger table (~75 rows) out of the original table to help measure performance. \n",
    "for i in range(1,5):\n",
    "    airbnb_df_large = airbnb_df_large.union(airbnb_df_large)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(airbnb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a23821b-e8e3-4df1-b61d-77a5dbc22aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Comparing Performance of Pandas, Spark, and Pandas API DataFrames\n",
    "\n",
    "In this section, we will explore the performance differences between **Pandas**, **Spark**, and **Pandas API on Spark (Koalas)** DataFrames. By calculating the mean of numeric columns, we will compare how each framework performs and understand the advantages of using distributed vs. non-distributed DataFrames in various environments.\n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "1. **Convert Spark DataFrame to Pandas DataFrame**.\n",
    "2. **Calculate the mean for numeric columns** using Pandas.\n",
    "3. **Calculate the mean for numeric columns** using Spark DataFrame.\n",
    "4. **Use Pandas API on Spark (Koalas)** to compute the same operation in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22afbc83-bb5f-43a9-8dc5-4390cf2eee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1 Preprocessing for Performance Test\n",
    "\n",
    "Before testing the performance of the three DataFrames, we need to select numeric columns and convert the Spark DataFrame into Pandas and Pandas API on Spark DataFrames. This will prepare the data for the performance test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae64e5a-e66b-4049-851e-3335f987d44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "\n",
    "# Select numeric columns for averaging\n",
    "numeric_columns_spark_df = [col for col in airbnb_df_large.columns if airbnb_df_large.schema[col].dataType in [DoubleType(), IntegerType()]]\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "airbnb_pandas_df = airbnb_df_large.toPandas()\n",
    "\n",
    "\n",
    "# Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "airbnb_pandas_on_spark_df = ps.DataFrame(airbnb_df_large)\n",
    "\n",
    "print(f\"Our test dataframe has {airbnb_df_large.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc48a43-b38f-4ca1-97d4-55fa3f259e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"airbnb_df_large: {}\".format(type(airbnb_df_large)))\n",
    "print(\"airbnb_pandas_on_spark_df: {}\".format(type(airbnb_pandas_on_spark_df)))\n",
    "print(\"airbnb_pandas_df: {}\".format(type(airbnb_pandas_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e477946-3681-4f57-94f7-f7213ad145be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2 Performance Test: Comparing Pandas, Spark, and Pandas API on Spark\n",
    "\n",
    "In this part of the demo, we can compare the performance of **Pandas**, **Spark**, and **Pandas API on Spark (Koalas)** for various data operations, such as computing the mean of numeric columns. This comparison helps in understanding the efficiency of each framework, particularly in different computational environments (single-node vs. distributed).\n",
    "\n",
    "The image below serves as a benchmark comparison, illustrating the execution times for each framework across different operations. Key insights:\n",
    "- **Suitability of Each Framework**: Spark’s distributed processing is highly effective with large datasets, making it ideal for big data tasks, while Pandas excels on smaller datasets due to its minimal overhead.\n",
    "- **Framework-Specific Strengths and Limitations**: Some operations, like joins, may perform differently based on the framework used, with Spark often handling large data joins more efficiently than Pandas or Pandas API on Spark.\n",
    "- **Importance of Tool Selection**: Choosing the appropriate tool for a given data size and complexity can significantly impact performance, which is crucial in production settings where optimization is key.\n",
    "\n",
    "![image_metrics](../Includes/images/image_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee6ae52-11fd-4d75-bc58-d705ddb22040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start_time_spark = time.time()\n",
    "# Calculate the mean for each numeric column using Spark\n",
    "spark_mean = airbnb_df_large.select([avg(col).alias(f\"avg_{col}\") for col in numeric_columns_spark_df])\n",
    "spark_time = time.time() - start_time_spark\n",
    "\n",
    "start_time_pandas = time.time()\n",
    "# Calculate the mean using Pandas\n",
    "pandas_mean = airbnb_pandas_df.mean()\n",
    "pandas_time = time.time() - start_time_pandas \n",
    "\n",
    "start_time_pandas_api_on_spark = time.time()\n",
    "# Calculate mean using Pandas API on Spark\n",
    "pandas_on_spark_mean = airbnb_pandas_on_spark_df.mean()\n",
    "pandas_on_spark_time = time.time() - start_time_pandas_api_on_spark\n",
    "\n",
    "# Display the time taken for each framework\n",
    "print(f\"Spark DataFrame: {spark_time} seconds\")\n",
    "print(f\"Pandas DataFrame: {pandas_time} seconds\")\n",
    "print(f\"Pandas API on Spark: {pandas_on_spark_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "445accc6-4f32-4466-a09d-f8fdf60b5dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more details on working with **PySpark SQL Functions** and **Pandas API on Spark (Koalas)**, explore the following resources:\n",
    "\n",
    "- [PySpark SQL Functions Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [Pandas API on Spark Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html)\n",
    "\n",
    "These resources provide comprehensive guidance on leveraging both PySpark's built-in functions and the Pandas API on Spark for distributed data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c342b6-7e0f-4932-83d2-229630c00dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Converting Between DataFrames\n",
    "\n",
    "In this section, we will explore how to convert between different types of DataFrames: Spark, Pandas, and Pandas API on Spark. This flexibility allows you to take advantage of the specific features and performance benefits of each framework based on your data processing needs.\n",
    "\n",
    "By converting between DataFrame types, you can use the familiar syntax of Pandas with the distributed capabilities of Spark, or switch to Pandas when you're working with smaller datasets locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838093ab-4155-465a-ba47-da9dcfa91000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "\n",
    "In some cases, you might want to work with a Pandas-like syntax while still leveraging Spark’s distributed architecture. This can be achieved by converting a Spark DataFrame into a Pandas API on Spark DataFrame (previously Koalas). Let’s see how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab7c8fe-374a-45f7-bd6c-af0898620684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(pandas_on_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e815ae83-dcfe-473f-9bfc-adfae6544dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "pandas_on_spark_df = airbnb_df.to_pandas_on_spark()\n",
    "\n",
    "# Display the DataFrame in Pandas API on Spark format\n",
    "display(pandas_on_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c5ce05c-26f5-4556-b5ae-0f824475d1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Convert Pandas API on Spark DataFrame Back to Spark DataFrame\n",
    "\n",
    "Once you have finished operations using Pandas API on Spark, you might need to convert it back to a Spark DataFrame to use Spark-specific functions or take advantage of Spark’s distributed processing features for tasks like machine learning with Spark MLlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cd7cad-f4dd-4e8f-a19f-57b8793efd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas API on Spark DataFrame back to Spark DataFrame\n",
    "spark_df = pandas_on_spark_df.to_spark()\n",
    "\n",
    "# Display the converted Spark DataFrame\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eac61caa-e079-4032-972c-794198a26a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3 Convert Pandas API on Spark DataFrame to Pandas DataFrame\n",
    "\n",
    "In some cases, you may want to bring the data back into a local Pandas DataFrame, such as when working with a smaller subset of the data or applying operations exclusive to Pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d659f39c-2135-47bd-a4e3-397009f6917b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more on Pandas API on Spark, you can refer to the official [Pandas API on Spark Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a4e87e-4398-4e1d-8f36-f1e51b0aaf6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas API on Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = pandas_on_spark_df.to_pandas()\n",
    "\n",
    "# Display the Pandas DataFrame\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf3301f-933f-402f-8518-7e7c56ef7f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Applying Pandas UDF to a Spark DataFrame\n",
    "\n",
    "In this section, you will use a **pre-trained RandomForest model** from Scikit-learn and apply it to a Spark DataFrame using a **Pandas UDF**. This process allows you to leverage the trained model to make predictions on distributed Spark DataFrames, providing scalable and efficient predictions across large datasets.\n",
    "\n",
    "**Why Use Pandas UDFs?**\n",
    "\n",
    "Pandas UDFs are optimized for distributed data processing in Spark. By applying Pandas UDFs, you can run functions on Spark DataFrames that expect Pandas DataFrames as input, which is useful for machine learning models trained outside of the Spark ecosystem, like Scikit-learn models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfe6c42-d812-4897-89a7-1e8d330b16e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Training a RandomForest Model\n",
    "\n",
    "First, we will train a **RandomForest model** using Scikit-learn on a **local Pandas DataFrame**. This model will predict the `price` column based on features in the Airbnb dataset.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Convert the Spark DataFrame to a Pandas DataFrame.\n",
    "2. Separate the features (`X`) and target variable (`y`).\n",
    "3. Train the RandomForest model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789a7eed-704e-4d47-8052-0ce1d0dc93b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for local model training\n",
    "airbnb_pandas_df = airbnb_df.toPandas()\n",
    "\n",
    "# Features and target variable\n",
    "X = airbnb_pandas_df.drop(columns=[\"price\"])  # Exclude target column\n",
    "y = airbnb_pandas_df[\"price\"]  # Target column\n",
    "\n",
    "# Train the RandomForest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e0d564-2e05-42ce-8e9f-7f38d67db574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 Define and Apply a Pandas UDF\n",
    "\n",
    "Now, we will define a **Pandas UDF** that takes in the features from the Spark DataFrame, uses the trained RandomForest model to make predictions, and returns the predicted values for each row.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define the Pandas UDF to apply the pre-trained RandomForest model. We do this by appending our function `predict_udf` with the decorator `@pandas_udf`.\n",
    "2. Use the UDF to process the input columns and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036af3ab-f097-484a-bce6-ae7d639061ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Define a Pandas UDF to apply the trained RandomForest model\n",
    "@pandas_udf(\"double\")  # Output type of the UDF is a double (for predicted price)\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    # Combine input columns into a single DataFrame for model prediction\n",
    "    features = pd.concat(cols, axis=1)\n",
    "    # Return predictions from the trained RandomForest model\n",
    "    return pd.Series(model.predict(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90eac1ac-6d4e-495a-8f5a-eb81727dc8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Applying the Pandas UDF to the Spark DataFrame\n",
    "\n",
    "Now, you can apply the `predict_udf` to the Spark DataFrame to generate predictions for each row. This allows you to distribute the prediction process across the cluster, making it scalable for larger datasets.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Apply the `predict_udf` to the Spark DataFrame.\n",
    "2. Exclude the target column (`price`) when applying the UDF.\n",
    "3. Add a new column, `prediction`, containing the predicted price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb1f852-925f-4526-8ad3-edaa8ba9a09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHBhbmRhc191ZGYKaW1wb3J0IHBhbmRhcyBhcyBwZAoKIyBEZWZpbmUgdGhlIGZlYXR1cmVzIHVzZWQgZm9yIHByZWRpY3Rpb24sIGV4Y2x1ZGluZyAicHJpY2UiCmZlYXR1cmVfbmFtZXMgPSBbY29sIGZvciBjb2wgaW4gYWlyYm5iX2RmLmNvbHVtbnMgaWYgY29sICE9ICJwcmljZSJdCgpAcGFuZGFzX3VkZigiZG91YmxlIikKZGVmIHByZWRpY3RfdWRmKCpjb2xzOiBwZC5TZXJpZXMpIC0+IHBkLlNlcmllczoKICAgICMgQ29tYmluZSBpbnB1dCBjb2x1bW5zIGludG8gYSBEYXRhRnJhbWUgZm9yIG1vZGVsIHByZWRpY3Rpb24KICAgIGZlYXR1cmVzID0gcGQuY29uY2F0KGNvbHMsIGF4aXM9MSkKICAgIGZlYXR1cmVzLmNvbHVtbnMgPSBmZWF0dXJlX25hbWVzICAjIFNldCB0aGUgY29ycmVjdCBmZWF0dXJlIG5hbWVzCiAgICAKICAgICMgUmV0dXJuIHByZWRpY3Rpb25zIGZyb20gdGhlIHRyYWluZWQgUmFuZG9tRm9yZXN0IG1vZGVsCiAgICByZXR1cm4gcGQuU2VyaWVzKG1vZGVsLnByZWRpY3QoZmVhdHVyZXMpKQoKIyBBcHBseSB0aGUgUGFuZGFzIFVERiB0byB0aGUgU3BhcmsgRGF0YUZyYW1lIGZvciBwcmVkaWN0aW9ucywgZXhjbHVkaW5nICJwcmljZSIKcHJlZGljdGlvbl9kZiA9IGFpcmJuYl9kZi5zZWxlY3QoW2NvbCBmb3IgY29sIGluIGZlYXR1cmVfbmFtZXNdKS53aXRoQ29sdW1uKAogICAgInByZWRpY3Rpb24iLCAKICAgIHByZWRpY3RfdWRmKCpbYWlyYm5iX2RmW2NvbF0gZm9yIGNvbCBpbiBmZWF0dXJlX25hbWVzXSkKKQoKIyBEaXNwbGF5IHRoZSBEYXRhRnJhbWUgd2l0aCBwcmVkaWN0aW9ucwpkaXNwbGF5KHByZWRpY3Rpb25fZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd259e5a\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd259e5a\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd259e5a\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd259e5a) SELECT `neighbourhood_cleansed`,COUNT(*) `column_a7f9c861192` FROM q GROUP BY `neighbourhood_cleansed`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd259e5a\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "neighbourhood_cleansed",
             "id": "column_a7f9c861190"
            },
            "y": [
             {
              "column": "*",
              "id": "column_a7f9c861192",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_a7f9c861192": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "8fb77380-d0a4-4046-9831-b0a2146511c4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 35.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "neighbourhood_cleansed",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "neighbourhood_cleansed",
           "type": "column"
          },
          {
           "alias": "column_a7f9c861192",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Define the features used for prediction, excluding \"price\"\n",
    "feature_names = [col for col in airbnb_df.columns if col != \"price\"]\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    # Combine input columns into a DataFrame for model prediction\n",
    "    features = pd.concat(cols, axis=1)\n",
    "    features.columns = feature_names  # Set the correct feature names\n",
    "    \n",
    "    # Return predictions from the trained RandomForest model\n",
    "    return pd.Series(model.predict(features))\n",
    "\n",
    "# Apply the Pandas UDF to the Spark DataFrame for predictions, excluding \"price\"\n",
    "prediction_df = airbnb_df.select([col for col in feature_names]).withColumn(\n",
    "    \"prediction\", \n",
    "    predict_udf(*[airbnb_df[col] for col in feature_names])\n",
    ")\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8490896-f1ab-42f2-861e-cce3931d7a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: Training Group-Specific Models with Pandas Function API\n",
    "\n",
    "In this section, you will learn how to **train group-specific models** for each neighborhood using the **Pandas Function API**. By splitting the dataset based on a grouping criterion (in this case, `neighbourhood_cleansed`), we can train individual machine learning models for each group and log them using **MLflow**. This approach is useful when different groups of data may benefit from customized models.\n",
    "\n",
    "**Why Group-Specific Models?**\n",
    "\n",
    "Group-specific models allow for fine-tuned predictions by training separate models for each group, which can often yield better results compared to training a single model for the entire dataset. In this example, each **neighborhood** will have its own trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3bcdc4-99af-413c-abc3-4288ac3ff0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Define the Group-Specific Model Training Function\n",
    "\n",
    "We will define a function that:\n",
    "\n",
    "- **Trains a RandomForest model** for each neighborhood group.\n",
    "- **Logs the model** in **MLflow**.\n",
    "- **Calculates the Mean Squared Error (MSE)** for model performance evaluation.\n",
    "- **Returns key metrics**, including the model path, MSE, and the number of records used in the training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Extract the `neighbourhood_cleansed` for each group.\n",
    "2. Define the feature set (`X_group`) and the target (`y_group`).\n",
    "3. Train a RandomForest model for the group.\n",
    "4. Log the model in MLflow.\n",
    "5. Return the model's URI, MSE, and other group-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6b8743-acfa-463b-b160-f5b81352eb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Define a pandas function to train a group-specific model and log each model with MLflow\n",
    "def train_group_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Get the neighborhood name\n",
    "    neighbourhood = df_pandas['neighbourhood_cleansed'].iloc[0]\n",
    "    \n",
    "    # Define features (X) and target variable (y)\n",
    "    X_group = df_pandas.drop(columns=[\"price\", \"neighbourhood_cleansed\"])\n",
    "    y_group = df_pandas[\"price\"]\n",
    "    \n",
    "    # Train a RandomForest model for the group\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_group, y_group)\n",
    "    \n",
    "    # Log the model using MLflow\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "        model_uri = mlflow.get_artifact_uri(\"random_forest_model\")\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE) for the group\n",
    "    predictions = model.predict(X_group)\n",
    "    mse = mean_squared_error(y_group, predictions)\n",
    "\n",
    "    # Return a DataFrame containing group information and model performance\n",
    "    return pd.DataFrame({\n",
    "        \"neighbourhood_cleansed\": [str(neighbourhood)],  # Neighborhood name\n",
    "        \"model_path\": [str(model_uri)],                  # MLflow model URI\n",
    "        \"mse\": [float(mse)],                             # Mean Squared Error\n",
    "        \"n_used\": [int(len(df_pandas))]                  # Number of records used in training\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c445f372-ffeb-4aa6-8f48-dedd3dd5e3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2 Apply the Group-Specific Function Using Pandas API\n",
    "\n",
    "Now, we will use the **Pandas Function API** to apply the group-specific model training function across each group. The `applyInPandas` method is applied on the grouped data, allowing us to train a model for each neighborhood.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define the schema for the output DataFrame, which includes the group name, model path, MSE, and the number of records used in training.\n",
    "2. Group the DataFrame by the `neighbourhood_cleansed` column and apply the `train_group_model` function.\n",
    "3. Display the results, showing the model path, MSE, and other metrics for each neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39f3fbd-ed2b-4cf5-9b81-c91fa2dec30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more information on using Pandas Function APIs in PySpark, refer to the [Pandas Function API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47ec25c-269d-48f0-8886-f894575286a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType# Define the schema for the output DataFrame\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"neighbourhood_cleansed\", StringType(), True),\n",
    "    StructField(\"model_path\", StringType(), True),\n",
    "    StructField(\"mse\", DoubleType(), True),\n",
    "    StructField(\"n_used\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Apply the group-specific model training function using 'applyInPandas'\n",
    "result_df = airbnb_df.groupby(\"neighbourhood_cleansed\").applyInPandas(train_group_model, schema=schema)\n",
    "\n",
    "# Display the result DataFrame showing the model path, MSE, and number of records for each group\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc60012-27a8-463e-bd86-3d14c953f57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Group-Specific Inference Using Pandas Function API\n",
    "\n",
    "In this part, you will perform **inference for each neighborhood** using the models we previously trained and logged in **MLflow**. The goal is to load the appropriate model for each neighborhood and make predictions based on the features from the test data. After that, we will compare the predicted prices to the actual prices and evaluate the accuracy of the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44bec510-5341-4e83-b579-a5409e22babe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.1 Define the Inference Function\n",
    "\n",
    "We will define a function that:\n",
    "\n",
    "- **Loads the trained model** from MLflow for each neighborhood.\n",
    "- **Uses the correct feature set** to make predictions for that group of data.\n",
    "- **Returns the predictions** and the **actual prices** for comparison.\n",
    "\n",
    "**Steps:**\n",
    "1. Extract the model path for the neighborhood from the DataFrame.\n",
    "2. Load the trained model from MLflow.\n",
    "3. Select the feature columns for the model.\n",
    "4. Make predictions for the current group using the loaded model.\n",
    "5. Return the predictions and actual prices for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c48a9d-2a92-4698-8700-5371f63c9323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a pandas function for group-specific inference using MLflow models\n",
    "def apply_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Retrieve the model path from the DataFrame (assumes model_path is present)\n",
    "    model_path = df_pandas[\"model_path\"].iloc[0]\n",
    "    \n",
    "    # Load the model from MLflow\n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "    \n",
    "    # Define the feature columns that were used during training\n",
    "    feature_columns = [\n",
    "        \"host_total_listings_count\", \"zipcode\", \"latitude\", \"longitude\", \"property_type\",\n",
    "        \"room_type\", \"accommodates\", \"bathrooms\", \"bedrooms\", \"beds\", \"bed_type\",\n",
    "        \"minimum_nights\", \"number_of_reviews\", \"review_scores_rating\", \"review_scores_accuracy\",\n",
    "        \"review_scores_cleanliness\", \"review_scores_checkin\", \"review_scores_communication\",\n",
    "        \"review_scores_location\", \"review_scores_value\"\n",
    "    ]\n",
    "    \n",
    "    # Select only the feature columns for inference\n",
    "    X = df_pandas[feature_columns]\n",
    "    \n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Return a DataFrame with both predictions and the actual values (price)\n",
    "    return pd.DataFrame({\n",
    "        \"prediction\": predictions,           # Predicted price\n",
    "        \"actual_price\": df_pandas[\"price\"]   # Actual price for comparison\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef85b943-ea75-42c9-8d0a-cd8a54fead60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2 Apply the Inference Function\n",
    "\n",
    "Now, we will apply the `apply_model` function to each group of data (neighborhood) using the **Pandas Function API**. This will allow us to make predictions for each group of data using the corresponding trained model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define the schema for the output DataFrame, which will contain the predicted and actual prices.\n",
    "2. Join the original data with the trained model results to ensure each group has the corresponding model path.\n",
    "3. Apply the `apply_model` function using `applyInPandas`.\n",
    "4. Calculate the **accuracy** of the predictions by comparing the predicted prices with the actual prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eea350a-20fa-465c-b8fe-17af96207bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more details on using MLflow to track models, check out the [MLflow Documentation](https://mlflow.org/docs/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e589362-591c-47c0-a64c-8fc0b320e914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "\n",
    "# Define the schema to include the prediction and actual price columns\n",
    "inference_schema = \"prediction double, actual_price double\"\n",
    "\n",
    "# Ensure model_path and price are in the inference DataFrame by joining the training results\n",
    "inference_df = result_df.join(airbnb_df, \"neighbourhood_cleansed\")\n",
    "\n",
    "# Apply the model using Pandas Function API, grouped by 'neighbourhood_cleansed'\n",
    "inference_df = inference_df.groupby(\"neighbourhood_cleansed\").applyInPandas(\n",
    "    apply_model, \n",
    "    schema=inference_schema\n",
    ")\n",
    "\n",
    "# Display the result DataFrame with predictions and actual prices\n",
    "display(inference_df)\n",
    "\n",
    "# Calculate overall accuracy (percentage of predictions within 10% of actual prices)\n",
    "inference_df = inference_df.withColumn(\n",
    "    \"accuracy\", \n",
    "    (abs(inference_df[\"prediction\"] - inference_df[\"actual_price\"]) / inference_df[\"actual_price\"]) < 0.1\n",
    ")\n",
    "overall_accuracy = inference_df.filter(\"accuracy = true\").count() / inference_df.count() * 100\n",
    "\n",
    "# Display overall accuracy\n",
    "print(f\"Overall prediction accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04e68914-cf91-48d0-a670-8507d546b82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Conclusion\n",
    "In this demo, we explored the versatility and efficiency of using Pandas, Spark, and Pandas API DataFrames for data processing, conversion, and model training. We compared the performance of these frameworks, learned how to convert between different DataFrame types, and applied Pandas UDFs to Spark DataFrames for distributed model inference. Furthermore, we demonstrated how to train group-specific models using Pandas Function APIs and performed group-specific inference using models logged with MLflow. This demo showcased how to combine Spark's distributed power with Pandas' ease of use for scalable machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b982ee6-efee-44e8-b591-84bbb7be8c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04.1 - Pandas APIs",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
