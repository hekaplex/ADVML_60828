{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde58d22-e641-49fb-bcf7-e3eca9110abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ed51b7-31dc-40b8-8099-cb0ccaefc21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Demo: Model Development with Spark\n",
    "Welcome to this Demo on **model development** using **Apache Spark** and **Delta Lake**. In this Demo, we will explore the process of developing a machine learning model from data preparation to model deployment. By leveraging Spark ML and Delta Lake, we will perform critical tasks such as reading data from Delta tables, transforming it, and building a regression model that can be evaluated and registered in Unity Catalog using **MLflow**.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "_By the end of this demo, you will be able to:_\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - **Read** a Delta table into a Spark DataFrame.\n",
    "   - **Perform** data manipulation using the Spark DataFrame API.\n",
    "   - **Write** transformed data back to a Delta table.\n",
    "\n",
    "2. **Model Development:**\n",
    "   - Perform a reproducible **train-test split** using Spark ML.\n",
    "\n",
    "   - **Model Preparation:**\n",
    "     - Assemble a feature vector using `VectorAssembler` in Spark ML.\n",
    "\n",
    "   - **Model Training:**\n",
    "     - Fit a regression model using Spark ML.\n",
    "     - Create and fit a `Pipeline` to automate the training and evaluation process.\n",
    "\n",
    "   - **Model Evaluation:**\n",
    "     - Use the trained model to compute predictions on test data.\n",
    "     - Measure model performance using evaluation metrics like **Root Mean Squared Error** (RMSE) and **R²** (Coefficient of Determination).\n",
    "\n",
    "   - **Model Registration:**\n",
    "     - Log and register the trained model in **Unity Catalog** using **MLflow** for versioning and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84696b2-e60b-462e-8073-f98b09d8a4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d52295b6-93b9-4e0f-973e-fea63b85b74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3445edd-c2ac-4490-a7f0-8bef71d055c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. In particular, you will be creating a database called `new_craw` within unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2cd462c-6077-4e20-87fb-04065d075575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup-Demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34e3d5b-7e02-4852-8e86-0aa1bc30c2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb769734-e30d-4407-8cb9-c97664c069ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.wine_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6428e5f-07ae-4de0-bb40-d51c27b87fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Part 1: Data Preparation\n",
    "In this section, We will Show how to prepare the dataset for machine learning by reading data from a Delta table, performing data manipulations using the Spark DataFrame API, and writing the cleaned data back to a Delta table for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e119a700-69e2-4195-a670-82360a9812c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Read a Delta Table into a Spark DataFrame\n",
    "Delta Lake, built on top of Apache Spark, provides ACID transactions, scalable metadata handling, and the unification of batch and streaming data. This makes it ideal for handling large datasets while ensuring data integrity and performance.\n",
    "\n",
    "**Instructions:**\n",
    "- Define the path to the Delta table that contains the data.\n",
    "- Use the **`spark.read.format(\"delta\")`** function to load data from the Delta table into a Spark DataFrame.\n",
    "- Verify the schema and the loaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d24bf75-e08d-4942-a7a5-f38fb21718d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**[Delta Lake Documentation](https://docs.delta.io/latest/delta-intro.html)**: Learn more about Delta Lake’s core features, including ACID transactions and schema enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c535262-2fa6-4de9-957c-c93bdb36c41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the Delta table\n",
    "data_path = f\"{DA.paths.working_dir}/v01/large_wine_quality_delta\"\n",
    "\n",
    "# Load data into a Spark DataFrame\n",
    "df = spark.read.format(\"delta\").load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cab035-8c4e-426b-823f-0b2d873508a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc205286-ba75-44d9-94e7-8b23d3dd2372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c78877b-5288-4ecd-8042-dab4049ad4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform Basic Data Manipulations Using the Spark DataFrame API\n",
    "\n",
    "Next, we will filter and select relevant columns from the dataset for model training. The Spark DataFrame API allows us to easily perform these operations.\n",
    "\n",
    "**Instructions:**\n",
    "- **Select relevant columns** for the regression task (such as features and target labels).\n",
    "- **Filter** the rows based on the condition where `quality` is greater than 3.\n",
    "- **Display summary statistics** to understand the dataset distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08924d9a-365e-4d39-9d2e-070e4a398bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**[PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)**: Dive deeper into Spark’s DataFrame API for data selection, filtering, and transformation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9905d14e-a4dc-45ed-99c7-f3ef29691f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select specific columns for the regression task\n",
    "df_selected = df.select(\"fixed_acidity\", \n",
    "                        \"volatile_acidity\", \n",
    "                        \"citric_acid\", \n",
    "                        \"residual_sugar\", \n",
    "                        \"chlorides\", \n",
    "                        \"free_sulfur_dioxide\", \n",
    "                        \"total_sulfur_dioxide\", \n",
    "                        \"density\", \n",
    "                        \"pH\", \n",
    "                        \"sulphates\", \n",
    "                        \"alcohol\", \n",
    "                        \"quality\"\n",
    "                        )\n",
    "\n",
    "# Filter rows where the quality is greater than 3 (basic filtering)\n",
    "df_filtered = df_selected.filter(col(\"quality\") > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365ce468-88c6-4965-839a-16f0fbfe07b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the summary statistics of the dataset\n",
    "display(df_filtered.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694f8394-877d-4425-87cb-62afd28ebbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df_filtered,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e2de25-fdc1-41cf-9192-2119081161de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Spark DataFrame to a Delta Table\n",
    "Once the data has been transformed and filtered, we can write it back to a Delta table. This allows us to maintain a versioned, scalable dataset that can be accessed and updated in subsequent steps of the pipeline.\n",
    "\n",
    "**Instructions:**\n",
    "- **Define the output** path for the Delta table.\n",
    "- **Write the transformed data** back to the Delta table in \"append\" or \"overwrite\" mode.\n",
    "- **Verify the written data** by reading the Delta table again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db86319b-260b-4968-998e-3388adfddb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the output Delta table path\n",
    "output_delta_table = f\"{DA.catalog_name}.{DA.schema_name}.delta_table\"\n",
    "\n",
    "# Write the filtered DataFrame to the Delta table (Append Mode)\n",
    "df_filtered.write.format(\"delta\").mode(\"append\").saveAsTable(output_delta_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40fd31e-8172-48ab-8938-3257530f50b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_delta_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa436014-adf0-409b-b0b6-907559dad1b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe history dbacademy.labuser12229023_1769085858.delta_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7da5146-cf31-4159-b21b-ee7488df2ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the Delta table with new data\n",
    "df_filtered.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_delta_table)\n",
    "\n",
    "# Read the data back from the Delta table to verify\n",
    "df_output = spark.read.format(\"delta\").table(output_delta_table)\n",
    "\n",
    "# Display the newly saved Delta table\n",
    "display(df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4fd20d1-03fb-4220-94e0-66effacc3a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Model Development\n",
    "In this Part, we will focus on building a machine learning model using **Spark ML**. We will explore the steps required to prepare data for model training, train a regression model, and evaluate its performance.\n",
    "\n",
    "We will also cover how to create and fit a **Pipeline** in Spark to automate data transformations and model training, making it easier to manage and reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78cc1e09-c48a-46fd-b6a8-568f5cdc1831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform a Reproducible Train-Test Split Using Spark ML\n",
    "A crucial step in developing a machine learning model is to split the dataset into a **training set** and a **test set**. This ensures that the model is trained on one portion of the data and evaluated on another, helping assess its performance on unseen data. \n",
    "\n",
    "In this step, we will use **Spark ML** to split the data into 80% for training and 20% for testing. By setting a **seed**, we can ensure the *random* split is reproducible, meaning the data will be split the same way every time we run this step.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Use the `randomSplit` function from Spark to divide the dataset into training and test sets.\n",
    "2. Specify the proportions for the split: 80% of the data for training and 20% for testing.\n",
    "3. Set a random seed (e.g., `seed=42`) to ensure the split is reproducible across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418bcd7e-a93e-4400-a1a1-053b68dbc551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**[Spark ML DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html)**: Learn more about splitting data and handling DataFrames in Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e407ae-4228-449f-a208-0259a3806cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into 80% training and 20% testing sets\n",
    "train_df, test_df = df_filtered.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Display the number of records in each set\n",
    "print(f\"Training Data Count: {train_df.count()}\")\n",
    "print(f\"Test Data Count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba00cdf9-0dfe-461f-a19b-f66ff455d240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Preparation\n",
    "\n",
    "Once the data is split into training and test sets, the next step is to prepare the features for model training. This involves **assembling the selected features** into a single vector that can be fed into the machine learning model. Spark ML’s `VectorAssembler` is a key tool for this process, as it consolidates multiple feature columns into one vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cb5fa91-1650-48bf-acde-8acfaf4a5538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Assemble a Feature Vector Using Spark ML\n",
    "\n",
    "In this step, we will use VectorAssembler to combine the relevant feature columns into a **single feature vector**. This vector is necessary for feeding the data into machine learning models that expect the input features in a vectorized format. Additionally, we will apply **feature scaling** using StandardScaler to normalize the feature values, which is an important step for models like **Linear Regression, Support Vector Machines (SVM), and K-Nearest Neighbors (KNN)**. However, **tree-based models (like Gradient Boosted Trees and Random Forests) do not require feature scaling** as they are not sensitive to feature magnitude.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Select the feature columns** from the dataset that will be used to train the model.\n",
    "2. **Use VectorAssembler** to assemble these feature columns into a single vector named `features`.\n",
    "3. **Normalize the feature vector** using StandardScaler:\n",
    "   - `withMean=True` → Centers the data by subtracting the mean (zero-centered features).\n",
    "   - `withStd=True` → Scales features by dividing by the standard deviation (column-wise scaling).\n",
    "4. Apply the transformations to both the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76d683f-4503-4af5-9586-9e4b5b19fb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Further Exploration:**\n",
    "- **[VectorAssembler API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)**: Learn more about how to assemble features in Spark ML.\n",
    "- **[StandardScaler API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StandardScaler.html)**: Explore how to scale features for improved model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a59fb2-1ba9-4d1c-9d91-7afeec490aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"fixed_acidity\", \n",
    "                   \"volatile_acidity\", \n",
    "                   \"citric_acid\", \n",
    "                   \"residual_sugar\", \n",
    "                   \"chlorides\", \n",
    "                   \"free_sulfur_dioxide\", \n",
    "                   \"total_sulfur_dioxide\", \n",
    "                   \"density\", \n",
    "                   \"pH\", \n",
    "                   \"sulphates\", \n",
    "                   \"alcohol\"]\n",
    "\n",
    "# Assemble the feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Apply the assembler to the training and test datasets to create the 'features' column\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler_model = scaler.fit(train_df)\n",
    "\n",
    "# Transform both the training and test data using the same scaler model\n",
    "train_df = scaler_model.transform(train_df)\n",
    "test_df = scaler_model.transform(test_df)\n",
    "\n",
    "# Display the scaled features\n",
    "display(train_df.select(\"scaled_features\", \"quality\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8aaacf-d972-4f33-8b05-0ea8b91e77fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Training\n",
    "\n",
    "After preparing the feature vectors, the next step is to train a machine learning model. In this section, we will use a **Linear Regression Model** to fit a regression model. Additionally, we will streamline the model training process by creating and fitting a **Pipeline** in Spark ML, which automates the data transformations and model training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9a7b20-15a3-4e7b-99f1-0a00ae45f3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Fit a Model Using Spark ML\n",
    "\n",
    "In this step, we will train a machine learning model using the Linear Regression algorithm. Linear Regression is a common method for regression tasks, as it estimates relationships between the dependent variable and one or more independent variables.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Initialize the LinearRegression model** and specify the necessary parameters, such as the input feature column (scaled_features) and the target column (quality).\n",
    "2. **Train the model** on the training dataset using the fit() method.\n",
    "3. **Make predictions** on the test dataset using the transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79130ca9-52d1-408c-a2f1-49ea0c2f84c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLm1sLnJlZ3Jlc3Npb24gaW1wb3J0IExpbmVhclJlZ3Jlc3Npb24KCiMgSW5pdGlhbGl6ZSBMaW5lYXIgUmVncmVzc2lvbiBtb2RlbApsciA9IExpbmVhclJlZ3Jlc3Npb24oZmVhdHVyZXNDb2w9InNjYWxlZF9mZWF0dXJlcyIsIGxhYmVsQ29sPSJxdWFsaXR5IikKCiMgVHJhaW4gdGhlIG1vZGVsIHVzaW5nIHRoZSB0cmFpbmluZyBkYXRhCmxyX21vZGVsID0gbHIuZml0KHRyYWluX2RmKQoKIyBNYWtlIHByZWRpY3Rpb25zIG9uIHRoZSB0ZXN0IGRhdGEgICAgICAKbHJfcHJlZGljdGlvbnMgPSBscl9tb2RlbC50cmFuc2Zvcm0odGVzdF9kZikKCiMgRGlzcGxheSB0aGUgcHJlZGljdGlvbnMKZGlzcGxheShscl9wcmVkaWN0aW9ucy5zZWxlY3QoInNjYWxlZF9mZWF0dXJlcyIsICJxdWFsaXR5IiwgInByZWRpY3Rpb24iKSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewb9494cf\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewb9494cf\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewb9494cf\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewb9494cf) SELECT `quality`,`prediction` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewb9494cf\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "quality",
             "id": "column_77af8678202"
            },
            "y": [
             {
              "column": "prediction",
              "id": "column_77af8678204"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "box",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "prediction": {
             "type": "box",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "42cb4864-69a1-4ca6-b7d3-b6c6ec9d5d2c",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 29.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "quality",
           "type": "column"
          },
          {
           "column": "prediction",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"quality\")\n",
    "\n",
    "# Train the model using the training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data      \n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Display the predictions\n",
    "display(lr_predictions.select(\"scaled_features\", \"quality\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127cdb13-c0be-45ab-956a-b848e0c1a118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create and Fit a Pipeline Using Spark ML\n",
    "\n",
    "To automate and streamline the machine learning workflow, we can use **Pipelines** in Spark ML. A pipeline chains multiple stages of data transformation and model training, making the process reusable and easy to manage. In this case, we will chain the feature assembler, the StandardScaler, and the Linear Regression model into a single pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123f0797-6e87-41de-a04b-98b2367c8e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Further Exploration:\n",
    "- **[Linear Regression API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html)**: Learn more about Linear Regression for regression tasks.\n",
    "- **[Spark ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html)**: Explore how to build and use pipelines to streamline machine learning workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a7f5a2-d89b-4f13-b251-a01098c38722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# If the 'features' column already exists, drop it to avoid conflict\n",
    "if \"features\" in train_df.columns or \"scaled_features\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"features\", \"scaled_features\")\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "stages = [assembler, scaler, lr]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train the pipeline model on the training data\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f75550b3-35c1-4c36-b7b6-125029309a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Once the model has been trained, the next step is to evaluate its performance on unseen data. In this section, we will:\n",
    "- **Generate predictions** using the test dataset.\n",
    "- **Evaluate the model’s performance** using key regression metrics like **Root Mean Squared Error (RMSE)** and **R²** (Coefficient of Determination).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f1263f-7017-4f74-a191-0295cd009553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Compute Basic Predictions Using a Spark ML Model\n",
    "\n",
    "After the training phase, you can use the model to make predictions on the test data. The predictions are then compared with the actual values to measure the model’s accuracy.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Ensure the `features` column** is ready for predictions by dropping any pre-existing version in the test DataFrame.\n",
    "2. **Use the trained pipeline** to make predictions on the test data.\n",
    "3. **Display the predictions** alongside the actual values for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5587f216-bd09-4e7f-9a4b-b2685adacc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if \"features\" in test_df.columns or \"scaled_features\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"features\", \"scaled_features\")\n",
    "# Make predictions on the test data using the pipeline\n",
    "pipeline_predictions = pipeline_model.transform(test_df)\n",
    "\n",
    "# Display the predictions alongside actual values\n",
    "display(pipeline_predictions.select(\"scaled_features\", \"quality\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c97ff94c-00c1-4989-be2e-55dc53e1dffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Evaluate a Regression Model Using a Spark ML API\n",
    "\n",
    "To measure how well the model is performing, we will use two key metrics: **Root Mean Squared Error (RMSE)** and **R²** (Coefficient of Determination). RMSE gives us the average prediction error, while R² indicates how much variance in the target variable is explained by the model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Initialize evaluators** for both RMSE and R² metrics using the `RegressionEvaluator` API.\n",
    "2. **Evaluate the model** using both metrics.\n",
    "3. **Print the results** to assess the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0abf6fd-55d0-48ab-bf09-2125bd0e3e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Description:**\n",
    "- **RMSE**: Measures the average magnitude of the prediction errors, with lower values indicating better performance.\n",
    "- **R²**: Provides insight into how well the model explains the variance in the target variable, with a value closer to 1 indicating a better fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf0ce5c-8b0f-49d4-bbd5-6b10052ed109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Further Exploration:**\n",
    "- **[Spark ML RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html)**: Learn more about the metrics and evaluation methods available in Spark ML.\n",
    "- **[Root Mean Squared Error (RMSE)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.RegressionMetrics.html#pyspark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError)**: Understand how RMSE works and its significance in regression analysis.\n",
    "- **[Coefficient of Determination (R²)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.RegressionMetrics.html#pyspark.mllib.evaluation.RegressionMetrics.r2)**: Explore how R² measures the goodness of fit for a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5de810-36c7-43a9-9415-37e76f92a4f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize the regression evaluator for RMSE and R²\n",
    "evaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"quality\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"quality\", metricName=\"r2\")\n",
    "\n",
    "# Evaluate RMSE and R²\n",
    "rmse = evaluator_rmse.evaluate(pipeline_predictions)\n",
    "r2 = evaluator_r2.evaluate(pipeline_predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² (Coefficient of Determination): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31d83948-a20f-4890-a65e-8f72e9fa6e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Registration (Optional)\n",
    "\n",
    "Once the model is trained and evaluated, we can register it in **Unity Catalog** using **MLflow**. Model registration is essential for maintaining version control, enabling model sharing across teams, and facilitating model deployment in production environments. \n",
    "\n",
    "By logging the model with MLflow, you can track metrics like **RMSE** and **R²**, and make the model accessible for further usage, versioning, or deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb79619-7133-4360-8f14-47e17278449a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Register the Model in Unity Catalog with MLflow\n",
    "\n",
    "In this step, we will:\n",
    "1. Log the trained pipeline model to MLflow.\n",
    "2. Record evaluation metrics (RMSE and R²).\n",
    "3. Register the model in **Unity Catalog**, making it easy to share, test, manage, and deploy the model.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Set the registry URI** to Unity Catalog.\n",
    "2. **Infer the model signature** using the training data, which captures the input/output schema for reproducibility.\n",
    "3. **Log the model and evaluation metrics** using MLflow within a new MLflow run.\n",
    "4. **Register the model** in Unity Catalog for version control.\n",
    "5. **Set an alias** for the model version, such as \"champion,\" to identify the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65749e6d-57a0-41c6-a97b-d2194641bfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Further Exploration:**\n",
    "- **[MLflow Documentation](https://mlflow.org/docs/latest/index.html)**: Learn more about MLflow for model tracking, logging, and deployment.\n",
    "- **[Databricks Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)**: Explore how Unity Catalog facilitates model versioning, governance, and sharing.\n",
    "- **[MLflow Model Registry](https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/workspace-model-registry.html)**: Understand how to manage and deploy models using MLflow’s model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd312824-2880-48a9-9eb8-cd0a97741ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set the registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Define model name with 3-level namespace\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.wine-quality-model\"\n",
    "\n",
    "# Infer the signature using the original feature columns\n",
    "signature = infer_signature(train_df.select(*feature_columns), train_df.select(\"quality\"))\n",
    "\n",
    "# Start an MLflow run to log metrics and model\n",
    "with mlflow.start_run(run_name=\"Wine Quality Model Development\") as run:\n",
    "    \n",
    "    # Log the metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    \n",
    "    # Log the trained pipeline model to MLflow with the model signature\n",
    "    mlflow.spark.log_model(\n",
    "        pipeline_model, \n",
    "        \"wine_quality_pipeline_model\", \n",
    "        registered_model_name=model_name,\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    print(\"Model and metrics logged successfully in MLflow!\")\n",
    "    \n",
    "    # Print MLflow run link\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "    mlflow_run = f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}/#mlflow/experiments/{experiment_id}/runs/{run_id}\"\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "    print(f\"MLflow Run: {mlflow_run}\")\n",
    "\n",
    "# Register the model in Unity Catalog\n",
    "def get_latest_model_version(model_name):\n",
    "    model_version_infos = client.search_model_versions(f\"name = '{model_name}'\")\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "# Set an alias for the latest model version\n",
    "client.set_registered_model_alias(model_name, \"champion\", latest_model_version)\n",
    "\n",
    "print(f\"Model registered with version: {latest_model_version} and alias: 'champion'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a161fb4b-e604-4257-9adf-5dc037978386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we developed a machine learning model using **Apache Spark** and **Delta Lake**. We prepared data from a Delta table, built and evaluated a regression model using **Spark ML**, and assessed its performance with metrics like **RMSE** and **R²**. Finally, we registered the model in **Unity Catalog** with **MLflow**, demonstrating the seamless process of tracking and deploying models in production environments. This workflow showcases the power of Spark for scalable machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e5a0195-3227-4422-bb42-12986db602c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8998924798739690,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01.1 -  Model Development with Spark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
