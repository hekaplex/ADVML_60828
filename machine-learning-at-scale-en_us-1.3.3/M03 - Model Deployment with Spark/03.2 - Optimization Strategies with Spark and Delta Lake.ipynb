{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "433a71f3-56dc-4c22-a46e-9beb752848ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc1bf89-eeb5-47a3-9d2c-fb4b4ca6580a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo - Optimization Strategies with Spark and Delta Lake\n",
    "\n",
    "In this demo, you will learn how to use Delta Lakeâ€™s optimization techniques to enhance the performance of machine learning model deployments on Apache Spark. This demonstration will cover key Delta Lake features like `OPTIMIZE`, `VACUUM`, and advanced clustering techniques to improve query performance, reduce storage costs, and maintain data integrity.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "_By the end of this demo, you will be able to:_\n",
    "\n",
    "1. **Understand the impact of Delta Lake optimizations** on large-scale datasets used in Spark-based model deployments.\n",
    "2. **Perform core optimizations** like compaction and cleanup using Delta Lakeâ€™s `OPTIMIZE` and `VACUUM` commands.\n",
    "3. **Implement Z-Ordering and Liquid Clustering** to optimize the data layout for faster query performance.\n",
    "4. **Analyze and compare query performance** before and after applying optimizations.\n",
    "5. **Understand the impact of optimizations on real-time data access and inference performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c739314c-3802-460f-b43c-efc1beb35aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23a8969a-b684-4b7b-a652-61a9df3c6f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59fdda4c-db1c-47ec-84b8-83a9fa6faea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94df0e2f-33c1-4218-8cd0-1b86a289f925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U optuna mlflow==2.9.2 delta-spark joblibspark pyspark==3.5.3 databricks-feature-engineering==0.12.1 protobuf==4.25.3 --force-reinstall\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852b78cc-fb96-4a6c-b5c7-d15c91e6fba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca65f42-bd8c-4ac8-841b-0ac42429219f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e0f1db-c52b-47f5-babd-2bb91c5c7a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70542cc8-1e48-43a1-8811-450d5ddd07b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8187ee1a-62c3-45e4-9eed-6b48388a358c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimization Strategies with Spark and Delta Lake\n",
    "\n",
    "In this section, we will explore optimization techniques provided by Delta Lake to enhance performance, focusing on improving query speed, reducing storage costs, and maintaining data integrity for model serving.\n",
    "\n",
    "Delta Lake provides several automatic and manual optimization techniques that ensure fast and reliable querying over large datasets in distributed systems like Spark.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "311caa74-6efd-421a-91c8-242c04383216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enable Predictive Optimization for Your Account\n",
    "You must enable predictive optimization at the account level. You can then enable or disable predictive optimization at the catalog and schema levels.\n",
    "\n",
    "An account admin must complete the following steps to enable predictive optimization for all metastores in an account:\n",
    "\n",
    "* **Step 1:** Access the [Accounts Console](https://accounts.cloud.databricks.com/login).\n",
    "\n",
    "* **Step 2:** Navigate to **Settings**, then **Feature enablement**.\n",
    "\n",
    "* **Step 3:** Select **Enabled** next to **Predictive optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3110b244-457d-4d6d-863d-2965c359dac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Optimization Features in Delta Lake\n",
    "\n",
    "- **OPTIMIZE**: Coalesce small files into larger ones, improving read performance by reducing the overhead of managing small files.\n",
    "- **VACUUM**: Clean up old data files that are no longer referenced, ensuring that the storage system is efficient and up-to-date.\n",
    "- **Z-ORDER Clustering**: Optimize the data layout based on the clustering of specific columns, significantly speeding up queries.\n",
    "- **Liquid Clustering**: For more advanced automatic clustering, which incrementally clusters new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "191dd0cf-055e-468b-bd9d-06d45c1bf0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps for Running Delta Lake Optimizations:\n",
    "\n",
    "- **Run OPTIMIZE on Delta Tables**\n",
    "\n",
    "This step will compact smaller files into larger ones for faster query performance. Delta Lake helps with incremental clustering on optimized tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ee7396-0d5a-4622-b29e-691979abd2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIMIZE the Delta table to compact small files\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {DA.catalog_name}.{DA.schema_name}.distributed_predictions_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac814547-3398-4db7-9d26-5b3564732c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Run VACUUM on Delta Tables**\n",
    "\n",
    "VACUUM is used to remove files no longer referenced by Delta tables, such as old versions of data that were overwritten or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8022bc6c-15d8-412e-a734-7e1092189926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable the retention duration check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM to remove old files no longer referenced by the table\n",
    "spark.sql(f\"\"\"\n",
    "VACUUM {DA.catalog_name}.{DA.schema_name}.distributed_predictions_table RETAIN 0 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefe8f9e-e306-4a26-a773-7244d6772ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe detail distributed_predictions_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ede437-1aeb-4940-844f-7cca9bab20e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Advanced Optimization Strategies:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e001fda6-3368-4304-b593-e13976d3507b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Liquid Clustering\n",
    "Liquid Clustering incrementally clusters new data for optimal query performance, automatically selecting and updating clustering keys based on historical query workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06807dcd-0aac-4ec6-a8a4-f55b5689faef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE distributed_predictions_table\n",
    "CLUSTER BY (quality, alcohol);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fba1e2e-08af-4ad0-b981-e2bc3b720bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE distributed_predictions_table\n",
    "CLUSTER BY NONE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b456106f-fd9f-4d13-90ba-a97a60e373ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once enabled, predictive optimization intelligently selects the best clustering keys for performance improvement, monitoring and adjusting clustering columns over time based on query patterns.\n",
    "\n",
    "For more information, refer to the [Delta Lake Liquid Clustering Documentation](https://docs.databricks.com/en/delta/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e6082d-ca27-4818-a845-a6a7256f0896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Z-Ordering\n",
    "\n",
    "Z-Ordering clusters the data for faster query performance by optimizing the data layout on disk, reducing I/O operations during query execution. This is particularly useful if queries frequently filter on specific columns.\n",
    "\n",
    "**ðŸš¨Note: You will receive an error in the next cell because the table was created using liquid clustering. The next cell is only for code demonstration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61550e1a-5b15-49e1-bbea-84f38ea01897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Z-Ordering the Delta table based on columns used in queries (e.g., `quality`)\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {DA.catalog_name}.{DA.schema_name}.distributed_predictions_table\n",
    "ZORDER BY (quality)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90cd532b-b7e2-4d37-9283-47f012c2f79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Comparison of Query Performance Before and After Optimization\n",
    "\n",
    "Before applying the above optimizations, queries may take longer to execute due to the need to scan multiple small files or non-optimized layouts. After performing OPTIMIZE and VACUUM, expect:\n",
    "\n",
    "- 2x faster query performance.\n",
    "- Up to 50% savings in storage due to file compaction and cleanup of old data files.\n",
    "- No manual effort required for ongoing optimizations when predictive optimization is enabled at the account level.\n",
    "\n",
    "**Steps to Compare Query Performance:**\n",
    "\n",
    "- **Step 1:** Measure Query Performance Before Optimization\n",
    "- **Step 2:** Apply Delta Lake Optimizations\n",
    "- **Step 3:** Measure Query Performance After Optimization\n",
    "  - Run the same query again after optimizations and measure the time taken.\n",
    "- **Step 4:** Measure the time taken and compare the results.\n",
    "  - Calculate the improvement in query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f05b09-2350-4e3f-8bb1-164b3e7d91e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from time import time\n",
    "\n",
    "# Disable IO cache before running any query to ensure accurate performance measurement\n",
    "spark.conf.set('spark.databricks.io.cache.enabled', False)\n",
    "# Define the table and columns you will query\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.predictions_before_optimize\"\n",
    "\n",
    "# Step 1: Run a query on the Delta table before optimization\n",
    "query = f\"\"\"\n",
    "SELECT quality, COUNT(*)\n",
    "FROM {table_name}\n",
    "GROUP BY quality\n",
    "\"\"\"\n",
    "\n",
    "# Start the timer for the query before optimization\n",
    "start_time_before_optimize = time()\n",
    "df_before_optimize = spark.sql(query)\n",
    "display(df_before_optimize)  # Running the query and displaying the results\n",
    "\n",
    "# Calculate the time taken for the query before optimization\n",
    "time_before_optimize = time() - start_time_before_optimize\n",
    "print(f\"Query Execution Time Before Optimization: {time_before_optimize:.6f} seconds\")\n",
    "\n",
    "# Step 2: Perform Delta Lake Optimizations\n",
    "print(\"Applying Delta Lake Optimizations...\")\n",
    "\n",
    "# Optimize to compact small files\n",
    "spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "\n",
    "# Vacuum to remove old files\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\")\n",
    "\n",
    "# Disable IO cache\n",
    "spark.conf.set('spark.databricks.io.cache.enabled', False)\n",
    "\n",
    "# Step 3: Run the same query again after optimization\n",
    "start_time_after_optimize = time()\n",
    "df_after_optimize = spark.sql(query)\n",
    "display(df_after_optimize)  # Running the query and displaying the results\n",
    "\n",
    "# Calculate the time taken for the query after optimization\n",
    "time_after_optimize = time() - start_time_after_optimize\n",
    "print(f\"Query Execution Time After Optimization: {time_after_optimize:.6f} seconds\")\n",
    "\n",
    "# Step 4: Compare the results\n",
    "\n",
    "# Calculate the improvement in query performance\n",
    "performance_improvement = (time_before_optimize - time_after_optimize) / time_before_optimize * 100\n",
    "print(f\"Performance Improvement: {performance_improvement:.2f}%\")\n",
    "\n",
    "# Compare storage size before and after optimization\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "display(history_df)\n",
    "\n",
    "# Display the performance comparison results\n",
    "performance_comparison = {\n",
    "    \"Metric\": [\"Execution Time (Before)\", \"Execution Time (After)\", \"Performance Improvement\"],\n",
    "    \"Value\": [f\"{time_before_optimize:.2f} seconds\", f\"{time_after_optimize:.2f} seconds\", f\"{performance_improvement:.2f}%\"]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "performance_df = pd.DataFrame(performance_comparison)\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ce356a-e8c7-44c0-a7f4-d65adce05123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more information on Delta Lake optimizations, refer to the [Delta Lake Optimizations Documentation](https://docs.databricks.com/delta/optimizations.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b814a98-5c5b-4bed-9721-45473d9e2c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Impact of Delta Lake Optimizations on Model Serving\n",
    "By applying these Delta Lake optimizations, the model serving pipeline becomes faster and more resource-efficient, which leads to:\n",
    "\n",
    "- **Reduced Latency:** Quicker access to the relevant data, ensuring that model predictions are delivered with minimal delay.\n",
    "\n",
    "- **Improved Query Performance:** Optimized data layout and clustering lead to faster data access during inference, especially for real-time or high-frequency serving scenarios.\n",
    "\n",
    "- **Lower Resource Consumption:** With fewer small files and cleaner storage, the system uses fewer resources to retrieve data, resulting in better overall efficiency.\n",
    "\n",
    "Incorporating these optimizations ensures that your model serving pipeline is not only fast but also scalable, capable of handling real-time demands without compromising performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718350fa-33d6-43e1-b022-64a924e72d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointTag\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Initialize Databricks Workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Set the MLflow registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Define the model name\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.SparkML\" \n",
    "\n",
    "# Set the experiment\n",
    "experiment_name = f\"/Users/{DA.username}/experiments_SparkML\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Check if the model exists\n",
    "try:\n",
    "    model_version_champion = client.get_model_version_by_alias(\n",
    "        name=model_name, \n",
    "        alias=\"champion\"\n",
    "    ).version\n",
    "    print(f\"Champion model version: {model_version_champion}\")\n",
    "except mlflow.exceptions.RestException:\n",
    "    print(f\"Model {model_name} does not exist. Registering a new model.\")\n",
    "    \n",
    "    # Create a sample model (replace with your actual model training code)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load iris dataset\n",
    "    iris = load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a RandomForest model\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model with MLflow\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model, \n",
    "            artifact_path=\"model-artifacts\", \n",
    "            signature=signature\n",
    "        )\n",
    "        run_id = run.info.run_id\n",
    "\n",
    "    # Register the model\n",
    "    model_uri = f\"runs:/{run_id}/model-artifacts\"\n",
    "    model_details = mlflow.register_model(model_uri, model_name)\n",
    "    model_version_champion = model_details.version\n",
    "    print(f\"Registered new model version: {model_version_champion}\")\n",
    "\n",
    "# Define the endpoint configuration\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_champion,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the endpoint configuration input from the dictionary\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n",
    "\n",
    "# Construct the endpoint name\n",
    "endpoint_name = f\"{DA.username}_SparkML_Demo_03\"\n",
    "endpoint_name = endpoint_name.replace(\".\", \"-\")\n",
    "endpoint_name = endpoint_name.replace(\"@\", \"-\")\n",
    "endpoint_name = endpoint_name.replace(\"+\", \"_\")\n",
    "# Create the endpoint_name key/value pair to be passed on in the job configuration\n",
    "dbutils.jobs.taskValues.set(key = \"endpoint_name\", value = endpoint_name)\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "# Attempt to create or update the serving endpoint\n",
    "try:\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"lab1_jobs_model\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "    else:\n",
    "        raise(e)\n",
    "    \n",
    "# Display the endpoint URL\n",
    "displayHTML(f'Our Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{endpoint_name}\">Model Serving Endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2021bb39-7f59-4bc0-a490-03334f54051d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš¨ **Deleting the Model Serving Endpoint**\n",
    "\n",
    "After completing the demo, it's important to clean up any resources that were created, including the model serving endpoint. Deleting the endpoint ensures that you're not consuming unnecessary resources, and helps maintain a clean workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73e2a0c0-a676-4deb-9ce5-e6fb78edb761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_model_serving_endpoint(endpoint_name):\n",
    "    w.serving_endpoints.delete(name=endpoint_name)\n",
    "    print(endpoint_name, \"endpoint is deleted!\")\n",
    "        \n",
    "delete_model_serving_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4a1df8-42ce-496b-986b-d3c7cb2ab3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Conclusion\n",
    "\n",
    "In this demo, we explored key Delta Lake optimization strategies to enhance the performance of machine learning workloads deployed on Apache Spark. By applying optimizations like **OPTIMIZE**, **VACUUM**, and **Z-ORDER clustering**, we demonstrated how to improve query execution times and reduce storage costs. Additionally, we introduced **Liquid Clustering** as an advanced technique for dynamically managing data layout based on query patterns.\n",
    "\n",
    "We also performed a performance comparison to quantify the improvements achieved by these optimizations, emphasizing the importance of efficient data layout and management in large-scale data systems. With these Delta Lake techniques, you can significantly reduce latency and optimize resource consumption, making your model serving pipeline more scalable and responsive to real-time demands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77646198-c759-4949-be72-4a420030d94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8998924798740873,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03.2 - Optimization Strategies with Spark and Delta Lake",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
