{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc4ead1-c5b3-4000-af33-719d7627d3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bfff42f-ae8a-484d-9926-8bf5fa3e0e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo - Model Deployment with Spark\n",
    "\n",
    "In this demo, you will learn how to deploy machine learning models using Apache Spark. We will explore different deployment strategies, comparing single-node and distributed Spark ML models, and performing inference using Spark UDFs to scale predictions efficiently across large datasets.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "_By the end of this demo, you will be able to:_\n",
    "\n",
    "1. **Understand deployment methods** for machine learning models using Spark, including single-node and distributed model deployments.\n",
    "2. **Compare single-node vs. distributed model deployments** using Spark DataFrames and Spark MLlib.\n",
    "3. **Perform parallelized inference** using `spark_udf` on distributed data to efficiently handle large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ed4231-3884-4ef1-9d4d-e3a737a82bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be648dd-d43e-4187-90b1-a1f882b889b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "190a28a7-5ec0-4fc7-9a07-f3db463e9b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9ac047-95c7-4ced-b7f5-5ce6d61253a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U optuna mlflow==2.9.2 delta-spark joblibspark pyspark==3.5.3 databricks-feature-engineering==0.12.1\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2e450f-4313-47a8-a77b-e43f521e46ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3ba3a1-feba-419b-ba24-6529f8461be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup-Demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "154e1bcb-55f2-4edc-85be-64e3f71202bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49b6b3d-d61b-48e2-a920-c585c2ed6d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.wine_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c089870c-44da-4fb8-955d-2c858da66a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-Steps: Data Preparation\n",
    "\n",
    "Before diving into model deployment, we first need to prepare our dataset for both training and inference. In this demo, we will use the **Wine Quality** dataset. The data will be loaded from a Delta table, and we will perform necessary manipulations such as assembling features and splitting the dataset into training and test sets.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Load the Wine Quality dataset** from Delta Lake.\n",
    "2. **Assemble features** into a vector using `VectorAssembler` for Spark ML training.\n",
    "3. **Split the dataset** into training (80%) and testing (20%) sets for both Spark ML and single-node Scikit-learn training.\n",
    "4. **Convert the Spark DataFrame** into Pandas DataFrame for single-node model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9477e8e2-402a-440b-afae-9367259e4c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Load the large Wine Quality dataset from the new Delta table\n",
    "data_path = f\"{DA.paths.working_dir}/v01/large_wine_quality_delta\"\n",
    "df = spark.read.format(\"delta\").load(data_path)\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    \"fixed_acidity\", \"volatile_acidity\", \"citric_acid\", \"residual_sugar\", \n",
    "    \"chlorides\", \"free_sulfur_dioxide\", \"total_sulfur_dioxide\", \"density\", \n",
    "    \"pH\", \"sulphates\", \"alcohol\"\n",
    "]\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets (for SparkML Model Training)\n",
    "train_df, test_df = df_with_features.randomSplit([0.8, 0.2], seed=42)\n",
    "# Convert Spark DataFrame to Pandas DataFrame for single-node model training\n",
    "train_pandas_df = train_df.select(feature_columns + [\"quality\"]).toPandas()\n",
    "test_pandas_df = test_df.select(feature_columns + [\"quality\"]).toPandas()\n",
    "\n",
    "# Split features and target variable\n",
    "X_train = train_pandas_df.drop(columns=[\"quality\"])\n",
    "y_train = train_pandas_df[\"quality\"]\n",
    "X_test = test_pandas_df.drop(columns=[\"quality\"])\n",
    "y_test = test_pandas_df[\"quality\"]\n",
    "\n",
    "# Display the first few rows of the DataFrame to ensure correctness\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbca58fa-2090-4873-86fa-44aa1e43d57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Methods to Deploy Machine Learning Models in Spark\n",
    "\n",
    "Spark provides multiple approaches for deploying machine learning models, making it a powerful and flexible framework for both data processing and model serving. Depending on the size of your dataset and the computational requirements, you can choose from two main approaches for model deployment:\n",
    "\n",
    "- **Single-node model deployment:** The model is trained and executed on a single machine. Even though the model is single-node, Spark can still distribute the data for processing. This approach works well for smaller datasets and simpler applications.\n",
    "\n",
    "- **Distributed model deployment:** This approach fully leverages Spark's distributed architecture to handle large datasets and complex computations. Models are deployed across multiple nodes in a cluster, making it suitable for large-scale machine learning tasks.\n",
    "\n",
    "For more information on Spark MLlib and its capabilities for distributed machine learning, refer to the [Apache Spark MLlib Documentation](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba619be-d43e-4ce0-a302-7835bdc0446b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why Use Spark for Model Deployment?\n",
    "Apache Spark provides a robust platform for deploying machine learning models at scale. Spark’s ability to process distributed data and its seamless integration with Spark MLlib, Scikit-learn, and other ML libraries makes it a versatile tool for both small and large datasets. By the end of this demo, you’ll have a solid understanding of how to choose between single-node and distributed model deployment and how Delta Lake optimizations can enhance your model's performance.\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Scalability:** Spark is designed to handle massive datasets, scaling seamlessly across clusters. This makes it ideal for training and deploying models on large data.\n",
    "- **Versatility:** Spark integrates with various machine learning libraries, including Scikit-learn, Spark MLlib, XGBoost, and more. This allows you to choose the best tools for training your models while benefiting from Spark’s distributed architecture.\n",
    "- **Efficiency:** With its in-memory processing and parallelized operations, Spark ensures efficient data processing and model serving. This is useful for both batch and real-time inference scenarios.\n",
    "\n",
    "By deploying machine learning models on Spark, you can achieve high-performance, scalable model serving that meets both real-time and batch processing needs.\n",
    "\n",
    "**For further exploration:**  \n",
    "- [Spark MLlib Documentation](https://spark.apache.org/docs/latest/ml-guide.html): Learn more about Spark’s native machine learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5520bd-c345-487f-84ec-729ddfc7559a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Comparing Single-node and Distributed Model Deployments in Spark\n",
    "In this section, you will explore and compare two different methods for deploying a machine learning model in Spark using the same model configurations in **Part 1**. We will specifically focus on comparing the training and inference time for deploying the **Decision Tree model** in both single-node and distributed configurations.\n",
    "\n",
    "- **Single-node model deployment**: We will use the **DecisionTreeRegressor** model from `Scikit-learn`, which runs on a single machine to perform training and inference.\n",
    "- **Distributed model deployment**: We will use the **DecisionTreeRegressor** model from Spark MLlib, which performs distributed training and inference across multiple nodes in a Spark cluster.\n",
    "\n",
    "Both methods will be applied to the same dataset. However, the deployment approaches will differ significantly in scalability and computational performance. The single-node model executes training and inference on a single machine, while the distributed model leverages Spark’s parallel processing capabilities across a cluster.\n",
    "\n",
    "The objective of this comparison is to evaluate the execution efficiency of both configurations in terms of training and inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14eed82-bac4-4136-b661-4e73884322c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Single-node Model Deployment with Spark DataFrames\n",
    "In this section, we will perform single-node model deployment using the **DecisionTreeRegressor** from Scikit-learn. While the model training and inference are executed on a single machine, Spark’s distributed data capabilities will still be leveraged to handle and distribute the data efficiently.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Train a DecisionTreeRegressor model** using Scikit-learn. Although the data is distributed across the cluster, the training will occur on a single machine.\n",
    "2. **Perform inference on the test data** using the trained model.\n",
    "3. **Log the model with MLflow** for future reference and tracking.\n",
    "4. **Evaluate the model’s performance** by comparing the time taken for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b43bbad-3afd-43a8-a25f-af9586307f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import mlflow\n",
    "import time\n",
    "\n",
    "# Set the active experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/Model_Deployment_with_Spark\")\n",
    "\n",
    "# Train a Decision Tree model using Scikit-learn\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "training_time_single_node = time.time() - start_time\n",
    "print(f\"Training time (single-node): {training_time_single_node} seconds\")\n",
    "\n",
    "# Log model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.sklearn.log_model(model, \"decision_tree_single_node\")\n",
    "    run_id_single_node = run.info.run_id  # Capture the run ID\n",
    "    print(f\"Single-node model logged in run: {run_id_single_node}\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    inference_time_single_node = time.time() - start_time\n",
    "    print(f\"Inference time (single-node): {inference_time_single_node} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b873f5d-026b-40e7-a7e6-aff333aaef5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distributed Model Deployment with Spark MLlib\n",
    "\n",
    "In this section, we will demonstrate the deployment of a **DecisionTreeRegressor** model using Spark MLlib. Spark MLlib fully leverages the distributed nature of Spark, making it scalable and ideal for training and inference on large datasets.\n",
    "\n",
    "We will train a **DecisionTreeRegressor** model, perform distributed inference, and log the trained model using MLflow for future tracking and deployments.\n",
    "\n",
    "**Steps:**\n",
    "1. **Train a DecisionTreeRegressor model** using Spark MLlib on a distributed Spark DataFrame.\n",
    "2. **Log the trained model** using MLflow for version control and future inference.\n",
    "3. **Set an alias** for the registered model version to easily identify and retrieve the best-performing model.\n",
    "4. **Perform distributed inference** on the test data.\n",
    "\n",
    "**Walkthrough:**\n",
    "- **Initialize the DecisionTreeRegressor model**: We will use Spark's `DecisionTreeRegressor` to initialize and train the model on distributed data, allowing for scalable training.\n",
    "- **Create a pipeline**: By using Spark's `Pipeline`, we can chain multiple steps, including feature vector assembly and model training, into a single unified workflow.\n",
    "- **Train the model**: The training will be executed across a distributed Spark DataFrame, which allows for efficient and scalable processing of large datasets.\n",
    "- **Log the model with MLflow**: The trained model will be logged into MLflow for tracking, reproducibility, and potential future deployments.\n",
    "- **Set model alias**: We will assign a \"champion\" alias to the latest model version in MLflow, making it easier to reference the best-performing version for future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0aa7e1-3789-4dd5-8e28-a2719e5b76fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow\n",
    "import time\n",
    "\n",
    "# Remove or rename the existing 'features' column if it exists\n",
    "if 'features' in train_df.columns:\n",
    "    train_df = train_df.drop('features')\n",
    "if 'features' in test_df.columns:\n",
    "    test_df = test_df.drop('features')\n",
    "\n",
    "# Define and train the DecisionTreeRegressor model in distributed mode\n",
    "decision_tree = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"quality\", maxDepth=5)\n",
    "\n",
    "# Create a pipeline for training\n",
    "pipeline = Pipeline(stages=[assembler, decision_tree])\n",
    "\n",
    "# Measure training time for the distributed model\n",
    "start_time = time.time()\n",
    "dt_model = pipeline.fit(train_df)\n",
    "training_time_distributed = time.time() - start_time\n",
    "print(f\"Training time (distributed): {training_time_distributed} seconds\")\n",
    "\n",
    "# Log the model using MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.spark.log_model(dt_model, artifact_path=\"model-artifacts\")\n",
    "    run_id_distributed = run.info.run_id  # Capture the run ID\n",
    "    print(f\"Distributed model logged in run: {run_id_distributed}\")\n",
    "\n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    predictions = dt_model.transform(test_df)\n",
    "    inference_time_distributed = time.time() - start_time\n",
    "    print(f\"Inference time (distributed): {inference_time_distributed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f99362f8-3c9a-4f20-a89e-178d98028e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparison: Single-Node vs. Distributed Model\n",
    "\n",
    "In this section, we will compare the performance of the **Decision Tree Regressor** model deployed in both single-node and distributed configurations. Rather than focusing solely on error metrics, we will assess the performance based on the following key factors:\n",
    "\n",
    "1. **Training Time**: Measure the time taken to train the model in both single-node and distributed configurations. This will highlight the efficiency of each approach in handling data processing and training.\n",
    "2. **Inference Time**: Measure the time taken to perform inference on the test data using each model. This will help compare the scalability of the deployment methods when applying the trained model to make predictions.\n",
    "3. **Resource Utilization**: Optionally, monitor and compare resource utilization (like memory and CPU usage) for each approach to understand the resource efficiency of each deployment.\n",
    "\n",
    "By focusing on training and inference times, this comparison emphasizes the practical differences in scalability and efficiency between single-node and distributed deployments using Spark. This comparison will help you choose the best approach based on your dataset size and computational needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa702f71-2c50-41c5-8adf-e342cfa771b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a comparison table for training and inference times\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Single-Node DecisionTreeRegressor\", \"Distributed DecisionTreeRegressor\"],\n",
    "    \"Training Time (seconds)\": [training_time_single_node, training_time_distributed],\n",
    "    \"Inference Time (seconds)\": [inference_time_single_node, inference_time_distributed]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Time Comparison between Single-Node and Distributed Gradient Boosting Models:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f483cbc-63ef-495a-a5c9-e0207d2dd5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Performing Parallelized Inference Using `spark_udf`\n",
    "\n",
    "**Why Use UDFs for Parallelized Inference?**\n",
    "\n",
    "User Defined Functions (UDFs) in Spark allow you to apply custom functions to data in parallel. By defining a UDF, you can distribute the inference workload across a Spark cluster. This method ensures that even if your model is single-node, the inference can scale across your distributed data.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Define a Spark UDF** that applies your trained Scikit-learn model to distributed data.\n",
    "2. **Use the UDF** to perform inference on the distributed Spark DataFrame.\n",
    "3. **Log the results** and view predictions using Spark.\n",
    "\n",
    "\n",
    "**For further learning:**  \n",
    "You can explore more about UDFs and parallelized operations in the [Spark UDF Documentation](https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503fcc75-c8e8-4b14-846a-d36c6d338e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, struct\n",
    "import mlflow.pyfunc\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#Define a Spark UDF that uses your trained model\n",
    "\n",
    "# Load the logged model from MLflow using the run_id captured earlier\n",
    "model_uri = f\"runs:/{run_id_single_node}/decision_tree_single_node\"  # Replace 'run_id' with your logged model's run ID\n",
    "\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri,\n",
    "    result_type=\"double\",\n",
    "    env_manager=\"local\"   # <-- Add this\n",
    ")\n",
    "\n",
    "# Prepare the Spark DataFrame by creating feature vectors\n",
    "# (Ensure that 'features' column doesn't already exist from earlier)\n",
    "if \"features\" not in df_with_features.columns:\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    df_with_features = assembler.transform(df_with_features)\n",
    "\n",
    "#Apply the UDF to distributed data for parallelized inference\n",
    "# The UDF is applied on the 'features' column to generate predictions\n",
    "predictions = df_with_features.withColumn(\n",
    "    \"predicted_quality\",\n",
    "    predict_udf(struct(*feature_columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d70734-a4d1-4e6e-8aec-9df74473a84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display predictions\n",
    "display(predictions.select(*feature_columns, \"quality\", \"predicted_quality\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e39b3a9-a695-464b-a147-7654db3fa3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving predictions to a Delta table:\n",
    "predictions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.single_node_udf_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52642b03-da93-49ca-8414-f6f402ce5b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distributed Inference Using Spark ML\n",
    "\n",
    "**Why Use Distributed Inference with Spark ML?**\n",
    "\n",
    "For larger datasets and machine learning models, Spark MLlib provides an efficient way to perform inference in a distributed manner. Spark MLlib handles both data distribution and computation, making it highly scalable and ideal for large-scale workloads. \n",
    "\n",
    "Distributed inference allows us to apply the trained model to data across multiple nodes, improving performance and enabling the model to handle large datasets in real-time.\n",
    "\n",
    "**Steps:**\n",
    "1. **Prepare Features**: Use Spark’s `VectorAssembler` to combine multiple feature columns into a single feature vector.\n",
    "2. **Train a Distributed Model**: Train a distributed **Decision Tree Regressor** model with Spark ML on your dataset.\n",
    "3. **Perform Distributed Inference**: Apply the trained model to new data using Spark ML's `transform()` method.\n",
    "4. **Save the Predictions**: Store the inference results (predictions) in a Delta table for further analysis or reporting.\n",
    "\n",
    "**For further exploration** [Spark MLlib's Guide](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e94ade-2f21-4ea8-9663-f08e792b67aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if \"features\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"features\")\n",
    "\n",
    "# Perform inference on the test data using the distributed GBT model\n",
    "predictions_dt = dt_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "display(predictions_dt.select(\"features\", \"quality\", \"prediction\"))\n",
    "\n",
    "# Save predictions to a Delta table in Unity Catalog\n",
    "predictions_dt.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.distributed_predictions_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ac16d4-8ff0-49ff-88b8-237989d32d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we explored two key deployment strategies using Spark: single-node deployment with Scikit-learn and distributed deployment with Spark MLlib. We highlighted the benefits and challenges of each approach in terms of scalability and processing power. Additionally, we demonstrated parallelized inference using Spark UDFs, showcasing how to leverage Spark’s distributed architecture for efficient large-scale predictions. This comprehensive approach to model deployment ensures that machine learning models can efficiently scale and serve predictions across large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "314736d3-e2ba-4efe-a99c-4e521e3c7c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03.1 - Model Deployment with Spark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
