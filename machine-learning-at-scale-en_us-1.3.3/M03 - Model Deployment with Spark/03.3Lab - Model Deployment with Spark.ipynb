{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6030e4f1-ec0c-4243-92b9-a1285e91aa4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b3c5f0-144b-4701-9031-37d6fedc6088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Model Deployment with Spark \n",
    "\n",
    "In this lab, you will gain hands-on experience in deploying machine learning models using Apache Spark and optimizing query performance with Delta Lake. You will explore both single-node and distributed model deployment strategies, log models using MLflow for tracking, and apply advanced Delta Lake optimization techniques like **OPTIMIZE**, **VACUUM**, and **Liquid Clustering** to enhance performance and resource efficiency.\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- **Task 1: Single-Node Model Deployment**  \n",
    "  - Train a single-node **Gradient Boosted Tree (GBT)** model using **Scikit-learn**.\n",
    "  - Log the trained model in **MLflow**.\n",
    "  - Use **Spark UDFs** to perform parallelized inference on distributed data.\n",
    "\n",
    "- **Task 2: Distributed Model Deployment with Spark MLlib**  \n",
    "  - Train a distributed **Gradient Boosted Tree (GBT)** model using Spark MLlib.\n",
    "  - Log the model using **MLflow** and set a model alias for easy reference.\n",
    "  - Perform distributed inference and save predictions to a Delta table.\n",
    "\n",
    "- **Task 3: Delta Lake Optimizations**  \n",
    "  - Apply Delta Lake optimization strategies like **OPTIMIZE** to compact small files.\n",
    "  - Perform **VACUUM** to clean up old, unused data files.\n",
    "  - Apply **Z-ORDER Clustering** for faster reads on frequently queried columns.\n",
    "  - Enable **Liquid Clustering** to incrementally optimize data layout.\n",
    "\n",
    "- **Task 4: Performance Comparison Before and After Optimization**  \n",
    "  - Measure query performance before and after applying Delta Lake optimizations.\n",
    "  - Compare the improvements in query speed and resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4381d6ad-b798-409b-9ced-73d42c9af9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8857c2d9-b2ff-4b82-ac8d-85fbd97241cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd0166df-6497-47f1-a102-4f43093414db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6f3782d-cca4-4e71-9cd0-fbe5212be67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U optuna mlflow>=3.0 delta-spark joblibspark pyspark==3.5.3\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2f2333-d19a-4312-b026-9fa759ccad3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe83d95f-74ff-408b-819b-1bf864e58da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup-lab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "719cb2ee-7d1b-463b-83a7-e472d7637cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6cde754-5db7-46a3-90f9-b145c9ccca44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.california_housing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef421f7-4e52-4e24-95d5-0038afe325bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Pre-Steps: Data Preparation and Feature Engineering\n",
    "Before you dive into model deployment, you need to prepare the California Housing dataset for model training and testing. This includes loading the dataset from Delta format, performing feature engineering, and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a779fc52-abf8-4e89-bfd9-e72f86dd4676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Loading the California Housing Dataset\n",
    "In this step, you'll load the California Housing dataset from Delta Lake and prepare it for both Spark and Scikit-learn model training. The dataset will be split into an 80/20 ratio for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e92dde-8183-4223-b6a5-9ce1890b97af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## Load the California Housing dataset from Delta table\n",
    "data_path = f\"{DA.paths.datasets.california_housing}/data\"\n",
    "df = spark.read.format(\"delta\").load(data_path)\n",
    "\n",
    "## Define feature columns\n",
    "feature_columns = [\n",
    "    \"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \n",
    "    \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"\n",
    "]\n",
    "\n",
    "## Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "## Split the data into training and test sets (for SparkML Model Training)\n",
    "train_df, test_df = df_with_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Convert Spark DataFrame to Pandas DataFrame for single-node model training\n",
    "train_pandas_df = train_df.select(feature_columns + [\"label\"]).toPandas()\n",
    "test_pandas_df = test_df.select(feature_columns + [\"label\"]).toPandas()\n",
    "\n",
    "## Split features and target variable for Scikit-learn training\n",
    "X_train = train_pandas_df.drop(columns=[\"label\"])\n",
    "y_train = train_pandas_df[\"label\"]\n",
    "X_test = test_pandas_df.drop(columns=[\"label\"])\n",
    "y_test = test_pandas_df[\"label\"]\n",
    "\n",
    "## Display the first few rows of the dataset to verify data loading\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e2fbf7-fb43-4c3d-987f-aa9e95a2430c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Single-Node Model Deployment with XGBoost and Spark UDFs\n",
    "\n",
    "In this task, you will train an **XGBRegressor** model using a **single-node training** approach. After training the model, you will log it using **MLflow** to enable tracking and version control. Additionally, you will apply **Spark UDFs** to perform parallelized inference on distributed data using the trained model. This approach demonstrates how to deploy a single-node model and leverage Spark's distributed computing capabilities for inference.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Train an XGBRegressor model** on the provided training data.\n",
    "2. **Log the trained model** in MLflow for tracking and reproducibility.\n",
    "3. **Perform inference** on the test data to evaluate model predictions.\n",
    "4. **Apply Spark UDFs** to deploy the model and perform distributed, parallelized inference on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b00b663-3f22-4421-a90b-0c6038068015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.1: Training and Logging the XGBoost Model  \n",
    "\n",
    "In this step, you will train an **XGBRegressor** model on the training dataset. After training, you will log the model in MLflow, allowing you to track the model's parameters and performance metrics for future reference.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Train an **XGBRegressor** model with specified parameters on the training data.\n",
    "- **Step 2:** Log the trained model using MLflow to track the training run, including metrics like training and inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2caa7a-ac1c-4ff2-87ab-f8aed96d3a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import time\n",
    "## Set the active experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/lab_experiment\")\n",
    "## Train the XGBRegressor model\n",
    "xgb_model = XGBRegressor(\n",
    "    <FILL_IN>\n",
    ")\n",
    "\n",
    "## Measure training time\n",
    "start_time = time.time()\n",
    "xgb_model.fit(<FILL_IN>)\n",
    "training_time_single_node = time.time() - start_time\n",
    "print(f\"Training time (single-node): {training_time_single_node:.2f} seconds\")\n",
    "\n",
    "## Log the XGBRegressor model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.xgboost.log_model(<FILL_IN>)\n",
    "    run_id_single_node = run.info.run_id  # Capture the MLflow run ID for later use\n",
    "    print(f\"Model logged in run: {<FILL_IN>}\")\n",
    "\n",
    "    ## Measure inference time\n",
    "    start_time = time.time()\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    inference_time_single_node = time.time() - start_time\n",
    "    print(f\"Inference time (single-node): {inference_time_single_node:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5bb886-358b-4129-bb65-cfe4f7cada25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import time\n",
    "## Set the active experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/lab_experiment\")\n",
    "## Train the XGBRegressor model\n",
    "xgb_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "## Measure training time\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "training_time_single_node = time.time() - start_time\n",
    "print(f\"Training time (single-node): {training_time_single_node:.2f} seconds\")\n",
    "\n",
    "## Log the XGBRegressor model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.xgboost.log_model(xgb_model, \"xgboost_model\")\n",
    "    run_id_single_node = run.info.run_id  # Capture the MLflow run ID for later use\n",
    "    print(f\"Model logged in run: {run_id_single_node}\")\n",
    "\n",
    "    ## Measure inference time\n",
    "    start_time = time.time()\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    inference_time_single_node = time.time() - start_time\n",
    "    print(f\"Inference time (single-node): {inference_time_single_node:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df17df79-fc6e-4c35-a097-20315195d849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.2: Performing Parallelized Inference Using Spark UDFs  \n",
    "\n",
    "In this step, you will use Spark UDFs to perform distributed inference by applying the trained **XGBRegressor** model across a Spark DataFrame. This approach allows you to scale the inference process across the cluster, taking advantage of Spark's parallel processing capabilities.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Load the trained XGBRegressor model from MLflow using its URI.\n",
    "- **Step 2:** Define a Spark UDF that applies the loaded model to perform distributed inference on each data partition.\n",
    "- **Step 3:** Display the results, including the original features, true labels, and predicted values, to evaluate the model's predictions on distributed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b34064b1-8a73-4e7a-a787-ba69322dffe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, struct\n",
    "import mlflow.pyfunc\n",
    "\n",
    "## Perform inference on the test set\n",
    "y_pred = xgb_model.predict(<FILL_IN>)\n",
    "## Load the logged model from MLflow\n",
    "model_uri = f\"runs:/{run_id_single_node}/xgboost_model\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(<FILL_IN>)\n",
    "\n",
    "## Apply the UDF for inference on the distributed dataset\n",
    "predictions = df_with_features.withColumn(<FILL_IN>)))\n",
    "\n",
    "## Display the predictions along with original features and labels\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a678fb-b21d-4708-8498-627b86c740e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import pandas_udf, struct\n",
    "import mlflow.pyfunc\n",
    "\n",
    "## Perform inference on the test set using the trained model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "## Log the model with MLflow\n",
    "model_uri = f\"runs:/{run_id_single_node}/xgboost_model\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(spark, model_uri, result_type=\"double\")\n",
    "\n",
    "## Apply the UDF for inference on the distributed dataset\n",
    "predictions = df_with_features.withColumn(\"predicted_label\", predict_udf(struct(*feature_columns)))\n",
    "\n",
    "## Display the predictions along with original features and labels\n",
    "display(predictions.select(*feature_columns, \"label\", \"predicted_label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a64ee38-0080-4145-9a2c-fd6c0d8f6a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Distributed Model Deployment with Spark MLlib  \n",
    "\n",
    "In this task, you will train and deploy a **XGBRegressor** in a distributed environment using **Spark MLlib**. You will log the trained model with **MLflow** and perform distributed inference on the test dataset. Additionally, the predictions will be saved to a Delta table for further analysis. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Train a distributed XGBRegressor model** using Spark MLlib.\n",
    "2. **Log the trained model** in MLflow for future use.\n",
    "3. **Perform distributed inference** on the test dataset using the trained model.\n",
    "4. **Save predictions** to a Delta table in Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090d0fe2-828a-4255-99bb-62665f602ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.1: Training and Logging the XGBRegressor Model\n",
    "\n",
    "In this task, you will train an **XGBRegressor** model using the distributed Spark DataFrame. By preparing the features within Spark and converting to Pandas, you will leverage the power of both Spark for data processing and XGBoost for model training. After training, you will log the model in MLflow for version control and future use.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Prepare the features using **VectorAssembler** to transform the data for model training.\n",
    "- **Step 2:** Convert the training data into a Pandas DataFrame and train the **XGBRegressor** model.\n",
    "- **Step 3:** Log the trained model in **MLflow**, including its signature, to enable model tracking and reproducibility.\n",
    "- **Step 4:** Measure and print both the training and inference times to assess model performance.\n",
    "- **Step 5:** Save the trained model's URI in MLflow for easy reference in subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f96dd74d-749e-4214-a850-2cdd2d7ac0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Distributed Model Deployment with XGBRegressor using Spark MLlib\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import struct\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "## Set the MLflow registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "## Create a VectorAssembler to prepare the features\n",
    "assembler = VectorAssembler(<FILL_IN>)\n",
    "\n",
    "## Assemble the features in Spark DataFrame\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "## Split the data into training and test sets\n",
    "train_df, test_df = df_with_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Prepare the training data as Pandas DataFrame for XGBoost\n",
    "X_train = train_df.select(feature_columns).toPandas()\n",
    "y_train = train_df.select(\"label\").toPandas()\n",
    "X_test = test_df.select(feature_columns).toPandas()\n",
    "y_test = test_df.select(\"label\").toPandas()\n",
    "\n",
    "## Create the XGBRegressor model\n",
    "xgb_regressor = XGBRegressor(\n",
    "    <FILL_IN>\n",
    ")\n",
    "\n",
    "## Measure training time\n",
    "start_time = time.time()\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "training_time_distributed = time.time() - start_time\n",
    "print(f\"Training time (distributed): {training_time_distributed} seconds\")\n",
    "\n",
    "## Log the XGBRegressor model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the XGBRegressor model and infer its signature\n",
    "    signature = infer_signature(<FILL_IN>)\n",
    "    mlflow.xgboost.log_model(<FILL_IN>)\n",
    "    run_id_distributed = run.info.run_id\n",
    "\n",
    "## Measure inference time\n",
    "start_time = time.time()\n",
    "y_pred = xgb_regressor.predict(<FILL_IN>)\n",
    "inference_time_distributed = time.time() - start_time\n",
    "print(f\"Inference time (distributed): {inference_time_distributed} seconds\")\n",
    "\n",
    "## Save the trained model's URI for inference\n",
    "model_uri = f\"runs:/{run_id_distributed}/xgboost_model\"\n",
    "print(f\"Model saved at {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d545e8-940c-4b1a-89e5-0ada0f163c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Distributed Model Deployment with XGBRegressor using Spark MLlib\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import struct\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "## Set the MLflow registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "## Create a VectorAssembler to prepare the features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "## Assemble the features in Spark DataFrame\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "## Split the data into training and test sets\n",
    "train_df, test_df = df_with_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Prepare the training data as Pandas DataFrame for XGBoost\n",
    "X_train = train_df.select(feature_columns).toPandas()\n",
    "y_train = train_df.select(\"label\").toPandas()\n",
    "X_test = test_df.select(feature_columns).toPandas()\n",
    "y_test = test_df.select(\"label\").toPandas()\n",
    "\n",
    "## Create the XGBRegressor model\n",
    "xgb_regressor = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "## Measure training time\n",
    "start_time = time.time()\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "training_time_distributed = time.time() - start_time\n",
    "print(f\"Training time (distributed): {training_time_distributed} seconds\")\n",
    "\n",
    "## Log the XGBRegressor model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the XGBRegressor model and infer its signature\n",
    "    signature = infer_signature(X_train, xgb_regressor.predict(X_train))\n",
    "    mlflow.xgboost.log_model(xgb_regressor, \"xgboost_model\", signature=signature)\n",
    "    run_id_distributed = run.info.run_id\n",
    "\n",
    "## Measure inference time\n",
    "start_time = time.time()\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "inference_time_distributed = time.time() - start_time\n",
    "print(f\"Inference time (distributed): {inference_time_distributed} seconds\")\n",
    "\n",
    "## Save the trained model's URI for inference\n",
    "model_uri = f\"runs:/{run_id_distributed}/xgboost_model\"\n",
    "print(f\"Model saved at {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90eb18eb-ed73-477f-ba4b-127c9e29d3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.2: Performing Distributed Inference and Saving Predictions to Delta Table\n",
    "\n",
    "In this step, you will apply the trained **Gradient Boosted Tree (GBT)** model to the test dataset and perform distributed inference across the Spark cluster. The predictions will be saved to a Delta table in Unity Catalog for further analysis.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Convert the test dataset to a Pandas DataFrame and use the trained **XGBRegressor** model to perform inference.\n",
    "- **Step 2:** Combine the original test dataset with the predicted values and convert the results back to a Spark DataFrame.\n",
    "- **Step 3:** Save the predictions to a Delta table for future analysis.\n",
    "- **Step 4:** Explore the results by displaying the predictions alongside the actual labels, allowing for a comparison of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91818e5f-cb03-46a8-8396-f97f6c398719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Perform inference on the test dataset using the distributed XGBRegressor model\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Convert the test dataset to a Pandas DataFrame for prediction\n",
    "X_test = test_df.select(feature_columns).toPandas()\n",
    "\n",
    "## Perform inference using the trained XGBRegressor model\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "## Convert predictions to a Pandas DataFrame\n",
    "predictions_pd = <FILL_IN>\n",
    "\n",
    "## Combine the original test DataFrame with the predictions\n",
    "test_df_with_preds = test_df.toPandas()  # Convert test_df to Pandas DataFrame\n",
    "test_df_with_preds[\"predicted_label\"] = <FILL_IN>\n",
    "\n",
    "## Convert the combined DataFrame back to a Spark DataFrame\n",
    "predictions_spark_df =<FILL_IN>\n",
    "\n",
    "## Display predictions\n",
    "display(predictions_spark_df.select(*feature_columns, \"label\", \"predicted_label\"))\n",
    "\n",
    "## Save predictions to a Delta table in Unity Catalog\n",
    "predictions_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.distributed_xgb_predictions_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533b23fb-732e-42ff-9d9d-ec76fc7c38ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Perform inference on the test dataset using the distributed XGBRegressor model\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Convert the test dataset to a Pandas DataFrame for prediction\n",
    "X_test = test_df.select(feature_columns).toPandas()\n",
    "\n",
    "## Perform inference using the trained XGBRegressor model\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "## Convert predictions to a Pandas DataFrame\n",
    "predictions_pd = pd.DataFrame(y_pred, columns=[\"predicted_label\"])\n",
    "\n",
    "## Combine the original test DataFrame with the predictions\n",
    "test_df_with_preds = test_df.toPandas()  # Convert test_df to Pandas DataFrame\n",
    "test_df_with_preds[\"predicted_label\"] = predictions_pd[\"predicted_label\"]\n",
    "\n",
    "## Convert the combined DataFrame back to a Spark DataFrame\n",
    "predictions_spark_df = spark.createDataFrame(test_df_with_preds)\n",
    "\n",
    "## Display predictions\n",
    "display(predictions_spark_df.select(*feature_columns, \"label\", \"predicted_label\"))\n",
    "\n",
    "## Save predictions to a Delta table in Unity Catalog\n",
    "predictions_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.distributed_xgb_predictions_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "912c2236-79c7-4d20-b433-ca5da6f0fc37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Further Exploration:**  \n",
    "\n",
    "For more information on model tuning and deployment using Spark MLlib, refer to the [Spark MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3827fc40-bf58-4bb8-a058-893bf29c901d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Delta Lake Optimization Strategies\n",
    "\n",
    "In this section, you will explore how to optimize query performance and storage efficiency using Delta Lake features like **OPTIMIZE**, **VACUUM**, and **Liquid Clustering**. These optimization techniques will help compact data files, clean up unused files, and enhance query performance, especially for frequently accessed data.\n",
    "\n",
    "**Key Delta Lake Optimization Features**:\n",
    "- **OPTIMIZE**: Compacts small files into larger ones, improving query read performance.\n",
    "- **VACUUM**: Removes old, unused data files, optimizing storage and ensuring a clean data state.\n",
    "- **Z-ORDER Clustering**: Clusters data based on columns frequently used in queries, speeding up read times by organizing the data more efficiently.\n",
    "- **Liquid Clustering**: Automatically adjusts clustering over time based on query patterns for optimal performance.\n",
    "\n",
    "**Task Outline:**\n",
    "\n",
    "- **Step 1:** Apply **OPTIMIZE** to compact small files in the Delta table.\n",
    "- **Step 2:** Apply **VACUUM** to clean up old files and reclaim storage.\n",
    "- **Step 3:** Enable **Liquid Clustering** for automatic optimization.\n",
    "- **Step 4:** Measure query performance before and after optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5f5f29-20f5-4fd5-860b-7ddfe9ad911e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Enable Predictive Optimization for Your Account\n",
    "You must enable predictive optimization at the account level. You can then enable or disable predictive optimization at the catalog and schema levels.\n",
    "\n",
    "An account admin must complete the following steps to enable predictive optimization for all metastores in an account:\n",
    "\n",
    "* **Step 1:** Access the [Accounts Console](https://accounts.cloud.databricks.com/login).\n",
    "\n",
    "* **Step 2:** Navigate to **Settings**, then **Feature enablement**.\n",
    "\n",
    "* **Step 3:** Select **Enabled** next to **Predictive optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6105ee8-9065-4dab-866e-1c13c42bd277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.1: Applying OPTIMIZE to the Delta Table  \n",
    "\n",
    "In this task, you will apply **OPTIMIZE** on the Delta table to compact small files into larger ones. This operation reduces the overhead of reading many small files during queries, improving the overall performance of data reads.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Run the `OPTIMIZE` command on your Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c84fa2e-75dc-438b-bd71-1281ec570d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## OPTIMIZE the Delta table to compact small files\n",
    "spark.sql(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d01ed54-68a4-415b-a2b3-d90d6593a456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# OPTIMIZE the Delta table to compact small files\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {DA.catalog_name}.{DA.schema_name}.distributed_xgb_predictions_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb22d5b-9292-4718-9750-98bb7313c241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.2: Applying VACUUM to Clean Up Old Files\n",
    "\n",
    "The **VACUUM** operation removes old, unused data files from the Delta table that are no longer referenced by the latest version of the table. This helps reclaim storage space and ensures that only the necessary data files are retained.\n",
    "\n",
    "**Step:**  \n",
    "- **Step 1:** Run the VACUUM operation to remove old, unused data files from your Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af045be8-e167-4c9c-b2b0-2205f7706c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Disable the retention duration check and apply VACUUM\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "## Apply VACUUM to clean up old files and reclaim storage space\n",
    "display(spark.sql(f\"\"\"\n",
    "<FILL_IN>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a8fa80c-04e5-4ab5-b61a-b5addcf7a4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Disable the retention duration check and apply VACUUM\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# Apply VACUUM to clean up old files and reclaim storage space\n",
    "display(spark.sql(f\"\"\"\n",
    "VACUUM {DA.catalog_name}.{DA.schema_name}.distributed_xgb_predictions_table RETAIN 0 HOURS\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef3ac44-6355-4396-933d-0b841dd3f910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more information on distributed machine learning with Spark, visit [Spark MLlib's Guide](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a5a80a6-de66-45ab-8bea-e20108ee0d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3: Enabling Liquid Clustering\n",
    "\n",
    "**Liquid Clustering** is an advanced Delta Lake feature that dynamically clusters your data based on the query patterns it observes over time. This ensures that frequently queried columns are automatically clustered, leading to faster reads.\n",
    "\n",
    "**Steps:**  \n",
    "- **Step 1:** Enable **Liquid Clustering** on your Delta table by clustering it based on frequently queried columns.\n",
    "- **Step 2:** Query the Delta table's history to check the clustering progress after enabling Liquid Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21e6cb0-c728-4d5b-b7de-127b2c38f873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Enable Liquid Clustering on the Delta table, clustering by frequently queried columns\n",
    "ALTER TABLE <FILL_IN>\n",
    "CLUSTER BY (<FILL_IN>);\n",
    "\n",
    "-- Display the improvement after applying liquid clustering\n",
    "DESCRIBE <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48feb8f-d715-452d-ae59-66eb8bc4319a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sql\n",
    "-- Enable Liquid Clustering on the Delta table, clustering by frequently queried columns\n",
    "ALTER TABLE distributed_xgb_predictions_table\n",
    "CLUSTER BY (label);\n",
    "\n",
    "-- Display the improvement after applying liquid clustering\n",
    "DESCRIBE HISTORY distributed_xgb_predictions_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a3f105-cef9-455b-a82e-e0714ce16ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.4: Measuring Query Performance Before and After Optimization  \n",
    "\n",
    "To truly understand the impact of these optimizations, we will measure the query performance before and after applying the Delta Lake optimizations. This will help you observe the improvements in query speed and resource efficiency.\n",
    "\n",
    "**Steps:**  \n",
    "- **Step 1:** Run the same query on your Delta table before and after applying the optimizations.\n",
    "- **Step 2:** Measure the time taken for the query before optimization.\n",
    "- **Step 3:** Apply **OPTIMIZE** and **VACUUM**.\n",
    "- **Step 4:** Measure the query performance again and compare the improvement in speed.\n",
    "- **Step 5:** Display the performance improvement percentage and discuss how the Delta Lake optimizations affected the query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b166c150-51ba-411c-b38a-e97ad6dac1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "## Define the table and query\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.distributed_dt_predictions_table\"\n",
    "\n",
    "## Define the query you will run before and after optimizations\n",
    "query = <FILL_IN>\n",
    "\n",
    "## Measure query performance before optimization\n",
    "start_time_before = time()\n",
    "df_before = spark.sql(query)\n",
    "time_before = time() - start_time_before\n",
    "print(f\"Query Execution Time Before Optimization: {time_before:.2f} seconds\")\n",
    "\n",
    "## Apply Delta Lake optimizations (OPTIMIZE and VACUUM were already applied earlier)\n",
    "\n",
    "## Measure query performance after optimization\n",
    "start_time_after = time()\n",
    "df_after = spark.sql(query)\n",
    "time_after = <FILL_IN>\n",
    "print(f\"Query Execution Time After Optimization: {time_after:.2f} seconds\")\n",
    "\n",
    "## Calculate the performance improvement\n",
    "performance_improvement = <FILL_IN> / time_before * 100\n",
    "print(f\"Performance improvement after optimizations: {performance_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b482a3b-3e30-4f76-b350-9aefe813f103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from time import time\n",
    "\n",
    "## Define the table and query\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.distributed_xgb_predictions_table\"\n",
    "\n",
    "## Define the query you will run before and after optimizations\n",
    "query = f\"\"\"\n",
    "SELECT label, COUNT(*)\n",
    "FROM {table_name}\n",
    "GROUP BY label\n",
    "\"\"\"\n",
    "\n",
    "## Measure query performance before optimization\n",
    "start_time_before = time()\n",
    "df_before = spark.sql(query)\n",
    "time_before = time() - start_time_before\n",
    "print(f\"Query Execution Time Before Optimization: {time_before:.2f} seconds\")\n",
    "\n",
    "## Apply Delta Lake optimizations (OPTIMIZE and VACUUM were already applied earlier)\n",
    "\n",
    "## Measure query performance after optimization\n",
    "start_time_after = time()\n",
    "df_after = spark.sql(query)\n",
    "time_after = time() - start_time_after\n",
    "print(f\"Query Execution Time After Optimization: {time_after:.2f} seconds\")\n",
    "\n",
    "## Calculate the performance improvement\n",
    "performance_improvement = (time_before - time_after) / time_before * 100\n",
    "print(f\"Performance improvement after optimizations: {performance_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68bbb5d0-f4c2-495d-a69b-91a2af3ed58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you successfully explored various methods for deploying machine learning models using Spark, including single-node and distributed deployment strategies. You gained hands-on experience with:\n",
    "\n",
    "- Preparing data for model training and deployment using the Wine Quality dataset.\n",
    "- Training and deploying a single-node machine learning model using Scikit-learn, followed by leveraging **Spark UDFs** for parallelized inference on distributed data.\n",
    "- Training and deploying a distributed machine learning model using **Spark MLlib**, followed by performing distributed inference and logging the model with MLflow.\n",
    "- Applying **Delta Lake optimizations** such as **OPTIMIZE**, **VACUUM**, and **Liquid Clustering** to enhance query performance, reduce storage costs, and improve overall resource efficiency.\n",
    "\n",
    "Finally, you compared the query performance before and after these optimizations, observing how Delta Lake's features significantly improved the speed and efficiency of your model serving pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c861f0-ac55-4b98-ab22-3cafda7875cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "03.3Lab - Model Deployment with Spark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
