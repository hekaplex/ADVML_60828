{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1579723-4e79-45e6-9926-fb34cb893942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bd796d-1be1-47e7-868a-8ef4ef1f5c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demonstration - Monitoring Model Quality\n",
    "\n",
    "In this demonstration, we will investigate how Databricks enables drift detection using various tools and features such as Notebooks, Queries, Dashboards, Alerts, while emphasizing Lakehouse Monitoring. Lakehouse Monitoring is leveraged to monitor the quality and statistical properties of data and ML models within the Databricks Lakehouse architecture. It allows you to track data quality, detect anomalies, and monitor the performance of ML models and model-serving endpoints.\n",
    "\n",
    "As a part of the classroom setup, we will bring in a pre-generated inference table (with Change Data Feed enabled) utilizing Databricks' Marketplace as well as a baseline dataset to help demonstrate how we can detect and investigate drift. In practice, it is recommended that Delta Lake's Change Data Feed (CDF), which allows Databricks to track row-level changes between versions of a Delta table, be enabled on inference tables by default. By leveraging CDF, data scientists can more efficiently create and monitor inference tables. \n",
    "\n",
    "\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this demonstration, you will be able to do the following:\n",
    "\n",
    "- Understand how to apply Lakehouse Monitoring for detecting different types of drift. Here are the core concepts and features:\n",
    "    - Automated Dashboards built to analyze an inference profile\n",
    "    - Know various ways in which you can locate the dashboard (Workspace,  and Lineage and Quality in Catalog Explorer)\n",
    "    - Know various ways in which you can create additional widgets in the dashboard\n",
    "    - Automate profile and drift metric tables\n",
    "    - Out-of-the-box metrics and custom metrics\n",
    "    - Inspect inference profile lineage and create alerts for sending notifications when metrics change\n",
    "    - Using Lakehouse Monitoring to build custom for analyzing possible drift. \n",
    "        - Review of various types of drift and Lakehouse Monitoring can be applied\n",
    "- Understand additional configurations and features using the UI\n",
    "    - How to Update a baseline feature table\n",
    "    - Update slicing configurations\n",
    "\n",
    "**Note: This demonstration uses a mixture of SQL and Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84cdb6c-de45-4732-bf79-0673031910a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4da0fc7-7f9c-4dda-9a12-c10ad0389a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n",
    "\n",
    "\n",
    "* To utilize the generated Dashboard, you will need a SQL Warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c972e5-e93f-4f2e-8c80-cbd51d2c8ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "To get into the lesson, we first need to build some data assets and define some configuration variables required for this demonstration. When running the following cell, the output is hidden so our space isn't cluttered. To view the details of the output, you can hover over the next cell and click the eye icon. \n",
    "\n",
    "The cell after the setup, titled `View Setup Variables`, displays the various variables that were created. You can click the Catalog icon in the notebook space to the right to see that your catalog was created with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7803f063-161c-41d2-b340-e7b01c2e9755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1dc1724-da36-48c9-893e-8e66fd46082c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ce5583-a727-48ff-8ce3-8d757e6e894d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference and Feature Tables\n",
    "\n",
    "As a part of the classroom setup, we have created two tables:\n",
    "1. `model_logs`: This is our inference table. It contains inference data from a sequence of queries that were made prior to this demo and will act as our dataset that is drifting away from the baseline data `(You can search for the dataset in the Marketplace by the name: Inference Dataset for Machine Learning Operations Associate Course.)`. \n",
    "1. `baseline_features`: This is our baseline table and acts as our source of truth.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**A Remark on Inference Tables and Inference Profiles**\n",
    "\n",
    "It's important to distinguish between an inference table when thinking about the assets we will be working with. An inference table can be thought of as a collection of logs from the model that contain the input features at runtime and predicted labels. You can also include the true labels if they are available. On the other hand, an inference profile is an inference table that also contains profile metrics like F1-score, accuracy, or other evaluation metrics that measure the performance of the model on the inferred data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37dc8942-425a-4a67-9b2f-3a322dc0ce59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "primary_table_name = 'model_logs'\n",
    "baseline_table_name = 'baseline_features'\n",
    "\n",
    "primary_df = spark.read.format('delta').table(primary_table_name)\n",
    "primary_pdf = primary_df.toPandas()\n",
    "\n",
    "baseline_df = spark.read.format('delta').table(baseline_table_name)\n",
    "baseline_pdf = baseline_df.toPandas()\n",
    "\n",
    "\n",
    "inference_df = spark.read.format('delta').table('model_inference_table')\n",
    "\n",
    "\n",
    "\n",
    "display(baseline_df)\n",
    "display(primary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f571e2-0f06-4c77-8f80-32af8d7913dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Motivating Drift Investigation\n",
    "We'll take a cross-sectional snapshot of the distributions of BMI and Diabete_binary data to compare the ground truth to the data in our inference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c15af13-c4d9-4f71-9dde-58ce5d8d4e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare distributions for a feature\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Randomly sample from the pandas dataframe baseline_pdf with samples equal to the size of primary_pdf\n",
    "baseline_pdf_sample = baseline_pdf.sample(n=len(primary_pdf), random_state=1)\n",
    "\n",
    "\n",
    "# KS test\n",
    "stat, p_value = stats.ks_2samp(baseline_pdf_sample['BMI'], primary_pdf['BMI'])\n",
    "\n",
    "# Visualization 1\n",
    "plt.hist(baseline_pdf_sample['BMI'], bins=30, alpha=0.5, label=\"Training Data\")\n",
    "plt.hist(primary_pdf['BMI'], bins=30, alpha=0.5, label=\"Logged Data\")\n",
    "plt.legend()\n",
    "plt.title(\"BMI Distribution: Training vs Logged (Drift Detected)\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2\n",
    "plt.hist(baseline_pdf_sample['labeled_data'], bins=30, alpha=0.5, label=\"Training Data\")\n",
    "plt.hist(primary_pdf['Diabetes_binary'], bins=30, alpha=0.5, label=\"Predicted Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Target Distribution: Training vs Logged (Drift Detected)\")\n",
    "plt.show()\n",
    "\n",
    "print(f'KS Test statistic value: {stat} and p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c3a8c8-2cd3-49e2-8e91-cd218bd0cfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakehouse Monitoring\n",
    "\n",
    "To monitor the performance of the model we used to build our premade `model_logs` table, we will attach the monitor to this table. Our baseline table `baseline_features` is what we can consider as our *good* or *source of truth* data. The Databricks monitor creates out-of-the-box metrics that are important and sets up two tables: \n",
    "\n",
    "1. Profile metrics table: This keeps track of profile metrics, which is a summary statistic computed for each column. \n",
    "1. Drift metrics table:  This keeps track of changes in profile metrics compared to baseline or previous time window. \n",
    "\n",
    "These two tables are used to automatically generate a monitoring dashboard. Later, we will uses these tables to build an alert for drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e58e24-5dc8-4116-8bc4-ff836b922af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 1: Using the UI\n",
    "\n",
    "Let's first start with recalling how we can use the UI to setup with Lakehouse Monitoring\n",
    "\n",
    "To add a monitor on the inference table, \n",
    "\n",
    "1. Open the **Catalog** menu from the left menu bar.\n",
    "\n",
    "1. Select the table **model_logs** within your catalog and schema. \n",
    "\n",
    "1. Click on the **Quality** tab then on the **Enable** button.\n",
    "\n",
    "1. After clicking Enable, select **Configure**.\n",
    "\n",
    "1. As **Profile type** select **Inference profile**.\n",
    "\n",
    "1. As **Problem type** select **classification**.\n",
    "\n",
    "1. As the **Prediction column** select **Diabetes_binary**.\n",
    "\n",
    "1. As the **Label column** select **labeled_data**\n",
    "\n",
    "1. As **Metric granularities** select **5 minutes**, **1 hour**, and **1 day**. We will use the doctored timestamps to simulate requests that have been received over a large period of time. \n",
    "\n",
    "1. As **Timestamp column** select **timestamp**.\n",
    "\n",
    "1. As **Model ID column** select **model_id**.\n",
    "\n",
    "1. In Advanced Options --> **Unity catalog baseline table name (optional)** enter **Your baseline table name** from above along with your catalog and schema(Eg: catalog.schema.table_name).\n",
    "\n",
    "1. Click the **Create** button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b501f0-9562-4d1a-8b0d-d328abc4c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2: Pythonic Approach\n",
    "For a more programmatic approach, you can run the following code to initiate Lakehouse Monitoring. This method allows you to automate and customize the monitoring according to specific needs and thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e803ac-2549-4b28-bd65-d85b45bb2f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import MonitorInferenceLog, MonitorInferenceLogProblemType, MonitorInfoStatus, MonitorRefreshInfoState, MonitorMetric\n",
    "w = WorkspaceClient()\n",
    "full_primary_table_name = f'{DA.catalog_name}.{DA.schema_name}.{primary_table_name}'\n",
    "full_baseline_table_name = f\"{DA.catalog_name}.{DA.schema_name}.baseline_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299fa374-1489-4429-bca3-d3ed3c49487f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "help(w.quality_monitors.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a937633-421e-4e31-8432-c10ba74ef43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ML problem type, either \"classification\" or \"regression\"\n",
    "PROBLEM_TYPE = MonitorInferenceLogProblemType.PROBLEM_TYPE_CLASSIFICATION\n",
    "\n",
    "# Window sizes to analyze data over\n",
    "GRANULARITIES = [\"1 day\"]   \n",
    "\n",
    "# Directory to store generated dashboard\n",
    "ASSETS_DIR = f\"/Workspace/Users/{DA.username}/databricks_lakehouse_monitoring/{primary_table_name}\"\n",
    "\n",
    "# Optional parameters\n",
    "SLICING_EXPRS = [\"Age < 2\", \"Age > 15\", \"Sex = 1\", \"HighChol = 1\"]   # Expressions to slice data with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24b75cb-7930-4577-832c-cc40742a4005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Creating monitor for model_logs\")\n",
    "try: \n",
    "  info = w.quality_monitors.create(\n",
    "    table_name=full_primary_table_name,\n",
    "    inference_log=MonitorInferenceLog(\n",
    "      timestamp_col='timestamp',\n",
    "      granularities=GRANULARITIES,\n",
    "      model_id_col='model_id',  # Model version number \n",
    "      prediction_col='Diabetes_binary',  # Ensure this column is of type DOUBLE\n",
    "      problem_type=PROBLEM_TYPE,\n",
    "      label_col='labeled_data'  # Ensure this column is of type DOUBLE\n",
    "    ),\n",
    "    baseline_table_name=full_baseline_table_name,\n",
    "    slicing_exprs=SLICING_EXPRS,\n",
    "    output_schema_name=f\"{DA.catalog_name}.{DA.schema_name}\",\n",
    "    assets_dir=ASSETS_DIR\n",
    "  )\n",
    "\n",
    "  import time\n",
    "\n",
    "  # Wait for monitor to be created\n",
    "  while info.status ==  MonitorInfoStatus.MONITOR_STATUS_PENDING:\n",
    "    info = w.quality_monitors.get(table_name=full_primary_table_name)\n",
    "    time.sleep(10)\n",
    "\n",
    "  assert info.status == MonitorInfoStatus.MONITOR_STATUS_ACTIVE, \"Error creating monitor\"\n",
    "except Exception as e:\n",
    "  print(f\"Error creating monitor: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5641552e-c925-44b1-9c0d-5e75ae688b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A metric refresh will automatically be triggered on creation\n",
    "refreshes = w.quality_monitors.list_refreshes(table_name=full_primary_table_name).refreshes\n",
    "assert(len(refreshes) > 0)\n",
    "\n",
    "run_info = refreshes[0]\n",
    "while run_info.state in (MonitorRefreshInfoState.PENDING, MonitorRefreshInfoState.RUNNING):\n",
    "  run_info = w.quality_monitors.get_refresh(table_name=full_primary_table_name, refresh_id=run_info.refresh_id)\n",
    "  print(run_info)\n",
    "  time.sleep(30)\n",
    "\n",
    "assert run_info.state == MonitorRefreshInfoState.SUCCESS, \"Monitor refresh failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c24839c-4f8f-44b3-b5e9-2b9908052de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w.quality_monitors.get(table_name=full_primary_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1478372c-0f87-4981-bf42-64d6290be182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore the Lakehouse Monitoring Dashboard\n",
    "\n",
    "Methods for accessing the dashboard:\n",
    "1. Run the next cell and click on the generated link. This will take you to the `model_logs` in the Catalog Explorer.\n",
    "1. Select Quality and click on View Dashboard.  \n",
    "1. The Dashboard (and other assets) are automatically registered to the lineage of `model_logs` as a part of comprehensive and automated tracking with Unity Catalog. \n",
    "1. Setting the `ASSETS_DIR` gives the location for where the generated dashboards are actually stored. In this demo, this is set to `f\"/Workspace/Users/{DA.username}/databricks_lakehouse_monitoring/{primary_table_name}\"`. \n",
    "\n",
    "The dashboard provides a comprehensive view of the data quality and statistical properties of the tables being monitored. It helps in tracking data integrity, statistical distribution, and data drift over time. This is where you will prebuilt widgets that incorporate the metrics generated from the profile and drift metric tables. The dashboard should be thought of as a starting point for monitoring for drift, hence it is fully customizable. \n",
    "\n",
    "- At the top left you can toggle between viewing the **Canvas** or the underlying **Data** that was generated with dashboard using SQL logic. Note that if a widget is currently being used, then the dataset created cannot be deleted. \n",
    "- Select widget that reads # Inferences on the left side of the canvas\n",
    "    - Click on the kebab and select View Dataset. This will take you to the generated SQL code that you can use for further analysis with the SQL Editor or in this notebook.\n",
    "    - Try editing the code and clicking Run to see how you can analyze the underlying tables generated as a part of the Dashboard creation.\n",
    "- Back in the canvas, notice some metrics may appear as `null` or indicate **no data available**. This is expected since we are working with a sample dataset, since the automatically generated SQL code has logic that might not agree with this dataset such as comparing only baseline features or compute metrics on categorical features, which we are not considering here. If a baseline table was provided, drift metrics will be monitored. \n",
    "- For example, scroll down to the **Numerical Feature Drift** section and inspect the code for **# Features with Numerical Drift**. Notice that this SQL code only investigates the last window from the drift table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff58435-9fe4-4c63-bd87-dbf95fec82b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract workspace URL\n",
    "workspace_url = spark.conf.get('spark.databricks.workspaceUrl')\n",
    "\n",
    "# Construct the monitor dashboard URL\n",
    "monitor_dashboard_url = f\"https://{workspace_url}/explore/data/{DA.catalog_name}/{DA.schema_name}/model_logs?o={DA.schema_name}&activeTab=quality\"\n",
    "\n",
    "print(f\"Monitor Dashboard URL: {monitor_dashboard_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db66c52-4767-4181-b4fc-f8c2c986af0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸš¨ Instructor Note - Model Id Parameter Setup in in Data Tab of Dashboard**\n",
    "\n",
    "If you see â€œMissing selection for parameter: Model Idâ€ in the dashboard, this happens because the Model Id filter is not automatically applied to all widgets.\n",
    "\n",
    "To fix this:\n",
    "\n",
    "- In the dashboard toolbar, find the Model Id field at the top.\n",
    "- Manually type * (asterisk) in the Model Id box - this selects all model IDs.\n",
    "- Press Enter or click Run to refresh the dashboard.\n",
    "\n",
    "ðŸ’¡ Typing `*` tells the dashboard to include every available Model Id, so all widgets and queries render correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a704da-8ac9-484f-969e-92e7f137a162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "> **âš ï¸ Note:**  \n",
    "> You may encounter a `CAST_INVALID_INPUT` error in the **Numerical Feature Quantile Drift** widget.  \n",
    "> This occurs because the auto-generated query attempts to cast the string `\"Baseline\"` to a `TIMESTAMP` type in the `Window` column.  \n",
    "> While this error does not affect other metrics or drift detection functionality, you can optionally resolve it using the steps below.\n",
    "\n",
    "**Steps to Diagnose and Fix the Error (Optional)**\n",
    "- **Step 1:**  \n",
    "  Navigate to the **Numerical Feature Quantile Drift** widget in the monitoring dashboard.  \n",
    "  If there's a rendering failure, you will see a `CAST_INVALID_INPUT` error message displayed within the widget.\n",
    "- **Step 2:**  \n",
    "  Click the **kebab menu (â‹®)** in the top-right corner of the widget and select **View Dataset**.  \n",
    "  This opens the SQL query that powers the visual.\n",
    "- **Step 3:**  \n",
    "  In the dataset editor, scroll down and click **Diagnose error**.  \n",
    "  The Databricks Assistant will analyze the issue and provide a suggested fix.\n",
    "- **Step 4:**  \n",
    "  Click **Replace active cell content** to apply the Assistantâ€™s suggested query.  \n",
    "  The corrected query will ensure the `Window` column in the baseline portion uses a proper timestamp type, typically by replacing `\"Baseline\"` with `CAST(NULL AS TIMESTAMP)`.\n",
    "- **Step 5:**  \n",
    "  Click **Run** to execute the updated query.\n",
    "- **Step 6:**  \n",
    "  Once the query runs successfully, the widget should render correctly, and the error message will disappear.\n",
    "This fix helps ensure type consistency in the `UNION` clause across datasets, resolving the rendering error in the visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94098bd8-7f7b-42c6-8fd2-eb757e5d8176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a New Visual\n",
    "\n",
    "### Clone Method\n",
    "\n",
    "- Click the kebab on the **Inference Load Over Time**\n",
    "- Click **Clone**\n",
    "- Click on the new visual\n",
    "    - Notice the **Widget** pane has been automatically populated with the same configuration as the cloned visual.\n",
    "- Under Visualization on the widget configuration menu to the right, select **Counter** from the dropdown menu. \n",
    "- Click on the dropdown menu for the Value and select **Count** under Field and select **SUM** under **Transform**\n",
    "- Click on the dropdown menu under **Target** and select Window under Field and select **COUNT DISTINCT**\n",
    "- Double click on the title to change the title to **# Inferences and Intervals Monitored**. \n",
    "\n",
    "You will see a new counter displayed now that shows the total inference load over time and how many distinct windows there are. You can also resize the visual by dragging the corner inward or outward. \n",
    "\n",
    "- Select the kebab again\n",
    "- Here you can download the data for this visualization and copy a link to the widget if you would like to share it with your team. \n",
    "\n",
    "### Custom Visual\n",
    "\n",
    "Additionally, you build your own visual.\n",
    "- At the bottom of the canvas, there is a blue box that contains **Select** **Add a visualization**, **Add a textbox**, and **Add a filter**. Select **Add a visualization**. \n",
    "- In the visualization editor on the right of your screen:\n",
    "        - Select **profile_over_time** under **Dataset**\n",
    "        - Select **Counter** under **Visualization**\n",
    "        - Select **f1_score** under **Field** and **COUNT** under **Transform**\n",
    "        - Select **Title** under **Widget** and give it title **# F1-Scores Over Time**\n",
    "- Finally, inspect the SQL generated for this widget. \n",
    "\n",
    "### Additional Customization\n",
    "\n",
    "You can also build custom SQL code to build an even more customized visual by select **Create from SQL** at the very bottom of the Datasets pane to the left in the **Data** tab. Note this requires a detailed understanding of the profile and drift metrics tables and your specific goals for creating the new dataset(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fc37c6-465d-4a22-a2c9-ef0c9dc8972b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore the 2 Metric Tables\n",
    "\n",
    "Now that we have inspected the automatically generated dashboard, let's take a look at the two Delta tables that built this dashboard. \n",
    "\n",
    "1. **Navigate to the Catalog Explorer**:\n",
    "   - On the left-hand side of the Databricks workspace, click on **Catalog Explorer**.\n",
    "\n",
    "1. **Locate Your Catalog**:\n",
    "   - Find `dbacademy` catalog and select the `schema` that you are currently working in.\n",
    "\n",
    "1. **Explore the New Tables**:\n",
    "   - Look for two new tables:\n",
    "     - `model_logs_drift_metrics`\n",
    "     - `model_logs_profile_metrics`\n",
    "   - Click on each table to explore the metrics that were calculated.\n",
    "\n",
    "1. **Note on Metrics**:\n",
    "   - Similar to the **Model Logs Dashboard**, some metrics may appear as `null` depending on the column's characteristics or data availability. For example, we have no categorical features within this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd78f68-2cae-43ce-b670-636f2cac7f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A Closer Look at Out-Of-The-Box Metrics\n",
    "\n",
    "The profile metrics table and drift metrics table created by Databricks Lakehouse Monitoring contain different types of information to help monitor and analyze data.\n",
    "\n",
    "### Profile Metrics Table:\n",
    "1. **Summary Statistics:**  \n",
    "   Contains summary statistics for each column, such as count, number of null values, distinct count, minimum, maximum, average, standard deviation, and more.\n",
    "\n",
    "2. **Time Window and Slices:**  \n",
    "   Metrics are computed for specified time intervals (windows) and for each data slice defined by slicing expressions.\n",
    "\n",
    "3. **Grouping Columns:**  \n",
    "   Includes columns like time window, granularity, log type (input or baseline table), slice key and value, and model ID (for InferenceLog analysis).\n",
    "\n",
    "4. **Model Accuracy Metrics:**  \n",
    "   For InferenceLog analysis, it includes metrics like accuracy score, confusion matrix, precision, recall, and F1 score.\n",
    "\n",
    "5. **Fairness and Bias Statistics:**  \n",
    "   For classification models, it includes metrics like predictive parity, predictive equality, equal opportunity, and statistical parity.\n",
    "\n",
    "### Drift Metrics Table:\n",
    "1. **Drift Statistics:**  \n",
    "   Tracks changes in distribution for a metric, comparing either to a previous time window (consecutive drift) or to a baseline distribution (baseline drift).\n",
    "\n",
    "2. **Comparison Windows:**  \n",
    "   Includes columns for the current and comparison time windows.\n",
    "\n",
    "3. **Drift Types:**  \n",
    "   Indicates whether the drift is compared to the previous window or a baseline table.\n",
    "\n",
    "4. **Metrics Columns:**  \n",
    "   Includes differences in count, average, percent null, percent zeros, percent distinct, and other statistical tests like:\n",
    "   - Chi-squared test\n",
    "   - KS test\n",
    "   - Total variation distance\n",
    "   - L-infinity distance\n",
    "   - Jensen-Shannon distance\n",
    "   - Wasserstein distance\n",
    "   - Population stability index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d64332-c275-46b1-91cd-7522807fcea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display profile metrics table\n",
    "profile_table = f\"{DA.catalog_name}.{DA.schema_name}.model_logs_profile_metrics\"\n",
    "profile_df = spark.sql(f\"SELECT * FROM {profile_table}\")\n",
    "display(profile_df.orderBy(F.rand()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b6f454-00f0-4203-bc75-8a6cf2d70576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display profile metrics table\n",
    "drift_table = f\"{DA.catalog_name}.{DA.schema_name}.model_logs_drift_metrics\"\n",
    "drift_table_df = spark.sql(f\"SELECT * FROM {drift_table}\")\n",
    "display(drift_table_df.orderBy(F.rand()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8498cd1f-4779-42be-8caa-0b293dfc4684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's inspect the profile metrics table and the drift metrics table. Notice that the value of a cell will read `Null` when the metric does not apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d58453b-2b13-492b-8d39-e96bd171a0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Utilizing Profile Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655b6bd9-6172-40eb-8c24-e8627f927f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fb_cols = [\"window\", \"model_id\", \"slice_key\", \"slice_value\", \"predictive_parity\", \"predictive_equality\", \"equal_opportunity\", \"statistical_parity\"]\n",
    "fb_metrics_df = profile_df.select(fb_cols).filter(f\"column_name = ':table' AND slice_value = 'true'\")\n",
    "display(fb_metrics_df.orderBy(F.rand()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7cca20-a7b2-446f-9cbf-97b13eae24e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the profile metrics data from the Delta table\n",
    "profile_table = f\"{DA.catalog_name}.{DA.schema_name}.model_logs_profile_metrics\"\n",
    "profile_metrics_df = spark.read.table(profile_table)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "data = profile_metrics_df.toPandas()\n",
    "\n",
    "# Extract and format 'window' column (start and end timestamps)\n",
    "def process_window_column(row):\n",
    "    if isinstance(row, dict):\n",
    "        return {\n",
    "            \"start\": pd.to_datetime(row.get(\"start\", None)),\n",
    "            \"end\": pd.to_datetime(row.get(\"end\", None))\n",
    "        }\n",
    "    elif isinstance(row, str):  # If the row is still serialized as a string\n",
    "        try:\n",
    "            row_dict = json.loads(row)\n",
    "            return process_window_column(row_dict)\n",
    "        except Exception:\n",
    "            return {\"start\": None, \"end\": None}\n",
    "    else:\n",
    "        return {\"start\": None, \"end\": None}\n",
    "\n",
    "# Apply the processing function to the 'window' column\n",
    "data['window'] = data['window'].apply(process_window_column)\n",
    "\n",
    "# Extract start and end into separate columns for clarity\n",
    "data['window_start'] = data['window'].apply(lambda x: x['start'])\n",
    "data['window_end'] = data['window'].apply(lambda x: x['end'])\n",
    "\n",
    "# Drop rows with null or empty F1-scores\n",
    "data = data.dropna(subset=['f1_score'])\n",
    "data = data.dropna(subset =['accuracy_score'])\n",
    "\n",
    "# Extract relevant metrics\n",
    "# Replace 'f1_score' with the actual column name for F1-score in your table\n",
    "f1_scores = data[['window_start', 'window_end', 'f1_score', 'accuracy_score']]\n",
    "\n",
    "# Display F1-scores over time\n",
    "print(\"F1-scores over time (non-empty values):\")\n",
    "display(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a261d86-e88f-4cc6-9922-e0b61764d9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Utilizing Drift Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ca09bf-1605-4dab-b68d-8c916aaaf360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from model_logs_drift_metrics\n",
    "where drift_type = 'BASELINE' AND slice_key is null and column_name = 'BMI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8572f33c-c724-426e-9f9d-9adddbeda1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the drift metrics data from the Delta table\n",
    "drift_table = f\"{DA.catalog_name}.{DA.schema_name}.model_logs_drift_metrics\"\n",
    "drift_metrics_df = spark.read.table(drift_table)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "data = drift_metrics_df.toPandas()\n",
    "\n",
    "# Convert Timestamp objects to strings in 'window' and 'window_cmp'\n",
    "def convert_timestamp_to_string(d):\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, pd.Timestamp):\n",
    "                d[k] = v.isoformat()\n",
    "            elif isinstance(v, dict):\n",
    "                d[k] = convert_timestamp_to_string(v)\n",
    "    return d\n",
    "\n",
    "data['window'] = data['window'].apply(convert_timestamp_to_string)\n",
    "data['window_cmp'] = data['window_cmp'].apply(convert_timestamp_to_string)\n",
    "\n",
    "# Ensure JSON fields are strings\n",
    "data['window'] = data['window'].apply(json.dumps)\n",
    "data['window_cmp'] = data['window_cmp'].apply(json.dumps)\n",
    "\n",
    "# Convert the JSON string in 'window' and 'window_cmp' to dictionaries\n",
    "for index, row in data.iterrows():\n",
    "    row['window'] = json.loads(row['window'])\n",
    "    row['window_cmp'] = json.loads(row['window_cmp'])\n",
    "\n",
    "# Analyze the drift metrics\n",
    "drift_thresholds = {\n",
    "    \"ks_test\": 0.6,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_drift(row):\n",
    "    ks_test_value = row['ks_test'].get('statistic') if isinstance(row['ks_test'], dict) else row['ks_test']\n",
    "    if ks_test_value is not None and ks_test_value > drift_thresholds['ks_test']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "data['drift_detected'] = data.apply(check_drift, axis=1)\n",
    "\n",
    "# Display rows with drift detected\n",
    "drifted_rows = data[data['drift_detected']]\n",
    "no_drifted_rows = data[~data['drift_detected']]\n",
    "\n",
    "print(\"Rows with drift detected:\")\n",
    "display(drifted_rows)\n",
    "\n",
    "print(\"\\nRows with no drift detected:\")\n",
    "display(no_drifted_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdd1962-6d63-4c9b-ab83-886e4b8dc926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Lineage and Alerts for Drift Detection\n",
    "\n",
    "The following instructions are not meant to send you an alert to any device. It is only to demonstrate how to use lineage alongside alerts for your metric tables.\n",
    "\n",
    "1. **Navigate to Queries**:\n",
    "   - On the left-hand sidebar of the Databricks workspace, click on **Queries**.\n",
    "\n",
    "1. **Create a Query**:\n",
    "   - Click on **Create Query** in the top-right corner.\n",
    "   - Write a query to calculate the maximum value of the median BMI value (this median value is computed per window):\n",
    "\n",
    "         select\n",
    "         max(median) as largest_median\n",
    "         FROM model_logs_profile_metrics\n",
    "         WHERE column_name = 'BMI'\n",
    "         GROUP BY column_name;\n",
    "   - Give the Query a name like `Drift Alert 1`\n",
    "   > ****Note:**** Before running the query, make sure to set the current catalog and schema, and follow the `catalog.schema.table_name` structure.\n",
    "   - Once the query is complete, click **Save** to save it.\n",
    "\n",
    "1. **Create an Alert**:\n",
    "   - Navigate to **Alerts** on the left-hand sidebar and click **Create Alert**.\n",
    "   - Provide the alert with a **name** like `Test alert`.\n",
    "   - In the SQL editor panel, paste the query you have saved before.\n",
    "> ****Note:**** Before running the query, make sure to set the current catalog and schema, and follow the `catalog.schema.table_name` structure.\n",
    "\n",
    "   - Click the **Run all** button to validate the query.\n",
    "   - Define a **Trigger Condition**:\n",
    "     - Example: Trigger the alert when the value is above 50 (50 is the max).\n",
    "   - Click **Test condition**.\n",
    "     - If the result satisfies the condition, youâ€™ll see:`Triggered: Alert will trigger based on current data.`\n",
    "   - Under **Notify**, search and add users or destinations. \n",
    "   - Finally, click **View alert** and Set the **Schedule**.\n",
    "![alert](../Includes/images/alert.png)\n",
    "\n",
    "1. **Monitor Lineage and Updates**:\n",
    "   - Go back to the **Catalog** on the left sidebar.\n",
    "   - Navigate to your **model_logs_profile_metrics** table.\n",
    "   - Click on **Lineage** and then see the queries associated with this table.\n",
    "      - You will see the `Drift Alert 1` listed there.\n",
    "   - Whenever thereâ€™s an update or drift detected in your model or dataset, the alert will notify you automatically. You can then go to the table's **Lineage** tab to investigate the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4fc2ad-9c36-45e2-9113-dea7915291cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Profile and Drift Metrics for Other Types of Drift\n",
    "If we wish to monitor other types of drift (see below), then we can leverage the drift metrics table to do so.\n",
    "\n",
    "Here we outline the metrics to monitor **concept drift**, **model quality drift**, and **bias drift**, along with guidance on their application.\n",
    "\n",
    "| **Drift Type**      | **Definition**                                                                                                                                                                 | **Applicable Metrics**                                                                                                                                                                                                                                                                                        | **Additional Metrics**                                                                                                                                                                                                                              | **How to Use**                                                                                                                                                                                                                 |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Concept Drift**    | Changes in the relationship between input features and the target variable.                                                                                                 | - **KS Test (`ks_test`)**: Evaluate numeric feature drift.<br>- **Chi-Square Test (`chi_squared_test`)**: Analyze categorical feature drift.<br>- **TV Distance (`tv_distance`)**: Measure distributional differences.<br>- **Wasserstein Distance (`wasserstein_distance`)**: Capture numeric feature changes.<br>- **JS Distance (`js_distance`)**: Measure categorical similarity. | - **Predicted Probabilities (`pred_prob_delta`)**: Check prediction score changes.<br>- **Residual Distribution (`residual_delta`)**: Analyze residual distribution shifts over time.                                                               | - Compare feature distributions from training data to current data.<br>- Check conditional distributions of features given the target.<br>- Monitor predicted probabilities and residuals for deviations over time.                                   |\n",
    "| **Model Quality Drift** | Degradation in the model's predictive performance over time.                                                                                                               | - **Count Delta (`count_delta`)**: Detect changes in data volume.<br>- **Avg Delta (`avg_delta`)**: Highlight feature mean shifts.<br>- **Percent Null Delta (`percent_null_delta`)**: Track changes in missing data rates.<br>- **Non-Null Columns Delta (`non_null_columns_delta`)**: Identify missing or new features. | - **Performance Metrics**: Monitor metrics like accuracy, RMSE, AUC, or F1 score.<br>- **Feature Importance Drift**: Observe changes in feature importance.<br>- **Residual Metrics**: Track residual trends (e.g., mean, variance).                  | - Monitor metrics for deviations from expected ranges.<br>- Use proxy metrics and performance metrics to assess drift.<br>- Address significant deviations with retraining or adjustments.                                                           |\n",
    "| **Bias Drift**       | Changes that affect the fairness or equitable performance of the model across subgroups.                                                                                    | - **Chi-Square Test (`chi_squared_test`)**: Detect subgroup distributional shifts.<br>- **JS Distance (`js_distance`)**: Measure subgroup similarity changes.<br>- **Population Stability Index (`population_stability_index`)**: Track population shifts for subgroups.                                         | - **Outcome-Based Fairness Metrics**:<br> - **Disparate Impact**: Ratio of positive outcomes between subgroups.<br> - **Equalized Odds**: Differences in false positive/negative rates between subgroups.<br> - **Demographic Parity**: Positive prediction rates comparison.<br>- **Performance Metrics by Subgroup**: Assess subgroup-specific performance metrics (e.g., accuracy, recall). | - Monitor subgroup feature distributions and outcomes.<br>- Address fairness concerns with outcome-based metrics.<br>- Use metrics to identify and address bias comprehensively.                                                                    |\n",
    "\n",
    "### **Overall Guidance**\n",
    "- Use these metrics alongside domain knowledge and performance evaluations.\n",
    "- Contextual interpretation is critical to avoid false positives or overcorrecting.\n",
    "- Combine proxy metrics, qualitative checks, and direct performance metrics for a comprehensive drift monitoring framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700b3fb9-7371-46b2-be1d-bca12d3b9522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Configurations\n",
    "\n",
    "There are other types of advanced configurations you setup. Although, because this monitor was previously created, you will have to refresh the process by clicking on **Refresh Metrics** under **Quality** for the **model_logs** table. We provide a few below as a part of this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d940f6-dcbe-4c45-94b4-24fd87ea4a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom Metric Creation: Aggregate metric (using the UI)\n",
    "\n",
    "On top of the analysis and drift statistics that are automatically generated with Lakehouse Monitoring, you can create custom metrics as well. \n",
    "\n",
    "There are 3 types of custom metrics that can be used with Lakehouse Monitoring:\n",
    "1. Aggregated metrics\n",
    "1. Derived metrics\n",
    "1. Drift metrics\n",
    "\n",
    "\n",
    "To create and implement a custom metric in Databricks Lakehouse Monitoring, follow these steps:\n",
    "\n",
    "1. **Access the Catalog**\n",
    "   - Navigate to the **Catalog** from the left sidebar menu.\n",
    "\n",
    "1. **Open Model Logs**\n",
    "   - In the Catalog, locate and click on **model_logs**.\n",
    "\n",
    "1. **Edit Monitor Configuration**\n",
    "   - Click on the **Quality** tab.\n",
    "   - Click **Configuration**.\n",
    "   - Click on **Configuration** under Data profiling.\n",
    "\n",
    "1. **Enable Advanced Options**\n",
    "   - Scroll down to **Advanced Options** and expand the drop-down menu.\n",
    "\n",
    "1. **Add Custom Metric**\n",
    "   - Select **Add Custom Metric** from the menu.\n",
    "\n",
    "1. **Define the Custom Metric**\n",
    "   - Enter the following details for your custom metric:\n",
    "     - **Name**: `AverageDifferenceFruitsVeggies`\n",
    "     - **Type**: `Aggregate`\n",
    "     - **Input Columns**: `Fruits`, `Veggies`\n",
    "     - **Output Type**: `Double`\n",
    "     - **Definition**:\n",
    "       AVG(Fruits - Veggies)\n",
    "\n",
    "\n",
    "\n",
    "1. **Update and Refresh Monitor**\n",
    "   - Once the metric is added, update and refresh the monitor to compute and view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2c0e1c-1405-45d0-beab-67ab012262dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "An example of how to configure a custom metric this with Python (and SQL Jinja template) is provided here:\n",
    "\n",
    "```\n",
    "from databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "custom_metric = MonitorMetric(\n",
    "    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,\n",
    "    name=\"avg_diff_f1_f2\",\n",
    "    input_columns=[\":table\"],\n",
    "    definition=\"avg(Fruits - Veggies)\",\n",
    "    output_data_type=T.StructField(\"output\", T.DoubleType()).json(),\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "To read more on a more programmatic approach, please read this [official documentation](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2f053c-60ee-4794-8eec-7a6ce4c6b0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demonstration, we discussed Lakehouse Monitoring and took a deep dive into how to monitor various metrics using Dashboards. You learned about out-of-the-box metrics as well as how to create a custom metric with the UI and programmatically. We also discussed how **Lineage** with Delta tables and **Alerts** can be used within Databricks for sending updates for when drift does occur with your profile and drift metric tables. Finally, you learned about how to set advanced configurations with your inference table such as adding slicers and granularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98f97de-74ee-43b9-84cf-0884b612a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6309484787105547,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1 Demo - Monitoring Model Quality",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
