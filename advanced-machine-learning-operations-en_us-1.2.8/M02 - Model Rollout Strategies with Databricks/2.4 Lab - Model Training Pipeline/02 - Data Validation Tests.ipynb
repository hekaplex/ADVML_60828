{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef0cb910-5966-43f7-943b-20ff3365f5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d2c971-4215-4e8c-b21f-8ec167d0c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workflow Notebook - Data Validation Tests\n",
    "\n",
    "This notebook's purpose is to validate schema, missing values, and confirm nonnegative values using unittest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "124c0d1d-fa39-4b92-9a67-0d45f22636a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(<FILL_IN>)\n",
    "schema = dbutils.widgets.get(<FILL_IN>)\n",
    "silver_table_name = dbutils.widgets.get('<FILL_IN>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8726d53-9bcd-4fe3-89da-f4e61401b904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "silver_table_name = dbutils.widgets.get('silver_table_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae9ed4c-a483-4609-ae45-0a118cc6c046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE {catalog}.{schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17229685-c559-4e6f-8d5f-4fafe50a8139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, LongType\n",
    "from pyspark.sql.functions import col, sum, min\n",
    "\n",
    "\n",
    "class TestDataValidation(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Unit tests for schema validation, missing values, and non-negative values in PySpark DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"\n",
    "        Set up shared resources for the tests.\n",
    "        \"\"\"\n",
    "        # Load the test DataFrame (assume a table named 'diabetes' is present)\n",
    "        cls.df = spark.read.format(\"delta\").table(f\"{catalog}.{schema}.{silver_table_name}\").select(\n",
    "            'id', 'Diabetes_binary', 'HighBP', 'BMI', 'Smoker', 'Stroke', \n",
    "            'HeartDiseaseorAttack', 'Age'\n",
    "        )\n",
    "\n",
    "    def test_validate_schema(self):\n",
    "        \"\"\"\n",
    "        Test if the DataFrame schema matches the expected schema.\n",
    "        \"\"\"\n",
    "        expected_schema = StructType([\n",
    "            <FILL_IN>\n",
    "        ])\n",
    "        actual_schema = self.df.schema\n",
    "        self.assertEqual(\n",
    "            <FILL_IN>\n",
    "            f\"Schema validation failed.\\nExpected: {expected_schema}\\nActual: {actual_schema}\"\n",
    "        )\n",
    "\n",
    "    def test_validate_no_missing_values(self):\n",
    "        \"\"\"\n",
    "        Test that there are no missing (null) values in the DataFrame.\n",
    "        \"\"\"\n",
    "        missing_values = self.df.agg(*[\n",
    "            sum(col(c).isNull().cast(\"int\")).alias(c) for c in self.df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "        missing_columns = {col: missing_values[col] for col in self.df.columns if missing_values[col] > 0}\n",
    "        self.assertFalse(\n",
    "            <FILL_IN>\n",
    "            f\"Missing values found in the following columns: {missing_columns}\"\n",
    "        )\n",
    "\n",
    "    def test_validate_non_negative_values(self):\n",
    "        \"\"\"\n",
    "        Test that all columns in the DataFrame contain non-negative values (>= 0).\n",
    "        \"\"\"\n",
    "        negative_values = self.df.agg(*[\n",
    "            min(col(c)).alias(c) for c in self.df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "        negative_columns = {col: negative_values[col] for col in self.df.columns if negative_values[col] < 0}\n",
    "        self.assertFalse(\n",
    "            <FILL_IN>\n",
    "            f\"Negative values found in the following columns: {negative_columns}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the tests\n",
    "suite = unittest.<FILL_IN>\n",
    "unittest.<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7769410b-bbe4-470a-aa57-1ccf584bf4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "import unittest\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, LongType\n",
    "from pyspark.sql.functions import col, sum, min\n",
    "\n",
    "\n",
    "class TestDataValidation(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Unit tests for schema validation, missing values, and non-negative values in PySpark DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"\n",
    "        Set up shared resources for the tests.\n",
    "        \"\"\"\n",
    "        # Load the test DataFrame (assume a table named 'diabetes' is present)\n",
    "        cls.df = spark.read.format(\"delta\").table(silver_table_name).select(\n",
    "            'id', 'Diabetes_binary', 'HighBP', 'BMI', 'Smoker', 'Stroke', \n",
    "            'HeartDiseaseorAttack', 'Age'\n",
    "        )\n",
    "\n",
    "    def test_validate_schema(self):\n",
    "        \"\"\"\n",
    "        Test if the DataFrame schema matches the expected schema.\n",
    "        \"\"\"\n",
    "        expected_schema = StructType([\n",
    "            StructField(\"id\", LongType(), True),\n",
    "            StructField(\"Diabetes_binary\", IntegerType(), True),\n",
    "            StructField(\"HighBP\", IntegerType(), True),\n",
    "            StructField(\"BMI\", IntegerType(), True),\n",
    "            StructField(\"Smoker\", IntegerType(), True),\n",
    "            StructField(\"Stroke\", IntegerType(), True),\n",
    "            StructField(\"HeartDiseaseorAttack\", IntegerType(), True),\n",
    "            StructField(\"Age\", DoubleType(), True),\n",
    "        ])\n",
    "        actual_schema = self.df.schema\n",
    "        self.assertEqual(\n",
    "            actual_schema, expected_schema,\n",
    "            f\"Schema validation failed.\\nExpected: {expected_schema}\\nActual: {actual_schema}\"\n",
    "        )\n",
    "\n",
    "    def test_validate_no_missing_values(self):\n",
    "        \"\"\"\n",
    "        Test that there are no missing (null) values in the DataFrame.\n",
    "        \"\"\"\n",
    "        missing_values = self.df.agg(*[\n",
    "            sum(col(c).isNull().cast(\"int\")).alias(c) for c in self.df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "        missing_columns = {col: missing_values[col] for col in self.df.columns if missing_values[col] > 0}\n",
    "        self.assertFalse(\n",
    "            missing_columns,\n",
    "            f\"Missing values found in the following columns: {missing_columns}\"\n",
    "        )\n",
    "\n",
    "    def test_validate_non_negative_values(self):\n",
    "        \"\"\"\n",
    "        Test that all columns in the DataFrame contain non-negative values (>= 0).\n",
    "        \"\"\"\n",
    "        negative_values = self.df.agg(*[\n",
    "            min(col(c)).alias(c) for c in self.df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "        negative_columns = {col: negative_values[col] for col in self.df.columns if negative_values[col] < 0}\n",
    "        self.assertFalse(\n",
    "            negative_columns,\n",
    "            f\"Negative values found in the following columns: {negative_columns}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the tests\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestDataValidation)\n",
    "result = unittest.TextTestRunner().run(suite)\n",
    "\n",
    "# Check the number of failures\n",
    "if len(result.failures) > 0:\n",
    "    raise Exception(f\"Test failed: More than 1 failure detected ({len(result.failures)} failures).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6b09cc-d404-4363-a524-b1d0b3a30d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02 - Data Validation Tests",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
