{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e381c8-35b8-4c57-8506-cff9e15dab2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0230d62-bd8f-417e-a2fe-89e8300ae75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workflow Notebook - Train Model on Validated Features\n",
    "\n",
    "1. **Purpose of the Notebook**:\n",
    "   - In this third notebook, called **Train Model on Features**, we will train a machine learning model using the **validated features** from the previous notebook.\n",
    "\n",
    "2. **Process**:\n",
    "   - The validated feature table is read and used as input for model training.\n",
    "   - The resulting model is stored in **Unity Catalog** for centralized management and accessibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ff89921-3ee2-4606-95ab-312117a86880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(<FILL_IN>)\n",
    "schema = dbutils.widgets.get(<FILL_IN>)\n",
    "primary_key = dbutils.widgets.get(<FILL_IN>)\n",
    "target_column = dbutils.widgets.get(<FILL_IN>)\n",
    "silver_table_name = dbutils.widgets.get(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a1efabe-0734-44c9-bd6e-aa6db29d1591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "primary_key = dbutils.widgets.get('primary_key')\n",
    "target_column = dbutils.widgets.get('target_column')\n",
    "silver_table_name = dbutils.widgets.get('silver_table_name')\n",
    "delete_column = dbutils.widgets.get('delete_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b1a0308-2ac4-4017-bb6c-d070fee90d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE {catalog}.{schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd05187a-a858-445d-b76c-e2ba043ba5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.format('delta').table(silver_table_name)\n",
    "training_df = df.toPandas()\n",
    "\n",
    "included_features_list = [<FILL_IN>]\n",
    "smaller_included_features_list = [<FILL_IN>]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_large = training_df[included_features_list]\n",
    "X_small = training_df[smaller_included_features_list]\n",
    "y = training_df[target_column]\n",
    "X_large_train, X_large_test, y_large_train, y_large_test = <FILL_IN>\n",
    "X_small_train = X_large_train.drop(<FILL_IN>)\n",
    "X_small_test = X_large_test.drop(<FILL_IN>)\n",
    "y_small_train = y_large_train\n",
    "y_small_test = y_large_test\n",
    "\n",
    "# Save the test set for querying later\n",
    "df = spark.createDataFrame(X_large_test)\n",
    "df.write.format('delta').mode('overwrite').table(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88855d78-1352-4d91-882b-f603dc29b92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.format('delta').table(silver_table_name)\n",
    "training_df = df.toPandas()\n",
    "\n",
    "included_features_list = [c for c in df.columns if c not in [target_column, primary_key]]\n",
    "smaller_included_features_list = [c for c in included_features_list if c not in [delete_column]]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_large = training_df[included_features_list]\n",
    "X_small = training_df[smaller_included_features_list]\n",
    "y = training_df[target_column]\n",
    "X_large_train, X_large_test, y_large_train, y_large_test = train_test_split(X_large, y, test_size=0.2, random_state=42)\n",
    "X_small_train = X_large_train.drop(columns=[delete_column])\n",
    "X_small_test = X_large_test.drop(columns=[delete_column])\n",
    "y_small_train = y_large_train\n",
    "y_small_test = y_large_test\n",
    "\n",
    "df = spark.createDataFrame(X_large_test)\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('X_large_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ff797f-f476-4664-bdef-84704855243f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "def train_model(X_train,y, alias):\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name='mlflow-run') as run:\n",
    "        # Initialize the Random Forest classifier\n",
    "        rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        rf_classifier.fit(X_train, y_large_train)\n",
    "\n",
    "        # Enable autologging\n",
    "        <FILL_IN>\n",
    "\n",
    "        # Define the registered model name\n",
    "        registered_model_name = f\"{catalog}.{schema}.my_model_{schema}\"\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "        <FILL_IN>\n",
    "        )\n",
    "\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model-artifacts\"\n",
    "\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    # Define the model name \n",
    "    model_name = f\"{catalog}.{schema}.my_model_{schema}\"\n",
    "\n",
    "    # Register the model in the model registry\n",
    "    registered_model = <FILL_IN>\n",
    "\n",
    "    # Initialize an MLflow Client\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Assign an alias\n",
    "    client.set_registered_model_alias(\n",
    "        name= <FILL_IN>,  # The registered model name\n",
    "        alias=<FILL_IN>,  # The alias representing the dev environment\n",
    "        version=<FILL_IN>  # The version of the model you want to move to \"dev\"\n",
    "    )\n",
    "\n",
    "train_model(X_large_train,y_large_train, 'a')\n",
    "train_model(X_small_train,y_small_train, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6d0499-b95a-4b3f-9e27-ca5da0025ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "def train_model(X_train,y, alias):\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name='mlflow-run') as run:\n",
    "        # Initialize the Random Forest classifier\n",
    "        rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        rf_classifier.fit(X_train, y_large_train)\n",
    "\n",
    "        # Enable autologging\n",
    "        mlflow.sklearn.autolog(log_input_examples=True, silent=True)\n",
    "\n",
    "        # Define the registered model name\n",
    "        registered_model_name = f\"{catalog}.{schema}.my_model_{schema}\"\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "        rf_classifier,\n",
    "        artifact_path = \"model-artifacts\", \n",
    "        input_example=X_train[:3],\n",
    "        signature=infer_signature(X_train, y_large_train)\n",
    "        )\n",
    "\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model-artifacts\"\n",
    "\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    # Define the model name \n",
    "    model_name = f\"{catalog}.{schema}.my_model_{schema}\"\n",
    "\n",
    "    # Register the model in the model registry\n",
    "    registered_model = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "    # Initialize an MLflow Client\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Assign an alias\n",
    "    client.set_registered_model_alias(\n",
    "        name= registered_model.name,  # The registered model name\n",
    "        alias=alias,  # The alias representing the dev environment\n",
    "        version=registered_model.version  # The version of the model you want to move to \"dev\"\n",
    "    )\n",
    "\n",
    "train_model(X_large_train,y_large_train, 'a')\n",
    "train_model(X_small_train,y_small_train, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d8aa8a-cbc5-4556-a8b9-b8f170364dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04  - Train Models on Validated Features",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
