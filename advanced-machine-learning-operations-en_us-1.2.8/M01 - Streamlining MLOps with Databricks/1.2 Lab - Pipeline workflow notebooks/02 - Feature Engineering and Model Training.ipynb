{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1542b78d-192e-4d85-af30-da68a71daff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dc50b7-f6f9-44ad-9475-9a984259d7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering and Model Training\n",
    "This notebook performs feature engineering on the Telco dataset and trains a machine learning model to predict customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec9814d-f4ee-4581-859d-e9f0b43119c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88857041-0daf-4799-a780-dabe863c2089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c598591-b0c9-4379-8c97-b10998b4d5b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0069dac6-e890-4663-8dc2-6f98a3b0ea85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Feature Engineering and Model Training\n",
    "\n",
    "**Steps Covered:**\n",
    "- Feature Engineering\n",
    "- Model Training and Evaluation\n",
    "- Conditional Execution Logic for Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809d51bc-bebf-4172-a787-61adf62a02b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import col, log, sqrt, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Initialize the success flag and status message\n",
    "success_flag = True\n",
    "task_status = \"SUCCESS\"\n",
    "\n",
    "try:\n",
    "    # Step 1: Read the Transformed Data\n",
    "    print(\"Step 1: Reading transformed data...\")\n",
    "    transformed_data_path = f\"{DA.catalog_name}.{DA.schema_name}.transformed_telco_data\"\n",
    "    try:\n",
    "        transformed_data = spark.table(transformed_data_path)\n",
    "        print(f\"Transformed data successfully loaded. Rows: {transformed_data.count()}\")\n",
    "        display(transformed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading transformed data: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 2: Perform Feature Engineering\n",
    "    print(\"Step 2: Performing feature engineering...\")\n",
    "    try:\n",
    "        engineered_data = transformed_data \\\n",
    "            .withColumn(\"log_tenure\", log(col(\"tenure\") + 1)) \\\n",
    "            .withColumn(\"sqrt_MonthlyCharges\", sqrt(col(\"MonthlyCharges\") + 1)) \\\n",
    "            .withColumn(\"log_TotalCharges\", log(col(\"TotalCharges\") + 1)) \\\n",
    "            .withColumn(\"is_senior\", when(col(\"SeniorCitizen\") == 1, \"Yes\").otherwise(\"No\")) \\\n",
    "            .withColumn(\"Churn\", when(col(\"Churn\") == \"Yes\", 1).otherwise(0).cast(\"int\")) \\\n",
    "            .withColumn(\"error_column\", col(\"NonExistentColumn\"))  # Deliberate error\n",
    "        print(\"Feature engineering completed successfully.\")\n",
    "        display(engineered_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature engineering: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 3: Save the Feature-Engineered Data\n",
    "    print(\"Step 3: Saving feature-engineered data...\")\n",
    "    feature_engineered_data_path = f\"{DA.catalog_name}.{DA.schema_name}.feature_engineered_telco_data\"\n",
    "    try:\n",
    "        engineered_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(feature_engineered_data_path)\n",
    "        print(f\"Feature-engineered data saved to: {feature_engineered_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature-engineered data: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 4: Train the Model\n",
    "    print(\"Step 4: Training the model...\")\n",
    "    try:\n",
    "        # Assemble features for ML model\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"tenure\", \"MonthlyCharges\", \"log_tenure\", \"sqrt_MonthlyCharges\"],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        feature_data = assembler.transform(engineered_data)\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_data, test_data = feature_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "        # Train a Random Forest classifier\n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Churn\")\n",
    "        model = rf.fit(train_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        predictions = model.transform(test_data)\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Model accuracy: {accuracy}\")\n",
    "        display(predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training or evaluating the model: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 5: Log the Model in MLflow with Signature\n",
    "    print(\"Step 5: Logging the model in MLflow...\")\n",
    "    experiment_name = f\"/Shared/{DA.username}_telco_churn\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    try:\n",
    "        # Infer the signature from the input and output data\n",
    "        input_sample = train_data.select(\"features\").toPandas()\n",
    "        output_sample = predictions.select(\"prediction\").toPandas()\n",
    "        signature = infer_signature(input_sample, output_sample)\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.spark.log_model(\n",
    "                model, \n",
    "                artifact_path=\"model\", \n",
    "                registered_model_name=f\"{DA.catalog_name}.{DA.schema_name}.telco_churn_model\",\n",
    "                signature=signature\n",
    "            )\n",
    "        print(\"Model logged in MLflow successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging the model in MLflow: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "except Exception:\n",
    "    print(\"Notebook exited: FAILURE\")\n",
    "    dbutils.jobs.taskValues.set(key=\"feature_engineering_status\", value=\"FAILURE\")\n",
    "    success_flag = False\n",
    "\n",
    "# Final output\n",
    "if success_flag:\n",
    "    print(\"Notebook exited: SUCCESS\")\n",
    "    dbutils.jobs.taskValues.set(key=\"feature_engineering_status\", value=\"SUCCESS\")\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "else:\n",
    "    print(\"Notebook exited: FAILURE\")\n",
    "    dbutils.notebook.exit(\"FAILURE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a54e0c-a10c-4e20-b745-229d7a48470d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import col, log, sqrt, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Initialize the success flag and status message\n",
    "success_flag = True\n",
    "task_status = \"SUCCESS\"\n",
    "\n",
    "try:\n",
    "    # Step 1: Read the Transformed Data\n",
    "    print(\"Step 1: Reading transformed data...\")\n",
    "    transformed_data_path = f\"{DA.catalog_name}.{DA.schema_name}.transformed_telco_data\"\n",
    "    try:\n",
    "        transformed_data = spark.table(transformed_data_path)\n",
    "        print(f\"Transformed data successfully loaded. Rows: {transformed_data.count()}\")\n",
    "        display(transformed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading transformed data: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 2: Perform Feature Engineering\n",
    "    print(\"Step 2: Performing feature engineering...\")\n",
    "    try:\n",
    "        engineered_data = transformed_data \\\n",
    "            .withColumn(\"log_tenure\", log(col(\"tenure\") + 1)) \\\n",
    "            .withColumn(\"sqrt_MonthlyCharges\", sqrt(col(\"MonthlyCharges\") + 1)) \\\n",
    "            .withColumn(\"log_TotalCharges\", log(col(\"TotalCharges\") + 1)) \\\n",
    "            .withColumn(\"is_senior\", when(col(\"SeniorCitizen\") == 1, \"Yes\").otherwise(\"No\")) \\\n",
    "            .withColumn(\"Churn\", when(col(\"Churn\") == \"Yes\", 1).otherwise(0).cast(\"int\"))\n",
    "            #.withColumn(\"error_column\", col(\"NonExistentColumn\"))  \n",
    "         # UNCOMMENT THE LINE ABOVE TO INTRODUCE A DELIBERATE ERROR\n",
    "        print(\"Feature engineering completed successfully.\")\n",
    "        display(engineered_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature engineering: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 3: Save the Feature-Engineered Data\n",
    "    print(\"Step 3: Saving feature-engineered data...\")\n",
    "    feature_engineered_data_path = f\"{DA.catalog_name}.{DA.schema_name}.feature_engineered_telco_data\"\n",
    "    try:\n",
    "        engineered_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(feature_engineered_data_path)\n",
    "        print(f\"Feature-engineered data saved to: {feature_engineered_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature-engineered data: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 4: Train the Model\n",
    "    print(\"Step 4: Training the model...\")\n",
    "    try:\n",
    "        # Assemble features for ML model\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"tenure\", \"MonthlyCharges\", \"log_tenure\", \"sqrt_MonthlyCharges\"],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        feature_data = assembler.transform(engineered_data)\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_data, test_data = feature_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "        # Train a Random Forest classifier\n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Churn\")\n",
    "        model = rf.fit(train_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        predictions = model.transform(test_data)\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Model accuracy: {accuracy}\")\n",
    "        display(predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training or evaluating the model: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "    # Step 5: Log the Model in MLflow with Signature\n",
    "    print(\"Step 5: Logging the model in MLflow...\")\n",
    "    experiment_name = f\"/Shared/{DA.username}_telco_churn\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    try:\n",
    "        # Infer the signature from the input and output data\n",
    "        input_sample = train_data.select(\"features\").toPandas()\n",
    "        output_sample = predictions.select(\"prediction\").toPandas()\n",
    "        signature = infer_signature(input_sample, output_sample)\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.spark.log_model(\n",
    "                model, \n",
    "                artifact_path=\"model\", \n",
    "                registered_model_name=f\"{DA.catalog_name}.{DA.schema_name}.telco_churn_model\",\n",
    "                signature=signature\n",
    "            )\n",
    "        print(\"Model logged in MLflow successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging the model in MLflow: {e}\")\n",
    "        task_status = \"FAILURE\"\n",
    "        raise\n",
    "\n",
    "except Exception:\n",
    "    print(\"Notebook exited: FAILURE\")\n",
    "    dbutils.jobs.taskValues.set(key=\"feature_engineering_status\", value=\"FAILURE\")\n",
    "    success_flag = False\n",
    "\n",
    "# Final output\n",
    "if success_flag:\n",
    "    print(\"Notebook exited: SUCCESS\")\n",
    "    dbutils.jobs.taskValues.set(key=\"feature_engineering_status\", value=\"SUCCESS\")\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "else:\n",
    "    print(\"Notebook exited: FAILURE\")\n",
    "    dbutils.notebook.exit(\"FAILURE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6937f6-908a-481a-8693-40c51a690cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02 - Feature Engineering and Model Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
