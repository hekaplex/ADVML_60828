{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814e22ab-bbbd-4815-8d44-773b9f053f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6376976f-460b-4f1a-b929-f38778cbda29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Evaluation and Testing with Accuracy Check\n",
    "\n",
    "This notebook evaluates the performance of a trained model, generates predictions, and validates its accuracy against a defined threshold to determine the pipeline's progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7657d210-fa4b-4381-91fb-f6c8beb4fd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16021192-208f-4d61-a35e-b24c628b2859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce945343-6427-4ca0-834e-4aff47a6bdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ce3cad-15fa-4479-8c22-966a7455ba01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps Covered in This Notebook\n",
    "\n",
    "1. **Model Evaluation and Accuracy Check**:\n",
    "   - Load feature-engineered data and assemble features.\n",
    "   - Retrieve the trained model from Unity Catalog, generate predictions, and evaluate accuracy using the Area Under ROC (AUC) metric. \n",
    "   - Compare the accuracy against a user-defined threshold to determine if the pipeline should proceed.\n",
    "\n",
    "2. **Metrics Logging and Validation**:\n",
    "   - Save predictions to Delta tables and log evaluation metrics (accuracy and threshold) to MLflow for tracking.\n",
    "   - Exit the notebook with a success or failure status based on the accuracy validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba911e7-8dc7-4e36-acd5-33cee78c8ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "# Widgets for environment-specific configurations\n",
    "dbutils.widgets.text(\"accuracy_threshold\", \"0.6\", \"Accuracy Threshold\")\n",
    "dbutils.widgets.text(\"model_name\", \"telco_churn_model\", \"Model Name\")\n",
    "\n",
    "# Get widget values\n",
    "accuracy_threshold = float(dbutils.widgets.get(\"accuracy_threshold\"))\n",
    "model_name = dbutils.widgets.get(\"model_name\") if dbutils.widgets.get(\"model_name\") == 'telco_churn_model' else 'telco_churn_model_dev'\n",
    "\n",
    "# Define paths for data and predictions\n",
    "feature_engineered_data_path = f\"{DA.catalog_name}.{DA.schema_name}.feature_engineered_telco_data\"\n",
    "prediction_table_path = f\"{DA.catalog_name}.{DA.schema_name}.prediction_table\"\n",
    "\n",
    "# Step 1: Load Feature-Engineered Data\n",
    "print(\"Step 1: Loading feature-engineered data...\")\n",
    "try:\n",
    "    feature_data = spark.table(feature_engineered_data_path)\n",
    "    if feature_data.count() == 0:\n",
    "        raise ValueError(\"Feature-engineered data is empty. Ensure data transformation and feature engineering steps are complete.\")\n",
    "    print(f\"Feature data contains {feature_data.count()} rows.\")\n",
    "    display(feature_data)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load feature-engineered data: {e}\")\n",
    "\n",
    "# Step 2: Assemble Features\n",
    "print(\"Step 2: Assembling features...\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tenure\", \"MonthlyCharges\", \"log_tenure\", \"sqrt_MonthlyCharges\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "try:\n",
    "    feature_data = assembler.transform(feature_data)\n",
    "    print(\"Features successfully assembled.\")\n",
    "    display(feature_data)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to assemble features: {e}\")\n",
    "\n",
    "# Step 3: Load the Model from Unity Catalog\n",
    "print(\"Step 3: Loading the model...\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Define full Unity Catalog model path\n",
    "full_model_name = f\"{DA.catalog_name}.{DA.schema_name}.{model_name}\"\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    # Retrieve all model versions from Unity Catalog\n",
    "    all_versions = client.search_model_versions(f\"name='{full_model_name}'\")\n",
    "\n",
    "    # Check if \"Champion\" alias exists\n",
    "    champion_version = None\n",
    "    for mv in all_versions:\n",
    "        version_details = client.get_model_version(full_model_name, mv.version)\n",
    "        if \"Champion\" in version_details.aliases:\n",
    "            champion_version = mv.version\n",
    "            break\n",
    "\n",
    "    # If no Champion alias exists, assign it to the latest version\n",
    "    if champion_version is None:\n",
    "        latest_version = max(int(mv.version) for mv in all_versions)\n",
    "        client.set_registered_model_alias(full_model_name, \"Champion\", version=latest_version)\n",
    "        champion_version = latest_version\n",
    "        print(f\"Alias 'Champion' assigned to version {latest_version}.\")\n",
    "\n",
    "    # Load model from Unity Catalog using the alias\n",
    "    model_uri = f\"models:/{full_model_name}@Champion\"\n",
    "    print(f\"Attempting to load model from: {model_uri}\")\n",
    "    model = mlflow.spark.load_model(model_uri)\n",
    "    print(f\"Model loaded successfully from: {model_uri}\")\n",
    "\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    raise ValueError(f\"Failed to load the model from Unity Catalog. Ensure the model '{model_name}' is registered and available. Error: {e}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Unexpected error while loading the model: {e}\")\n",
    "\n",
    "# Step 4: Generate Predictions\n",
    "print(\"Step 4: Generating predictions...\")\n",
    "try:\n",
    "    predictions = model.transform(feature_data)\n",
    "    if predictions.count() == 0:\n",
    "        raise ValueError(\"No predictions generated. Verify the model and input data compatibility.\")\n",
    "    print(f\"Predictions generated successfully: {predictions.count()} rows.\")\n",
    "    display(predictions)\n",
    "\n",
    "    # Enable schema migration for Delta table\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Save predictions to a Delta table with schema migration\n",
    "    predictions.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(prediction_table_path)\n",
    "    print(f\"Predictions saved to: {prediction_table_path}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to generate predictions: {e}\")\n",
    "\n",
    "# Step 5: Evaluate Model Accuracy\n",
    "print(\"Step 5: Evaluating model accuracy...\")\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Churn\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "try:\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Model accuracy (Area Under ROC): {accuracy}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to evaluate the model: {e}\")\n",
    "\n",
    "# Step 6: Compare Accuracy with Threshold\n",
    "print(\"Step 6: Validating accuracy against the threshold...\")\n",
    "if accuracy >= accuracy_threshold:\n",
    "    print(f\"Model accuracy {accuracy} meets the threshold {accuracy_threshold}. Proceeding with the pipeline.\")\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "else:\n",
    "    print(f\"Model accuracy {accuracy} is below the threshold {accuracy_threshold}. Stopping the pipeline.\")\n",
    "    dbutils.notebook.exit(\"FAILURE\")\n",
    "\n",
    "# Step 7: Log Evaluation Metrics to MLflow\n",
    "print(\"Step 7: Logging evaluation metrics to MLflow...\")\n",
    "experiment_name = f\"/Shared/{DA.username}_telco_churn_evaluation\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"accuracy_threshold\", accuracy_threshold)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    print(\"Evaluation metrics logged to MLflow successfully.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to log evaluation metrics to MLflow: {e}\")\n",
    "\n",
    "print(\"Notebook execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0590ece-1a41-4710-91c3-5ebf9262bf3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04 - Model Evaluation and Testing with Accuracy Check",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
