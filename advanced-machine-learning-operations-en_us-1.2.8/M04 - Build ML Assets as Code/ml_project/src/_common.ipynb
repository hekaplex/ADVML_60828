{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c19a77b-ad1c-4e98-a629-3877f94c8070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U databricks-sdk==0.36.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "603b5073-9aac-4ded-a920-f5a7bb35d161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors.platform import NotFound\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "class NestedNamespace:\n",
    "\n",
    "    def __init__(self, dictionary: dict = None, prefix=None):\n",
    "        prefix = prefix + '.' if prefix else ''\n",
    "        self.__setattr_direct('dictionary', dictionary or dict())\n",
    "        self.__setattr_direct('prefix', prefix)\n",
    "        self.__setattr_direct('iterator', None)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        name = self.prefix + name\n",
    "        return self.dictionary.get(name, NestedNamespace(dictionary=self.dictionary, prefix=name))\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        name = self.prefix + name\n",
    "        self.dictionary[name] = value\n",
    "\n",
    "        # since we've overwritten the node in the tree, prune branch by deleting any children/ancestors\n",
    "        name += '.'\n",
    "        children = [k for k in filter(lambda x: x.startswith(name), self.dictionary.keys())]\n",
    "        for k in children:\n",
    "            del(self.dictionary[k])\n",
    "\n",
    "    # bypass overridden behaviour to directly set attributes\n",
    "    def __setattr_direct(self, name, value):\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        args = [f\"{key}='{self[key]}'\" for key in self]\n",
    "        return f\"{self.__class__.__name__} ({', '.join(args)})\" if args else \"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__setattr_direct(\n",
    "            'iterator',\n",
    "            filter(\n",
    "                lambda x: x.startswith(self.prefix),\n",
    "                iter(self.dictionary)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.iterator).removeprefix(self.prefix) if self.iterator else None\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self.__getattr__(name)\n",
    "\n",
    "    def __setitem__(self, name, value):\n",
    "        return self.__setattr__(name, value)\n",
    "\n",
    "class DBAcademyHelper(NestedNamespace):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.workspace = WorkspaceClient()\n",
    "\n",
    "        try:\n",
    "            default_catalog = self.workspace.settings.default_namespace.get().namespace.value\n",
    "        except:\n",
    "            default_catalog = 'dbacademy'\n",
    "\n",
    "        meta = f'{default_catalog}.ops.meta'\n",
    "        catalog = None\n",
    "        schema = None\n",
    "\n",
    "        # query the metadata table and populate self with key/values\n",
    "        for row in spark.sql(f'SELECT key,value FROM {meta}').collect():\n",
    "            setattr(self, row['key'], row['value'])\n",
    "\n",
    "            if row['key'] == 'catalog_name':\n",
    "                catalog = row['value']\n",
    "            elif row['key'] == 'schema_name':\n",
    "                schema = row['value']\n",
    "\n",
    "        # set default catalog and schema according to metadata\n",
    "        if catalog:\n",
    "            spark.sql(f'USE CATALOG {catalog}')\n",
    "\n",
    "            if schema:\n",
    "                spark.sql(f'USE SCHEMA {schema}')\n",
    "\n",
    "    @classmethod\n",
    "    def add_init(cls, function_ref):\n",
    "        try:\n",
    "            initializers = getattr(cls, '_initializers')\n",
    "        except AttributeError:\n",
    "            initializers = list()\n",
    "\n",
    "        initializers += [function_ref]\n",
    "        setattr(cls, '_initializers', initializers)\n",
    "        return function_ref\n",
    "\n",
    "    @classmethod\n",
    "    def add_method(cls, function_ref):\n",
    "        setattr(cls, function_ref.__name__, function_ref)\n",
    "        return function_ref\n",
    "\n",
    "    def init(self):\n",
    "        for key in self:\n",
    "            value = self[key]\n",
    "            if value and type(value) == str:\n",
    "                try:\n",
    "                    spark.conf.set(f'DA.{key}', value)\n",
    "                    spark.conf.set(f'da.{key}', value)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            for i in getattr(self.__class__, '_initializers'):\n",
    "                i(self)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def print_copyrights(self):\n",
    "        datasets = self.datasets\n",
    "        for i in datasets:\n",
    "            catalog = datasets[i].split('.')[0]\n",
    "            description = spark.sql(\n",
    "                f'DESCRIBE CATALOG {catalog}'\n",
    "            ).where(\n",
    "                F.col('info_name') == 'Comment'\n",
    "            ).select(\n",
    "                'info_value'\n",
    "            ).collect()[0]['info_value']\n",
    "            print(description)\n",
    "\n",
    "    def workspace_find(self, item_type: str, value: str=None, member: str='name', api: str='list'):\n",
    "        method = getattr(getattr(self.workspace, item_type), api)\n",
    "        for item in method():\n",
    "            if getattr(item, member) == value:\n",
    "                return item\n",
    "\n",
    "    def unique_name(self, sep: str) -> str:\n",
    "        return self.pseudonym.replace(' ', sep)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "_common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
