{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e87b293-a919-4412-b09c-22909332c6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Training\n",
    "This notebook trains a machine learning model and registers it in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5c4564-b43c-4b88-bd78-1c571fe29acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b47f7e78-43a2-4d2a-bb92-bfa3bc97121a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9cc8d2e-74c6-4680-8e7e-2dabf5f521ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "# Initialize success flag\n",
    "success_flag = True\n",
    "\n",
    "try:\n",
    "    # Step 1: Widgets for environment-specific configurations\n",
    "    dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"staging\", \"prod\"], \"Environment Name\")\n",
    "    env = dbutils.widgets.get(\"env\")\n",
    "    \n",
    "    # Step 2: Read the feature-engineered data\n",
    "    try:\n",
    "        feature_data_path = f\"{DA.catalog_name}.{DA.schema_name}.feature_engineered_data\"\n",
    "        feature_data = spark.read.format(\"delta\").table(feature_data_path)\n",
    "        print(\"Feature-engineered data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading feature-engineered data: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 3: Assemble features for ML model\n",
    "    try:\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\n",
    "                \"HighBP\", \"BMI\", \"Smoker\", \"PhysActivity\", \"Fruits\",\n",
    "                \"Veggies\", \"MentHlth_squared\", \"BMI_squared\",\n",
    "                \"BMI_MentHlth_interaction\", \"Age\"\n",
    "            ],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        feature_data = assembler.transform(feature_data)\n",
    "        print(\"Feature vector assembled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature assembly: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 4: Split the dataset into training and test sets\n",
    "    try:\n",
    "        train_data, test_data = feature_data.randomSplit([0.8, 0.2], seed=42)\n",
    "        print(\"Data split into training and test sets successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data splitting: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 5: Train a Random Forest model\n",
    "    try:\n",
    "        rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Diabetes_binary\")\n",
    "        model = rf.fit(train_data)\n",
    "        print(\"Model trained successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 6: Evaluate the model\n",
    "    try:\n",
    "        evaluator = RegressionEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        predictions = model.transform(test_data)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(f\"Model evaluation completed. RMSE: {rmse}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 7: Register the model in MLflow\n",
    "    try:\n",
    "        experiment_name = f\"/Shared/{DA.username}_adv_mlops_demo_diabetes\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        signature = infer_signature(\n",
    "            train_data.select(\"features\").toPandas(),\n",
    "            model.transform(train_data).select(\"prediction\").toPandas()\n",
    "        )\n",
    "        \n",
    "        model_base_name = f\"diabetes_model_{env}\"\n",
    "        full_model_name = f\"{DA.catalog_name}.{DA.schema_name}.{model_base_name}\"\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param(\"environment\", env)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.spark.log_model(\n",
    "                model,\n",
    "                artifact_path=\"model\",\n",
    "                registered_model_name=full_model_name,\n",
    "                signature=signature\n",
    "            )\n",
    "        print(f\"Model registered in MLflow: {full_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model registration: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "    \n",
    "    # Step 8: Ensure the model is registered and available in MLflow\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        model_version_infos = client.search_model_versions(f\"name = '{full_model_name}'\")\n",
    "        if model_version_infos:\n",
    "            latest_version = max([int(info.version) for info in model_version_infos])\n",
    "            model_uri = f\"models:/{full_model_name}/{latest_version}\"\n",
    "            loaded_model = mlflow.spark.load_model(model_uri)\n",
    "            print(f\"Model registered and loaded for environment: {env}\")\n",
    "        else:\n",
    "            print(f\"Model {full_model_name} is not registered.\")\n",
    "            success_flag = False\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying model registration: {e}\")\n",
    "        success_flag = False\n",
    "        raise\n",
    "\n",
    "except Exception:\n",
    "    success_flag = False\n",
    "    print(\"Notebook exited: FAILURE\")\n",
    "    dbutils.notebook.exit(\"FAILED\")\n",
    "\n",
    "# Final output\n",
    "if success_flag:\n",
    "    print(\"Notebook exited: SUCCESS\")\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "else:\n",
    "    dbutils.notebook.exit(\"FAILED\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "train_model",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
