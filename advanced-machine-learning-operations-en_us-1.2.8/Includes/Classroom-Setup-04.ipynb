{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad401f9-2b80-4594-9706-3f996607fff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f121a9da-1667-449a-8944-a176d09dcd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA = DBAcademyHelper()  # Create the DA object\n",
    "DA.init()                                           # Performs basic intialization including creating schemas and catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf362219-3bb7-4cef-a96b-4b818bde3da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import configparser\n",
    "import ipywidgets as widgets\n",
    "from urllib.parse import urlparse, urlunsplit\n",
    "\n",
    "# Token Class Definition\n",
    "class Token:\n",
    "    def __init__(self, config_path='./M01 - Automated and Orchestrated  CI-CD Pipleine/var/credentials.cfg'):\n",
    "        self.config_path = config_path\n",
    "        self.token = self.get_credentials()\n",
    "\n",
    "    def get_credentials(self):\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(self.config_path):\n",
    "            raise FileNotFoundError(f\"The configuration file was not found at {self.config_path}\")\n",
    "\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(self.config_path)\n",
    "\n",
    "        if 'DEFAULT' not in config:\n",
    "            raise KeyError(\"The section 'DEFAULT' was not found in the configuration file.\")\n",
    "\n",
    "        if 'db_token' not in config['DEFAULT']:\n",
    "            raise KeyError(\"The key 'db_token' was not found in the 'DEFAULT' section of the configuration file.\")\n",
    "\n",
    "        token = config['DEFAULT']['db_token']\n",
    "        \n",
    "        # Print the token for debugging purposes\n",
    "        print(f\"db_token: {token}\")\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8423bd31-bae7-428e-8896-53dc5ced4cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def init_DA(name, reset=True, pipeline=False):\n",
    "\n",
    "    if pipeline:\n",
    "        DA.paths.stream_source = f\"{DA.paths.working_dir}/stream_source\"\n",
    "        DA.paths.storage_location = f\"{DA.paths.working_dir}/storage_location\"\n",
    "        DA.pipeline_name = f\"{DA.unique_name(sep='-')}: ETL Pipeline - {name.upper()}\"\n",
    "        DA.lookup_db = f\"{DA.unique_name(sep='_')}_lookup\"\n",
    "\n",
    "        if reset:  # create lookup tables\n",
    "            import pandas as pd\n",
    "\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {DA.lookup_db}\")\n",
    "            DA.clone_source_table(f\"{DA.lookup_db}.date_lookup\", source_name=\"date-lookup\")\n",
    "            DA.clone_source_table(f\"{DA.lookup_db}.user_lookup\", source_name=\"user-lookup\")\n",
    "\n",
    "            df = spark.createDataFrame(pd.DataFrame([\n",
    "                (\"device_id_not_null\", \"device_id IS NOT NULL\", \"validity\", \"bpm\"),\n",
    "                (\"device_id_valid_range\", \"device_id > 110000\", \"validity\", \"bpm\"),\n",
    "                (\"heartrate_not_null\", \"heartrate IS NOT NULL\", \"validity\", \"bpm\"),\n",
    "                (\"user_id_not_null\", \"user_id IS NOT NULL\", \"validity\", \"workout\"),\n",
    "                (\"workout_id_not_null\", \"workout_id IS NOT NULL\", \"validity\", \"workout\"),\n",
    "                (\"action_not_null\",\"action IS NOT NULL\",\"validity\",\"workout\"),\n",
    "                (\"user_id_not_null\",\"user_id IS NOT NULL\",\"validity\",\"user_info\"),\n",
    "                (\"update_type_not_null\",\"update_type IS NOT NULL\",\"validity\",\"user_info\")\n",
    "            ], columns=[\"name\", \"condition\", \"tag\", \"topic\"]))\n",
    "            df.write.mode(\"overwrite\").saveAsTable(f\"{DA.lookup_db}.rules\")\n",
    "        \n",
    "    return DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a46870-c1bb-436d-92b1-06de861be6c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Lesson with Configurations\n",
    "LESSON = \"generate_tokens\"\n",
    "DA = init_DA(LESSON, reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2581b46-2532-4c73-b342-9600b1668b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Credentials File Method\n",
    "@DBAcademyHelper.add_method\n",
    "def load_credentials(self):\n",
    "    import configparser\n",
    "    c = configparser.ConfigParser()\n",
    "    c.read(filenames=f\"var/credentials.cfg\")\n",
    "    try:\n",
    "        token = c.get('DEFAULT', 'db_token')\n",
    "        host = c.get('DEFAULT', 'db_instance')\n",
    "        os.environ[\"DATABRICKS_HOST\"] = host\n",
    "        os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "    except:\n",
    "        token = ''\n",
    "        host = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}'\n",
    "\n",
    "    return token, host    \n",
    "@DBAcademyHelper.add_method\n",
    "def create_credentials_file(self, db_token, db_instance):\n",
    "    contents = f\"\"\"\n",
    "db_token = \"{db_token}\"\n",
    "db_instance = \"{db_instance}\"\n",
    "    \"\"\"\n",
    "    self.write_to_file(contents, \"credentials.py\")\n",
    "None\n",
    "\n",
    "# Get Credentials Method\n",
    "@DBAcademyHelper.add_method\n",
    "def get_credentials(self):\n",
    "    (current_token, current_host) = self.load_credentials()\n",
    "\n",
    "    @widgets.interact(host=widgets.Text(description='Host:',\n",
    "                                        placeholder='Paste workspace URL here',\n",
    "                                        value=current_host,\n",
    "                                        continuous_update=False),\n",
    "                      token=widgets.Password(description='Token:',\n",
    "                                             placeholder='Paste PAT here',\n",
    "                                             value=current_token,\n",
    "                                             continuous_update=False)\n",
    "    )\n",
    "    def _f(host='', token=''):\n",
    "        u = urlparse(host)\n",
    "        host = urlunsplit((u.scheme, u.netloc, '', '', ''))\n",
    "\n",
    "        if host and token:\n",
    "            os.environ[\"DATABRICKS_HOST\"] = host\n",
    "            os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "\n",
    "            contents = f\"\"\"\n",
    "[DEFAULT]\n",
    "db_token = {token}\n",
    "db_instance = {host}\n",
    "            \"\"\"\n",
    "            # Save credentials in the new location\n",
    "            os.makedirs('./var', exist_ok=True)\n",
    "            with open(\"./var/credentials.cfg\", \"w\") as f:\n",
    "                print(f\"Credentials stored ({f.write(contents)} bytes written).\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6acc88d-2ee6-4081-9e76-acd55b4b4c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import configparser\n",
    "import ipywidgets as widgets\n",
    "from urllib.parse import urlparse, urlunsplit\n",
    "\n",
    "# Method to Load GitHub Credentials\n",
    "@DBAcademyHelper.add_method\n",
    "def load_git_credentials(self):\n",
    "    \"\"\"\n",
    "    Load GitHub credentials from a configuration file.\n",
    "    \"\"\"\n",
    "    config_path = \"var/git_credentials.cfg\"\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    if os.path.exists(config_path):\n",
    "        config.read(config_path)\n",
    "        github_username = config.get(\"DEFAULT\", \"github_username\", fallback=\"\")\n",
    "        repo_name = config.get(\"DEFAULT\", \"repo_name\", fallback=\"\")\n",
    "        github_token = config.get(\"DEFAULT\", \"github_token\", fallback=\"\")\n",
    "    else:\n",
    "        github_username = \"\"\n",
    "        repo_name = \"\"\n",
    "        github_token = \"\"\n",
    "\n",
    "    return github_username, repo_name, github_token\n",
    "\n",
    "# Method to Create GitHub Credentials File\n",
    "@DBAcademyHelper.add_method\n",
    "def create_git_credentials_file(self, github_username, repo_name, github_token):\n",
    "    \"\"\"\n",
    "    Save GitHub credentials to a configuration file.\n",
    "    \"\"\"\n",
    "    contents = f\"\"\"\n",
    "[DEFAULT]\n",
    "github_username = {github_username}\n",
    "repo_name = {repo_name}\n",
    "github_token = {github_token}\n",
    "    \"\"\"\n",
    "    os.makedirs(\"./var\", exist_ok=True)\n",
    "    with open(\"var/git_credentials.cfg\", \"w\") as file:\n",
    "        file.write(contents)\n",
    "        print(f\"GitHub credentials stored ({len(contents)} bytes written).\")\n",
    "\n",
    "# Method to Get GitHub Credentials with Widgets\n",
    "@DBAcademyHelper.add_method\n",
    "def get_git_credentials(self):\n",
    "    \"\"\"\n",
    "    Create widgets for GitHub credentials input.\n",
    "    \"\"\"\n",
    "    current_username, current_repo_name, current_token = self.load_git_credentials()\n",
    "\n",
    "    @widgets.interact(\n",
    "        username=widgets.Text(\n",
    "            description=\"Username:\",\n",
    "            placeholder=\"Enter GitHub username\",\n",
    "            value=current_username,\n",
    "            continuous_update=False\n",
    "        ),\n",
    "        repo=widgets.Text(\n",
    "            description=\"Repo Name:\",\n",
    "            placeholder=\"Enter repository name\",\n",
    "            value=current_repo_name,\n",
    "            continuous_update=False\n",
    "        ),\n",
    "        token=widgets.Password(\n",
    "            description=\"Token:\",\n",
    "            placeholder=\"Paste GitHub PAT here\",\n",
    "            value=current_token,\n",
    "            continuous_update=False\n",
    "        )\n",
    "    )\n",
    "    def _f(username=\"\", repo=\"\", token=\"\"):\n",
    "        if username and repo and token:\n",
    "            contents = f\"\"\"\n",
    "[DEFAULT]\n",
    "github_username = {username}\n",
    "repo_name = {repo}\n",
    "github_token = {token}\n",
    "            \"\"\"\n",
    "            os.makedirs(\"./var\", exist_ok=True)\n",
    "            with open(\"var/git_credentials.cfg\", \"w\") as file:\n",
    "                file.write(contents)\n",
    "                print(f\"GitHub credentials stored ({len(contents)} bytes written).\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7daff54-8d5a-4163-87d5-722828a0cd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def update_bundle(self):\n",
    "    import ipywidgets as widgets\n",
    "    import os\n",
    "    \n",
    "    #os.remove(\"my_project/requirements-dev.txt\")\n",
    "    #os.remove(\"my_project/pytest.ini\")\n",
    "    #os.remove(\"my_project/tests/main_test.py\")\n",
    "    #os.rmdir(\"my_project/tests\")\n",
    "    #os.remove(\"my_project/scratch/exploration.ipynb\")\n",
    "    #os.remove(\"my_project/scratch/README.md\")\n",
    "    #os.rmdir(\"my_project/scratch\")\n",
    "    \n",
    "    current_cluster = self.client.clusters.get_current()['cluster_id']\n",
    "\n",
    "    import subprocess\n",
    "    import json\n",
    "\n",
    "    def get_policy_id(policy_name):\n",
    "        # Run the shell command to list cluster policies\n",
    "        result = subprocess.run(\n",
    "            [\"databricks\", \"cluster-policies\", \"list\", \"--output\", \"JSON\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error executing command:\", result.stderr)\n",
    "            return None\n",
    "\n",
    "        # Load JSON output\n",
    "        try:\n",
    "            policies = json.loads(result.stdout)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to decode JSON from command output\")\n",
    "            return None\n",
    "\n",
    "        # Search for the specified policy\n",
    "        for policy in policies:\n",
    "            if policy['name'] == policy_name:\n",
    "                return policy['policy_id']\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Specify the policy name\n",
    "    policy_name = \"DBAcademy DLT UC\"\n",
    "\n",
    "    # Get the policy ID\n",
    "    policy_id = get_policy_id(policy_name)\n",
    "\n",
    "    if policy_id:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(f\"Policy named '{policy_name}' not found\")\n",
    "\n",
    "    contents = f\"\"\"\n",
    "# The main job for my_project.\n",
    "resources:\n",
    "  jobs:\n",
    "    my_project_job:\n",
    "      name: my_project_job\n",
    "\n",
    "      schedule:\n",
    "        # Run every day at 8:37 AM\n",
    "        quartz_cron_expression: '44 37 8 * * ?'\n",
    "        timezone_id: Europe/Amsterdam\n",
    "\n",
    "      tasks:\n",
    "        - task_key: notebook_task\n",
    "          existing_cluster_id: {current_cluster}\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebook.ipynb\n",
    "        \n",
    "        - task_key: refresh_pipeline\n",
    "          depends_on:\n",
    "            - task_key: notebook_task\n",
    "          pipeline_task:\n",
    "            pipeline_id: ${{resources.pipelines.my_project_pipeline.id}}\n",
    "        \n",
    "        - task_key: main_task\n",
    "          depends_on:\n",
    "            - task_key: refresh_pipeline\n",
    "          existing_cluster_id: {current_cluster}\n",
    "          spark_python_task:\n",
    "            python_file: \"../src/my_project/main.py\"\n",
    "            \"\"\"\n",
    "    with open(\"my_project/resources/my_project_job.yml\", \"w\") as f:\n",
    "      print(f\"Updated the bundle yml file ({f.write(contents)} bytes written).\")\n",
    "\n",
    "  \n",
    "    pipelinecontents = f\"\"\"\n",
    "# The DLT pipeline for my_project.\n",
    "resources:\n",
    "  pipelines:\n",
    "    my_project_pipeline:\n",
    "      name: my_project_pipeline\n",
    "      target: my_project_${{bundle.environment}}\n",
    "      clusters:\n",
    "        - label: default\n",
    "          policy_id: {policy_id}\n",
    "          num_workers: 1\n",
    "        - label: maintenance\n",
    "          policy_id: {policy_id}\n",
    "      libraries:\n",
    "        - notebook:\n",
    "            path: ../src/dlt_pipeline.ipynb\n",
    "\n",
    "      configuration:\n",
    "        bundle.sourcePath: /Workspace/${{workspace.file_path}}/src\n",
    "            \"\"\"\n",
    "    with open(\"my_project/resources/my_project_pipeline.yml\", \"w\") as f:\n",
    "      print(f\"Updated the bundle yml file ({f.write(pipelinecontents)} bytes written).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e543a118-8ab2-4381-9419-e64a214b97dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the next lesson and load credentials\n",
    "LESSON = \"using_cli\"\n",
    "DA = DBAcademyHelper()\n",
    "\n",
    "# Load Credentials\n",
    "(db_token, db_instance) = DA.load_credentials()\n",
    "# Load and configure GitHub credentials\n",
    "(github_token, github_username, repo_name) = DA.load_git_credentials()\n",
    "DA.update_bundle()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-04",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
