{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265632e1-e461-461f-a1f2-81450fd2447c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2728673-864e-4539-95d9-3df6d4b74146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, config_path='../M01 - Streamlining MLOps with Databricks/var/credentials.cfg'):\n",
    "        self.config_path = config_path\n",
    "        self.token = self.get_credentials()\n",
    "\n",
    "    def get_credentials(self):\n",
    "        import configparser\n",
    "        import os\n",
    "\n",
    "        # Check if the file exists\n",
    "\n",
    "        if os.path.exists(self.config_path):\n",
    "            print(\"The configuration file was found in the specified path: M01 - Streamlining MLOps with Databricks/var/credentials.cfg\")\n",
    "\n",
    "        elif not os.path.exists(self.config_path):\n",
    "            print(\"The configuration file was not found in the specified path. Trying secondary path...\")\n",
    "            \n",
    "            config_path = './var/credentials.cfg'\n",
    "            self.config_path = config_path\n",
    "\n",
    "            print(f\"The configuration file was found in the specified path: M04/var/credentials.cfg\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The configuration file was not found at {self.config_path}\")\n",
    "\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(self.config_path)\n",
    "\n",
    "        if 'DEFAULT' not in config:\n",
    "            raise KeyError(\"The section 'DEFAULT' was not found in the configuration file.\")\n",
    "\n",
    "        if 'db_token' not in config['DEFAULT']:\n",
    "            raise KeyError(\"The key 'db_token' was not found in the 'DEFAULT' section of the configuration file.\")\n",
    "\n",
    "        token = config['DEFAULT']['db_token']\n",
    "        \n",
    "        # Print the token for debugging purposes\n",
    "        print(f\"db_token: {token}\")\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10a639a-3c62-45f7-9384-f1392cc8fdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the Token class to get the token\n",
    "token_obj = Token()\n",
    "token = token_obj.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c9cc79-757a-48ee-bbff-b09b9c049567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def init_DA(name, reset=True, pipeline=False):\n",
    "    DA = DBAcademyHelper()\n",
    "    DA.init()\n",
    "\n",
    "    if pipeline:\n",
    "        DA.paths.stream_source = f\"{DA.paths.working_dir}/stream_source\"\n",
    "        DA.paths.storage_location = f\"{DA.paths.working_dir}/storage_location\"\n",
    "        DA.pipeline_name = f\"{DA.unique_name(sep='-')}: ETL Pipeline - {name.upper()}\"\n",
    "        DA.lookup_db = f\"{DA.unique_name(sep='_')}_lookup\"\n",
    "\n",
    "        if reset:  # create lookup tables\n",
    "            import pandas as pd\n",
    "\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {DA.lookup_db}\")\n",
    "            DA.clone_source_table(f\"{DA.lookup_db}.date_lookup\", source_name=\"date-lookup\")\n",
    "            DA.clone_source_table(f\"{DA.lookup_db}.user_lookup\", source_name=\"user-lookup\")\n",
    "\n",
    "            df = spark.createDataFrame(pd.DataFrame([\n",
    "                (\"device_id_not_null\", \"device_id IS NOT NULL\", \"validity\", \"bpm\"),\n",
    "                (\"device_id_valid_range\", \"device_id > 110000\", \"validity\", \"bpm\"),\n",
    "                (\"heartrate_not_null\", \"heartrate IS NOT NULL\", \"validity\", \"bpm\"),\n",
    "                (\"user_id_not_null\", \"user_id IS NOT NULL\", \"validity\", \"workout\"),\n",
    "                (\"workout_id_not_null\", \"workout_id IS NOT NULL\", \"validity\", \"workout\"),\n",
    "                (\"action_not_null\",\"action IS NOT NULL\",\"validity\",\"workout\"),\n",
    "                (\"user_id_not_null\",\"user_id IS NOT NULL\",\"validity\",\"user_info\"),\n",
    "                (\"update_type_not_null\",\"update_type IS NOT NULL\",\"validity\",\"user_info\")\n",
    "            ], columns=[\"name\", \"condition\", \"tag\", \"topic\"]))\n",
    "            df.write.mode(\"overwrite\").saveAsTable(f\"{DA.lookup_db}.rules\")\n",
    "        \n",
    "    return DA\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d11ee1-eb63-45a9-8795-c2f7481eb6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse, urlunsplit\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# set CWD to folder containing notebook; this can go away for DBR14.x+ as its default behavior\n",
    "os.chdir('/Workspace' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()) + '/')\n",
    "\n",
    "# Create Credentials File Method\n",
    "@DBAcademyHelper.add_method\n",
    "def load_credentials(self):\n",
    "    import configparser\n",
    "    c = configparser.ConfigParser()\n",
    "    c.read(filenames=f\"var/credentials.cfg\")\n",
    "    try:\n",
    "        token = c.get('DEFAULT', 'db_token')\n",
    "        host = c.get('DEFAULT', 'db_instance')\n",
    "        os.environ[\"DATABRICKS_HOST\"] = host\n",
    "        os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "    except:\n",
    "        token = ''\n",
    "        host = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}'\n",
    "\n",
    "    return token, host    \n",
    "@DBAcademyHelper.add_method\n",
    "def create_credentials_file(self, db_token, db_instance):\n",
    "    contents = f\"\"\"\n",
    "db_token = \"{db_token}\"\n",
    "db_instance = \"{db_instance}\"\n",
    "    \"\"\"\n",
    "    self.write_to_file(contents, \"credentials.py\")\n",
    "None\n",
    "\n",
    "# Get Credentials Method\n",
    "@DBAcademyHelper.add_method\n",
    "def get_credentials(self):\n",
    "    (current_token, current_host) = self.load_credentials()\n",
    "\n",
    "    @widgets.interact(host=widgets.Text(description='Host:',\n",
    "                                        placeholder='Paste workspace URL here',\n",
    "                                        value=current_host,\n",
    "                                        continuous_update=False),\n",
    "                      token=widgets.Password(description='Token:',\n",
    "                                             placeholder='Paste PAT here',\n",
    "                                             value=current_token,\n",
    "                                             continuous_update=False)\n",
    "    )\n",
    "    def _f(host='', token=''):\n",
    "        u = urlparse(host)\n",
    "        host = urlunsplit((u.scheme, u.netloc, '', '', ''))\n",
    "\n",
    "        if host and token:\n",
    "            os.environ[\"DATABRICKS_HOST\"] = host\n",
    "            os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "\n",
    "            contents = f\"\"\"\n",
    "[DEFAULT]\n",
    "db_token = {token}\n",
    "db_instance = {host}\n",
    "            \"\"\"\n",
    "            # Save credentials in the new location\n",
    "            os.makedirs('./var', exist_ok=True)\n",
    "            with open(\"./var/credentials.cfg\", \"w\") as f:\n",
    "                print(f\"Credentials stored ({f.write(contents)} bytes written).\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0246186f-ecac-4949-aca0-2f7c22fed277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def update_bundle(self):\n",
    "    import ipywidgets as widgets\n",
    "    import os\n",
    "    import subprocess\n",
    "    import json\n",
    "\n",
    "    def try_remove(file_path):\n",
    "        \"\"\"Attempt to remove a file, ignoring errors if it doesn't exist.\"\"\"\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found, skipping: {file_path}\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied, could not remove: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {file_path}: {e}\")\n",
    "\n",
    "    def try_rmdir(dir_path):\n",
    "        \"\"\"Attempt to remove a directory and all its contents.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(dir_path):\n",
    "                for root, dirs, files in os.walk(dir_path, topdown=False):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Removed file: {file_path}\")\n",
    "                    for d in dirs:\n",
    "                        os.rmdir(os.path.join(root, d))\n",
    "                os.rmdir(dir_path)\n",
    "                print(f\"Removed directory: {dir_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found, skipping: {dir_path}\")\n",
    "        except OSError:\n",
    "            print(f\"Could not remove directory (it may still contain files or be locked): {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing directory {dir_path}: {e}\")\n",
    "\n",
    "    # List of files to remove\n",
    "    files_to_remove = [\n",
    "        \"my_project/requirements-dev.txt\",\n",
    "        \"my_project/pytest.ini\",\n",
    "        \"my_project/tests/main_test.py\",\n",
    "        \"my_project/scratch/README.md\",\n",
    "        \"my_project/resources/my_project.job.yml\",\n",
    "        \"my_project/resources/my_project.pipeline.yml\"\n",
    "    ]\n",
    "\n",
    "    # List of directories to remove (scratch removed later)\n",
    "    dirs_to_remove = [\n",
    "        \"my_project/tests\"\n",
    "    ]\n",
    "\n",
    "    # Try to remove each file\n",
    "    for file_path in files_to_remove:\n",
    "        try_remove(file_path)\n",
    "\n",
    "    # Try to remove each directory (except scratch)\n",
    "    for dir_path in dirs_to_remove:\n",
    "        try_rmdir(dir_path)\n",
    "\n",
    "    # Try to remove exploration notebook in Databricks workspace\n",
    "    try:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "\n",
    "        # Initialize Databricks Workspace Client\n",
    "        w = WorkspaceClient()\n",
    "        notebook_path_full = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "        # Extract path up to the course folder\n",
    "        base_path = \"/Workspace/Users/\" + \"/\".join(notebook_path_full.split(\"/\")[2:4])\n",
    "        # Define the workspace path of the notebook\n",
    "        notebook_path = base_path + \"/M04 - Build ML Assets as Code/my_project/scratch/exploration\"\n",
    "        \n",
    "\n",
    "        # Delete the notebook from the workspace\n",
    "        try:\n",
    "            w.workspace.delete(path=notebook_path)\n",
    "            print(f\"Successfully removed Databricks notebook: {notebook_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing notebook: {e}\")\n",
    "\n",
    "\n",
    "        # Now remove scratch directory after notebook deletion\n",
    "        try_rmdir(\"my_project/scratch\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing exploration notebook: {e}\")\n",
    "\n",
    "    # Get the current cluster ID\n",
    "    current_cluster = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "    def get_policy_id(policy_name):\n",
    "        \"\"\"Fetches the policy ID for a given cluster policy name.\"\"\"\n",
    "        result = subprocess.run(\n",
    "            [\"databricks\", \"cluster-policies\", \"list\", \"--output\", \"JSON\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error executing command:\", result.stderr)\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            policies = json.loads(result.stdout)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to decode JSON from command output\")\n",
    "            return None\n",
    "\n",
    "        for policy in policies:\n",
    "            if policy['name'] == policy_name:\n",
    "                return policy['policy_id']\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Specify the policy name\n",
    "    policy_name = \"DBAcademy DLT UC\"\n",
    "\n",
    "    # Get the policy ID\n",
    "    policy_id = get_policy_id(policy_name)\n",
    "\n",
    "    if policy_id:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(f\"Policy named '{policy_name}' not found\")\n",
    "\n",
    "    contents = f\"\"\"\n",
    "# The main job for my_project.\n",
    "resources:\n",
    "  jobs:\n",
    "    my_project_job:\n",
    "      name: my_project_job\n",
    "\n",
    "      schedule:\n",
    "        # Run every day at 8:37 AM\n",
    "        quartz_cron_expression: '44 37 8 * * ?'\n",
    "        timezone_id: Europe/Amsterdam\n",
    "\n",
    "      tasks:\n",
    "        - task_key: notebook_task\n",
    "          existing_cluster_id: {current_cluster}\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebook.ipynb\n",
    "        \n",
    "        - task_key: refresh_pipeline\n",
    "          depends_on:\n",
    "            - task_key: notebook_task\n",
    "          pipeline_task:\n",
    "            pipeline_id: ${{resources.pipelines.my_project_pipeline.id}}\n",
    "        \n",
    "        - task_key: main_task\n",
    "          depends_on:\n",
    "            - task_key: refresh_pipeline\n",
    "          existing_cluster_id: {current_cluster}\n",
    "          spark_python_task:\n",
    "            python_file: \"../src/my_project/main.py\"\n",
    "            \"\"\"\n",
    "\n",
    "    with open(\"my_project/resources/my_project.job.yml\", \"w\") as f:\n",
    "        print(f\"Updated the bundle yml file ({f.write(contents)} bytes written).\")\n",
    "\n",
    "    pipelinecontents = f\"\"\"\n",
    "# The DLT pipeline for my_project.\n",
    "resources:\n",
    "  pipelines:\n",
    "    my_project_pipeline:\n",
    "      name: my_project_pipeline\n",
    "      target: my_project_${{bundle.environment}}\n",
    "      clusters:\n",
    "        - label: default\n",
    "          policy_id: {policy_id}\n",
    "          num_workers: 1\n",
    "        - label: maintenance\n",
    "          policy_id: {policy_id}\n",
    "      libraries:\n",
    "        - notebook:\n",
    "            path: ../src/dlt_pipeline.ipynb\n",
    "\n",
    "      configuration:\n",
    "        bundle.sourcePath: ${{workspace.file_path}}/src\n",
    "            \"\"\"\n",
    "\n",
    "    with open(\"my_project/resources/my_project.pipeline.yml\", \"w\") as f:\n",
    "        print(f\"Updated the bundle yml file ({f.write(pipelinecontents)} bytes written).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae1804e-256a-47c2-be26-edef8d030d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LESSON = \"generate_tokens\"\n",
    "DA = init_DA(LESSON, reset=False)\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def create_credentials_file(self, db_token, db_instance):\n",
    "    contents = f\"\"\"\n",
    "db_token = \"{db_token}\"\n",
    "db_instance = \"{db_instance}\"\n",
    "    \"\"\"\n",
    "    self.write_to_file(contents, \"credentials.py\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e54dda26-d68e-40d5-8d3a-589ee884d6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Get the current cluster ID from Spark configuration\n",
    "cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "print(\"Cluster ID:\", cluster_id)\n",
    "\n",
    "# Path to the variables.yml file\n",
    "variables_file_path = \"./ml_project/resources/variables.yml\"\n",
    "\n",
    "# Read the existing YAML file\n",
    "with open(variables_file_path, \"r\") as file:\n",
    "    variables_data = yaml.safe_load(file)\n",
    "\n",
    "# Update the cluster_id_var.default with the actual cluster ID\n",
    "variables_data[\"variables\"][\"cluster_id_var\"][\"default\"] = cluster_id\n",
    "\n",
    "# Write the updated data back to the YAML file\n",
    "with open(variables_file_path, \"w\") as file:\n",
    "    yaml.dump(variables_data, file, default_flow_style=False)\n",
    "\n",
    "#print(f\"Updated cluster_id_var in {variables_file_path} with Cluster ID: {cluster_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bccd494b-b72d-4d74-bb24-1e11300fc8e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def update_ml_project_bundle(self):\n",
    "#     import shutil\n",
    "#     import yaml\n",
    "\n",
    "#     # Define the source and destination file paths\n",
    "#     source_file = \"./my_project/databricks.yml\"\n",
    "#     destination_folder = \"./ml_project/\"\n",
    "#     destination_file = f\"{destination_folder}/databricks.yml\"\n",
    "\n",
    "#     # Copy the file from the source to the destination\n",
    "#     try:\n",
    "#         shutil.copy(source_file, destination_file)\n",
    "#         #print(f\"File copied successfully from {source_file} to {destination_file}\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Source file {source_file} not found. Please check the path.\")\n",
    "#         return\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while copying the file: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Load the YAML file\n",
    "#     try:\n",
    "#         with open(destination_file, \"r\") as file:\n",
    "#             data = yaml.safe_load(file)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {destination_file}\")\n",
    "#         return\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while reading the file: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Update the 'bundle' name\n",
    "#     if \"bundle\" in data and \"name\" in data[\"bundle\"]:\n",
    "#         old_name = data[\"bundle\"][\"name\"]\n",
    "#         data[\"bundle\"][\"name\"] = \"ml_project\"\n",
    "#         #print(f\"Updated bundle name from '{old_name}' to 'ml_project'\")\n",
    "#     else:\n",
    "#         print(\"The 'bundle' or 'name' field was not found in the YAML file.\")\n",
    "#         return\n",
    "\n",
    "#     # Save the updated YAML back to the file\n",
    "#     try:\n",
    "#         with open(destination_file, \"w\") as file:\n",
    "#             yaml.safe_dump(data, file)\n",
    "#         #print(f\"Updated YAML file saved to: {destination_file}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while saving the file: {e}\")\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04af5c3-55c1-4cb6-a7a7-42c1ddce765c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Load the base path from the bundle validation output\n",
    "notebook_path_full = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "# Extract path up to the course folder\n",
    "base_path = \"/Workspace/Users/\" + \"/\".join(notebook_path_full.split(\"/\")[2:4])\n",
    "# Define the workspace path of the notebook\n",
    "#notebook_path = base_path + \"/M04 - Build ML Assets as Code/my_project/scratch/exploration\"\n",
    "bundle_workspace_path = base_path + \"/M04 - Build ML Assets as Code/ml_project\"\n",
    "src_relative_path = \"src\"  # This is the folder inside your project\n",
    "\n",
    "# Full absolute path to src in workspace\n",
    "src_workspace_path = f\"{bundle_workspace_path}/{src_relative_path}\"\n",
    "\n",
    "# Load the YAML file\n",
    "yaml_file = base_path + \"/M04 - Build ML Assets as Code/ml_project/resources/ml_project_job.job.yml\"\n",
    "\n",
    "with open(yaml_file, \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Update all notebook_task and spark_python_task paths\n",
    "for task in data[\"resources\"][\"jobs\"][\"ml_project_job\"][\"tasks\"]:\n",
    "    task_key = task.get(\"task_key\", \"unknown\")\n",
    "    if \"notebook_task\" in task:\n",
    "        notebook_name = os.path.basename(task[\"notebook_task\"][\"notebook_path\"])\n",
    "        task[\"notebook_task\"][\"notebook_path\"] = f\"{src_workspace_path}/{notebook_name}\"\n",
    "    elif \"spark_python_task\" in task:\n",
    "        python_file = os.path.basename(task[\"spark_python_task\"][\"python_file\"])\n",
    "        task[\"spark_python_task\"][\"python_file\"] = f\"{src_workspace_path}/{python_file}\"\n",
    "\n",
    "# Save the updated YAML\n",
    "with open(yaml_file, \"w\") as f:\n",
    "    yaml.safe_dump(data, f, default_flow_style=False)\n",
    "\n",
    "print(\"Updated databricks.yml with correct notebook and script paths.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-4.1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
