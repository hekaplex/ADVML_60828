{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f39669e-723e-46c6-a472-d11b9d366c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dbacademy import dbgems\n",
    "\n",
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Represents pipeline settings with option to provide as JSON definition.\n",
    "\n",
    "      Attributes:\n",
    "          Configurable: name, storage, target, configuration, notebooks, policy\n",
    "          Auto-generated: libraries (from notebooks), clusters (from policy)\n",
    "          Non-configurable: continuous (False), development (True), photon (False)\n",
    "\n",
    "      Methods:\n",
    "          get_pipeline_settings(convert_to_json=False): \n",
    "              Returns pipeline settings as dictonary or JSON string\n",
    "\n",
    "          print_pipeline_config(): \n",
    "              Displays parameters as text and input textboxes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, notebooks, configuration, storage, target, policy=None, photon=False):\n",
    "        \"\"\"\n",
    "        Defines PipelineConfig attributes.    \n",
    "\n",
    "            Use input values to set attributes: name, storage, target\n",
    "\n",
    "            Modify input values to set attributes:\n",
    "                configuration: add \"spark.master\": \"local[*]\" to configuration\n",
    "                notebooks: convert to list of notebook paths\n",
    "                libraries: create list of dictionaries from notebook paths\n",
    "                clusters: create based on policy\n",
    "            \n",
    "            Use default values to set attributes:\n",
    "                continuous: False\n",
    "                development: True\n",
    "                photon: False\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.storage = storage\n",
    "        self.target = target\n",
    "        self.configuration = {**configuration, \"spark.master\": \"local[*]\"}\n",
    "\n",
    "        basepath = dbgems.get_notebook_dir()\n",
    "\n",
    "        self.notebooks = [f\"{basepath}/{n}\" for n in notebooks]\n",
    "        self.libraries = [{\"notebook\": {\"path\": n}} for n in self.notebooks]\n",
    "        self.policy = policy\n",
    "    \n",
    "        if policy:\n",
    "            clusters = [{\"num_workers\": 0, \"policy_id\": policy.get(\"policy_id\")}]\n",
    "        else:\n",
    "            clusters = [{\"num_workers\": 0}]\n",
    "        self.clusters = clusters\n",
    "            \n",
    "        self.continuous = False\n",
    "        self.development = True\n",
    "        self.photon = photon\n",
    "\n",
    "\n",
    "    def get_pipeline_settings(self, convert_to_json=False):\n",
    "        \"\"\"\n",
    "        Returns pipeline settings as dictonary or JSON string.\n",
    "\n",
    "        :param convert_to_json: if True, return JSON string; if False (default), return dictionary\n",
    "        :return: pipeline settings as a dictionary or JSON string, based on convert_to_json\n",
    "        \"\"\"\n",
    "        params = dict()\n",
    "        params[\"name\"] = self.name\n",
    "        params[\"target\"] = self.target\n",
    "        params[\"storage\"] = self.storage\n",
    "        params[\"libraries\"] = self.libraries    \n",
    "        params[\"configuration\"] = self.configuration\n",
    "        params[\"clusters\"] = self.clusters        \n",
    "        params[\"continuous\"] = self.continuous\n",
    "        params[\"development\"] = self.development\n",
    "        params[\"photon\"] = self.photon\n",
    "\n",
    "        if not convert_to_json:\n",
    "            return params\n",
    "        else:\n",
    "            import json\n",
    "            return json.dumps(self.get_pipeline_settings())\n",
    "\n",
    "\n",
    "    def get_config_values(self):\n",
    "        \"\"\"\n",
    "        Returns a subset of pipeline parameters as a list of (name, value) tuples.\n",
    "        This is meant to be displayed using DBAcademyHelper.display_config_values to provide instructions.\n",
    "\n",
    "        Included settings:\n",
    "            Pipeline Name: <name>\n",
    "            Storage Location: <path>\n",
    "            Target: <name>\n",
    "            Policy: <name>\n",
    "\n",
    "            All configuration properties, excluding \"spark.master\"; usually includes \"source\"\n",
    "                source: <path>\n",
    "\n",
    "            All notebook paths, numbered starting from 1:\n",
    "                Notebook #1 Path: <path>\n",
    "                Notebook #2 Path: <path>\n",
    "\n",
    "        See also DBAcademyHelper.display_config_values\n",
    "        \"\"\"\n",
    "\n",
    "        config_values = [\n",
    "            (\"Pipeline Name\", self.name),\n",
    "            (\"Storage Location\", self.storage),\n",
    "            (\"Target\", self.target),            \n",
    "            (\"Policy\", \"None\" if self.policy is None else self.policy.get(\"name\"))\n",
    "        ]\n",
    "        for key, val in self.configuration.items():\n",
    "            if key != \"spark.master\": config_values.append((key, val))\n",
    "            \n",
    "        for i, path in enumerate(self.notebooks):\n",
    "            config_values.append((f\"Notebook #{i+1} Path\", path))\n",
    "            \n",
    "        return config_values\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3854f62f-bc32-4246-9649-df5e8a3fc1e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define helper functions to configure, create, and trigger pipelines with DBAcademyHelper\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def configure_pipeline(self, notebooks, name=None, source=None, configuration=None, photon=False):\n",
    "    \"\"\"\n",
    "    Creates PipelineConfig object for provided notebooks and optional settings.\n",
    "    Defines parameter values using DBAcademyHelper:\n",
    "        Attributes:\n",
    "            target\n",
    "            paths.storage\n",
    "            pipeline_name (if name not provided)\n",
    "            source (if source not provided)\n",
    "        Methods:\n",
    "            get_dlt_policy()\n",
    "    :param notebooks (list): list of notebook paths, relative to current notebook\n",
    "    :param source (str): value for \"source\" configuration property\n",
    "    :param configuration (dict): overrides source to include more than one \"source\" configuration property (optional)\n",
    "    :param name (str): overrides self.pipeline_name to create pipeline name\n",
    "    \"\"\"\n",
    "    \n",
    "    notebooks = [f\"Pipeline/{f}\" for f in notebooks]\n",
    "    \n",
    "    if name is None:\n",
    "        name = self.pipeline_name\n",
    "\n",
    "    if configuration:\n",
    "        assert source is None, \"source is ignored when configuration is provided\"\n",
    "    elif source:\n",
    "        configuration = {\"source\": source}\n",
    "    else:\n",
    "        configuration = {\"source\": self.paths.stream_source}\n",
    "    \n",
    "\n",
    "    self.pipeline_config = PipelineConfig(\n",
    "        name=name,\n",
    "        notebooks=notebooks,        \n",
    "        configuration=configuration,\n",
    "        storage=self.paths.storage_location,\n",
    "        target=self.schema_name,\n",
    "        policy=self.get_dlt_policy(),\n",
    "        photon=photon\n",
    "    )\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def get_dlt_policy(self):\n",
    "    \"\"\"\n",
    "    Returns cluster policy for DLT pipelines, created by Workspace-Setup script.\n",
    "    Get cluster policy using name provided by DBAcademy ClustersHelper.POLICY_DLT_ONLY\n",
    "    \"\"\"\n",
    "    from dbacademy import common\n",
    "    from dbacademy.dbhelper import ClustersHelper\n",
    "    dlt_policy = self.client.cluster_policies.get_by_name(ClustersHelper.POLICY_DLT_ONLY)\n",
    "    if dlt_policy is None: \n",
    "        common.print_warning(\"WARNING: Policy Not Found\", \n",
    "        f\"Could not find the cluster policy \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\".\\nPlease run the notebook Includes/Workspace-Setup to address this error.\")\n",
    "    return dlt_policy\n",
    "\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def generate_pipeline(self, config=None, show_validate=False):\n",
    "    \"\"\"\n",
    "    Creates pipeline using provided PipelineConfig or DBAcademyHelper.pipeline_config.\n",
    "\n",
    "    See also DBAcademyHelper.create_pipeline\n",
    "    See also DBAcademyHelper.validate_pipeline_config\n",
    "    See also PipelineConfig.print_pipeline_config\n",
    "\n",
    "    :param config: overrides DBAcademyHelper.pipeline_config (optional)\n",
    "    :param show_validate: if False (default), hide validation results\n",
    "    :return: pipeline ID\n",
    "    \"\"\"\n",
    "    if config is None: config = self.pipeline_config\n",
    "     \n",
    "    self.display_config_values(config.get_config_values())\n",
    "\n",
    "    self.pipeline_id = self.create_pipeline(config)\n",
    "\n",
    "    if show_validate: self.validate_pipeline_config(config, display=False)    \n",
    "\n",
    "    return self.pipeline_id\n",
    "\n",
    "\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def create_pipeline(self, config=None):\n",
    "    \"\"\"\n",
    "    Create or replace pipeline with the provided configuration, then display link to Pipeline UI.\n",
    "    See also DBAcademyHelper.create_pipeline_from_settings.\n",
    "\n",
    "    Sets DBAcademyHelper attribute:\n",
    "        pipeline_id: Pipeline ID of the created pipeline\n",
    "\n",
    "    :param config: PipelineConfig to override self.pipeline_config (optional)\n",
    "    :return: the pipeline ID of the created pipeline\n",
    "    \"\"\"\n",
    "    if not config: \n",
    "        config = self.pipeline_config\n",
    "    settings = config.get_pipeline_settings()\n",
    "\n",
    "    self.pipeline_id = self.create_pipeline_from_settings(settings)\n",
    "    \n",
    "    pipeline_url = f\"{dbgems.get_workspace_url()}#joblist/pipelines/{self.pipeline_id}\"\n",
    "    displayHTML(f\"\"\"Created the pipeline \"{settings[\"name\"]}\": <a href={pipeline_url}>{self.pipeline_id}</a>\"\"\")\n",
    "\n",
    "    return self.pipeline_id\n",
    "        \n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def create_pipeline_from_settings(self, settings):\n",
    "    \"\"\"\n",
    "    Create or replace pipeline with the provided configuration, then return pipeline ID.\n",
    "    See also DBAcademyHelper.client.pipelines.\n",
    "\n",
    "    :param settings: dictionary of pipeline settings\n",
    "    :return: pipeline ID of the created pipeline\n",
    "    \"\"\"\n",
    "    self.client.pipelines().delete_by_name(settings[\"name\"]) \n",
    "    response = self.client.pipelines().create(**settings)\n",
    "\n",
    "    return response.get(\"pipeline_id\")\n",
    "\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def start_pipeline(self, pipeline_id=None, blocking=True):\n",
    "    \"\"\"\n",
    "    Starts the pipeline and then blocks until it has completed, failed or was canceled\n",
    "    :param pipeline_id: overrides self.pipeline_id to identify pipeline (optional)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from dbacademy.dbrest import DBAcademyRestClient\n",
    "\n",
    "    if not pipeline_id: pipeline_id = self.pipeline_id\n",
    "    \n",
    "    client = DBAcademyRestClient() \n",
    "    start = client.pipelines().start_by_id(pipeline_id)  # start pipeline\n",
    "    update_id = start.get(\"update_id\")\n",
    "\n",
    "    # get status and block until done\n",
    "    update = client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
    "    state = update.get(\"update\").get(\"state\")\n",
    "\n",
    "    if blocking:\n",
    "      while state not in [\"COMPLETED\", \"FAILED\", \"CANCELED\"]:\n",
    "          duration = 15\n",
    "          time.sleep(duration)\n",
    "          print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n",
    "          update = client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
    "          state = update.get(\"update\").get(\"state\")\n",
    "\n",
    "      print(f\"The final state is {state}.\")\n",
    "\n",
    "      assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\"\n",
    "\n",
    "    else:\n",
    "      print(f\"The current state is {state}.\")\n",
    "      \n",
    "    return update_id\n",
    "\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def validate_pipeline_config(self, config, display=True):\n",
    "    \"\"\"\n",
    "    Validate the configuration of the pipeline.\n",
    "    \n",
    "    - validate pipeline with name exists\n",
    "    - validate settings for storage, target\n",
    "    - validate libraries has correct count, includes each notebook\n",
    "    - validate configuration parameters for source, spark.master\n",
    "    - validate cluster settings: cluster count, autoscaling disabled, cluster policy, worker count\n",
    "    - validate settings for development mode, current channel, pipeline triggered mode\n",
    "\n",
    "    :param config: PipelineConfig to identify and validate pipeline\n",
    "    :param display: if True, displays validation results in a table\n",
    "    \"\"\"\n",
    "    from dbacademy import common\n",
    "    from dbacademy.dbhelper import ClustersHelper\n",
    "    suite = self.tests.new(\"Pipeline Config\")\n",
    "\n",
    "    # validate pipeline with name exists        \n",
    "    pipeline = self.client.pipelines().get_by_name(config.name)        \n",
    "    suite.test_not_none(lambda: pipeline, \n",
    "                        description=f\"Create the pipeline \\\"<b>{config.name}</b>\\\".\", \n",
    "                        hint=\"Double check the spelling.\")\n",
    "    if pipeline is None: pipeline = {}\n",
    "    spec = pipeline.get(\"spec\", {})\n",
    "\n",
    "    # validate storage location and target\n",
    "    suite.test_equals(lambda: spec.get(\"storage\", None), config.storage, \n",
    "                      description=f\"Set the storage location to \\\"<b>{config.storage}</b>\\\".\", \n",
    "                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n",
    "    suite.test_equals(lambda: spec.get(\"target\", None), config.target, \n",
    "                      description=f\"Set the target to \\\"<b>{config.target}</b>\\\".\", \n",
    "                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n",
    "\n",
    "    # validate notebooks\n",
    "    libraries = [l.get(\"notebook\", {}).get(\"path\") for l in spec.get(\"libraries\", [])]\n",
    "    config_libraries = [l.get(\"notebook\", {}).get(\"path\") for l in config.libraries]\n",
    "    def test_notebooks():\n",
    "        if libraries is None: return False\n",
    "        if len(libraries) != len(config.notebooks): return False\n",
    "        for library in libraries:\n",
    "            if library not in config_libraries: return False\n",
    "        return True\n",
    "    hint = f\"\"\"Found the following {len(libraries)} notebook(s):<ul style=\"margin-top:0\">\"\"\"\n",
    "    for library in libraries: hint += f\"\"\"<li>{library}</li>\"\"\"\n",
    "    hint += \"</ul>\"\n",
    "    suite.test(test_function=test_notebooks, actual_value=libraries, description=\"Configure the Notebook library.\", hint=hint)\n",
    "\n",
    "    # validate configuration parameters: source, spark.master\n",
    "    suite.test_equals(lambda: spec.get(\"configuration\", {}).get(\"source\"), config.configuration.get(\"source\"), \n",
    "                      description=f\"Set the \\\"<b>source</b>\\\" configuration parameter to \\\"<b>{config.configuration.get('source')}</b>\\\".\", \n",
    "                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n",
    "    suite.test_equals(lambda: spec.get(\"configuration\", {}).get(\"spark.master\"), \"local[*]\", \n",
    "                      description=f\"Set the \\\"<b>spark.master</b>\\\" configuration parameter to \\\"<b>local[*]</b>\\\".\", \n",
    "                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n",
    "    # suite.test_length(lambda: spec.get(\"configuration\", {}), 2, description=f\"Set the two configuration parameters.\", hint=f\"Found [[LEN_ACTUAL_VALUE]] configuration parameter(s).\")                      \n",
    "\n",
    "    # validate cluster settings: cluster count, autoscaling disabled, cluster policy, worker count\n",
    "    suite.test_length(lambda: spec.get(\"clusters\"), expected_length=1, \n",
    "                      description=f\"Expected one and only one cluster definition.\", \n",
    "                      hint=\"Edit the config via the JSON interface to remove the second+ cluster definitions\")\n",
    "    suite.test_is_none(lambda: spec.get(\"clusters\")[0].get(\"autoscale\"), description=f\"Autoscaling should be disabled.\")\n",
    "\n",
    "    def test_cluster_policy():\n",
    "        cluster = spec.get(\"clusters\")[0]\n",
    "        policy_id = cluster.get(\"policy_id\")\n",
    "        if policy_id is None: common.print_warning(\"WARNING: Policy Not Set\", \n",
    "                                                   f\"Expected the policy to be set to \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\".\")\n",
    "        else:\n",
    "            policy_name = self.client.cluster_policies.get_by_id(policy_id).get(\"name\")\n",
    "            if policy_id != self.get_dlt_policy().get(\"policy_id\"):\n",
    "                common.print_warning(\"WARNING: Incorrect Policy\", \n",
    "                                     f\"Expected the policy to be set to \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\", found \\\"{policy_name}\\\".\")\n",
    "        return True\n",
    "\n",
    "    suite.test(test_function=test_cluster_policy, actual_value=None, \n",
    "               description=f\"The cluster policy should be <b>\\\"{ClustersHelper.POLICY_DLT_ONLY}\\\"</b>.\")\n",
    "    suite.test_equals(lambda: spec.get(\"clusters\")[0].get(\"num_workers\"), 0, \n",
    "                      description=f\"The number of spark workers should be <b>0</b>.\", hint=f\"Found [[ACTUAL_VALUE]] workers.\")\n",
    "\n",
    "    # validate pipeline development mode, current channel, pipeline triggered mode\n",
    "    suite.test_true(lambda: spec.get(\"development\") != self.is_smoke_test(), \n",
    "                    description=f\"The pipeline mode should be set to \\\"<b>Development</b>\\\".\")\n",
    "    suite.test(test_function=lambda: {spec.get(\"channel\") is None or spec.get(\"channel\").upper() == \"CURRENT\"}, \n",
    "               actual_value=spec.get(\"channel\"), \n",
    "               description=f\"The channel should be set to \\\"<b>Current</b>\\\".\", hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\"\")\n",
    "    suite.test_false(lambda: spec.get(\"continuous\"), \n",
    "                     description=f\"Expected the Pipeline mode to be \\\"<b>Triggered</b>\\\".\", \n",
    "                     hint=f\"Found \\\"<b>Continuous</b>\\\".\")\n",
    "\n",
    "    # validate photon enabled\n",
    "    # suite.test_true(lambda: spec.get(\"photon\"), description=f\"Photon should be enabled.\")                     \n",
    "\n",
    "    if display: suite.display_results()\n",
    "    assert suite.passed, \"One or more tests failed; please double check your work.\"  \n",
    "\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "_pipeline_config",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
