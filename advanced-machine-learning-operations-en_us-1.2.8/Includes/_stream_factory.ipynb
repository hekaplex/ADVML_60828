{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8dcf4b-c87f-4f51-84c4-bd8d54deb22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dbacademy import dbgems\n",
    "\n",
    "class StreamFactory:\n",
    "    \"\"\"\n",
    "    Incrementally loads data from a source dataset to a target directory.\n",
    "\n",
    "    Attributes:\n",
    "        source_dir: source path for datasets\n",
    "        target_dir: landing path for streams\n",
    "        max_batch: total number of batches before exhausting stream\n",
    "        batch: counter used to track current batch number\n",
    "\n",
    "    Methods:\n",
    "        load(continuous=False)\n",
    "        load_batch(target, batch, end): dataset-specific function provided at instantiation\n",
    "    \"\"\"\n",
    "    def __init__(self, source_dir, target_dir, load_batch, max_batch):\n",
    "        self.source_dir = source_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.load_batch = load_batch\n",
    "        self.max_batch = max_batch\n",
    "        self.batch = 1     \n",
    "        \n",
    "    def load(self, continuous=False):\n",
    "        \n",
    "        if self.batch > self.max_batch:\n",
    "            print(\"Data source exhausted\", end=\"...\")\n",
    "            total = 0                \n",
    "        elif continuous == True:\n",
    "            print(f\"Loading all batches to the stream\", end=\"...\")\n",
    "            total = self.load_batch(self.source_dir, self.target_dir, self.batch, self.max_batch)\n",
    "            self.batch = self.max_batch + 1\n",
    "        else:\n",
    "            print(f\"Loading batch #{self.batch} to the stream\", end=\"...\")\n",
    "            total = self.load_batch(self.source_dir, self.target_dir, self.batch, self.batch)\n",
    "            self.batch = self.batch + 1\n",
    "            \n",
    "        print(f\"Loaded {total:,} records\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb0ece4-e264-41ee-9656-20eb72816cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_user_reg_batch(datasets_dir, target_dir, batch_start, batch_end):\n",
    "\n",
    "    source_dataset = f\"{datasets_dir}/user-reg\"\n",
    "    target_path = f\"{target_dir}/user_reg\"\n",
    "\n",
    "    df = (spark.read\n",
    "          .format(\"json\")\n",
    "          .schema(\"device_id long, mac_address string, registration_timestamp double, user_id long\")\n",
    "          .load(source_dataset)\n",
    "          .withColumn(\"date\", F.col(\"registration_timestamp\").cast(\"timestamp\").cast(\"date\"))\n",
    "          .withColumn(\"batch\", F.when(F.col(\"date\") < \"2019-12-01\", F.lit(1)).otherwise(F.dayofmonth(F.col(\"date\"))+1))\n",
    "          .filter(f\"batch >= {batch_start}\")\n",
    "          .filter(f\"batch <= {batch_end}\")          \n",
    "          .drop(\"date\", \"batch\")\n",
    "          .cache())\n",
    "\n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()        \n",
    "\n",
    "\n",
    "def load_cdc_batch(datasets_dir, target_dir, batch_start, batch_end):\n",
    "    \n",
    "    source_dataset = f\"{datasets_dir}/pii/raw\"\n",
    "    target_path = f\"{target_dir}/cdc\"\n",
    "\n",
    "    df = (spark.read\n",
    "      .load(source_dataset)\n",
    "      .filter(f\"batch >= {batch_start}\")\n",
    "      .filter(f\"batch <= {batch_end}\")\n",
    "    )   \n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()\n",
    "\n",
    "\n",
    "def load_daily_batch(datasets_dir, target_dir, batch_start, batch_end):\n",
    "    \n",
    "    source_path = f\"{datasets_dir}/bronze\"\n",
    "    target_path = f\"{target_dir}/daily\"\n",
    "\n",
    "\n",
    "    df = (spark.read\n",
    "      .load(source_path)\n",
    "      .withColumn(\"day\", \n",
    "        F.when(F.col(\"date\") <= '2019-12-01', 1)\n",
    "        .otherwise(F.dayofmonth(\"date\")))\n",
    "      .filter(F.col(\"day\") >= batch_start)\n",
    "      .filter(F.col(\"day\") <= batch_end)\n",
    "      .drop(\"date\", \"week_part\", \"day\")  \n",
    "    )\n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()\n",
    "\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "_stream_factory",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
