{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0b9486-9b9f-42e8-aee3-65c47e8a8be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212361eb-e623-431b-a7a1-dc0be0a254d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0418ee7-a918-48ea-b991-7d5111cee8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_init\n",
    "def create_diabetes_table_from_marketplace(self):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from pyspark.sql import DataFrame, functions as F, types as T\n",
    "    from pyspark.sql.functions import col, monotonically_increasing_id, lit\n",
    "\n",
    "    spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "    spark.sql(f\"USE SCHEMA {DA.schema_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS diabetes\")\n",
    "\n",
    "\n",
    "    shared_volume_name = 'cdc-diabetes'\n",
    "    csv_name = 'diabetes_binary_5050split_BRFSS2015'\n",
    "\n",
    "    # # Load in the dataset we wish to work on\n",
    "    data_path = f\"{DA.paths.datasets.cdc_diabetes}/{shared_volume_name}/{csv_name}.csv\"\n",
    "\n",
    "    diabetes_df = spark.read.csv(data_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "    # Convert all columns to double type\n",
    "    for column in diabetes_df.columns:\n",
    "        diabetes_df = diabetes_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "    # Add a unique ID column\n",
    "    diabetes_df = diabetes_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "    diabetes_df.write.format('delta').saveAsTable('diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea60abe9-6d4a-4f94-90c7-804db096b4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_init\n",
    "def create_diabetes_features_table(self):\n",
    "    from pyspark.sql.functions import col, monotonically_increasing_id, lit\n",
    "    from pyspark.sql.types import DoubleType, StructType, StructField, StringType, IntegerType, LongType, DoubleType, ArrayType, MapType\n",
    "\n",
    "    # Set the catalog and schema\n",
    "    spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "    spark.sql(f\"USE SCHEMA {DA.schema_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS baseline_features\")\n",
    "\n",
    "    baseline_features_df = spark.read.format('delta').table('diabetes')\n",
    "\n",
    "    baseline_features_df = baseline_features_df.withColumn('model_id', lit(0)).withColumn('labeled_data', col('Diabetes_binary').cast(DoubleType()))\n",
    "    # Write the data back to Delta table\n",
    "    baseline_features_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", True) \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .saveAsTable(f\"baseline_features__no_drift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f20c7e5d-0b05-499f-81a8-46605fde39fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_init\n",
    "def create_artificial_drift(self):\n",
    "    from pyspark.sql.functions import lit, col\n",
    "\n",
    "    baseline_pdf = spark.read.format('delta').table('baseline_features__no_drift').toPandas()\n",
    "\n",
    "    # 1. Data Drift\n",
    "    # BMI: Add a positive shift to simulate an increase in average BMI\n",
    "    baseline_pdf['BMI'] = baseline_pdf['BMI'] + 15\n",
    "\n",
    "    # Age: Add random noise to simulate measurement inconsistencies\n",
    "    baseline_pdf['Age'] = baseline_pdf['Age'] + np.random.normal(0, 2, size=len(baseline_pdf))\n",
    "\n",
    "    # PhysActivity: Set a portion of the values to 0 to simulate reduced physical activity\n",
    "    phys_activity_indices = baseline_pdf.sample(frac=0.2).index  # Select 20% of the data\n",
    "    baseline_pdf.loc[phys_activity_indices, 'PhysActivity'] = 0\n",
    "\n",
    "    # 2. Concept Drift\n",
    "    # HighBP: Change labels for individuals with HighBP == 1\n",
    "    high_bp_indices = baseline_pdf[baseline_pdf['HighBP'] == 1].index\n",
    "    baseline_pdf.loc[high_bp_indices, 'labeled_data'] = np.random.choice(\n",
    "        [0, 1], p=[0.4, 0.6], size=len(high_bp_indices)\n",
    "    )\n",
    "\n",
    "    # MentHlth: Weaken the effect of poor mental health days on the target variable\n",
    "    ment_hlth_indices = baseline_pdf[baseline_pdf['MentHlth'] > 10].index\n",
    "    baseline_pdf.loc[ment_hlth_indices, 'labeled_data'] = np.random.choice(\n",
    "        [0, 1], p=[0.7, 0.3], size=len(ment_hlth_indices)\n",
    "    )\n",
    "\n",
    "    # 3. Model Quality Drift\n",
    "    # HeartDiseaseorAttack: Add random noise to reduce signal\n",
    "    baseline_pdf['HeartDiseaseorAttack'] = np.random.choice(\n",
    "        [0, 1], p=[0.7, 0.3], size=len(baseline_pdf)\n",
    "    )\n",
    "\n",
    "    # Education: Shuffle values to remove predictive power\n",
    "    baseline_pdf['Education'] = np.random.permutation(baseline_pdf['Education'])\n",
    "\n",
    "    # 4. Bias Drift\n",
    "    # Sex: Skew the proportion of Sex values\n",
    "    baseline_pdf['Sex'] = np.random.choice([0, 1], p=[0.8, 0.2], size=len(baseline_pdf))\n",
    "\n",
    "    # Income: Remove higher-income groups to underrepresent them\n",
    "    baseline_pdf.loc[baseline_pdf['Income'] > 4, 'Income'] = 4\n",
    "\n",
    "    # Simulating baseline predictions and inference predictions\n",
    "    baseline_pdf['Diabetes_binary'] = np.random.choice([0, 1], p=[0.5, 0.5], size=len(baseline_pdf))\n",
    "\n",
    "    # Method 1: Slightly change feature distributions in the inference data\n",
    "    baseline_pdf['BMI'] = baseline_pdf['BMI'] + np.random.normal(0, 1, size=len(baseline_pdf))\n",
    "\n",
    "    baseline_df = spark.createDataFrame(baseline_pdf)\n",
    "\n",
    "    cols_to_double = ['Diabetes_binary', 'Sex', 'HeartDiseaseorAttack']\n",
    "    # Convert all columns to double type\n",
    "    for column in cols_to_double:\n",
    "        baseline_df = baseline_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "    baseline_df.write.format('delta').mode('overwrite').saveAsTable('baseline_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0379e5c6-3f0c-4621-a415-bf72862cf977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_init\n",
    "def create_inference_table_from_marketplace(self):\n",
    "\n",
    "  from pyspark.sql.functions import col, lit, to_date, from_json\n",
    "  from pyspark.sql.types import TimestampType, StructType, StructField, StringType, IntegerType, LongType, DoubleType, ArrayType, MapType\n",
    "  import pandas as pd\n",
    "\n",
    "  spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "  spark.sql(f\"USE SCHEMA {DA.schema_name}\")\n",
    "\n",
    "  shared_volume_name = 'monitoring'\n",
    "  csv_name = 'inference_table'\n",
    "\n",
    "  # # Load in the dataset we wish to work on\n",
    "  data_path = f\"{DA.paths.datasets.diabetes_inference}/{shared_volume_name}/{csv_name}.csv\"\n",
    "\n",
    "#   data_path = '/Volumes/databricks_inference_dataset_for_machine_learning_operations_associate_course/v01/monitoring/inference_table.csv'\n",
    "  # Load the CSV file directly into a Spark DataFrame\n",
    "  inferences_pdf = pd.read_csv(\n",
    "      data_path,\n",
    "      header=0,\n",
    "      sep=\",\"\n",
    "  )\n",
    "\n",
    "  inferences_df = spark.createDataFrame(inferences_pdf)\n",
    "\n",
    "  # Define the schema for the request_metadata and response columns as a MapType or Array\n",
    "  metadata_schema = MapType(StringType(), StringType())\n",
    "  response_schema = StructType([\n",
    "      StructField(\"predictions\", ArrayType(DoubleType()))\n",
    "  ])\n",
    "\n",
    "  # Correct type casting for columns\n",
    "  inferences_df = inferences_df.withColumn(\"client_request_id\", col(\"client_request_id\").cast(StringType()))\n",
    "  inferences_df = inferences_df.withColumn(\"databricks_request_id\", col(\"databricks_request_id\").cast(StringType()))\n",
    "  inferences_df = inferences_df.withColumn(\"timestamp_ms\", col(\"timestamp_ms\").cast(LongType()))\n",
    "  inferences_df = inferences_df.withColumn(\"status_code\", col(\"status_code\").cast(IntegerType()))\n",
    "  inferences_df = inferences_df.withColumn(\"execution_time_ms\", col(\"execution_time_ms\").cast(LongType()))\n",
    "  inferences_df = inferences_df.withColumn(\"sampling_fraction\", col(\"sampling_fraction\").cast(DoubleType()))\n",
    "\n",
    "  # Parse the date column into DateType\n",
    "  inferences_df = inferences_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "  # Ensure `request_metadata`, `request`, and `response` are treated as strings before parsing\n",
    "  inferences_df = inferences_df.withColumn(\"request_metadata\", col(\"request_metadata\").cast(StringType()))\n",
    "  inferences_df = inferences_df.withColumn(\"request\", col(\"request\").cast(StringType()))\n",
    "  inferences_df = inferences_df.withColumn(\"response\", col(\"response\").cast(StringType()))\n",
    "\n",
    "  # Parse the JSON string in `request_metadata` into a MapType\n",
    "  inferences_df = inferences_df.withColumn(\"request_metadata\", from_json(\"request_metadata\", metadata_schema))\n",
    "  inferences_df = inferences_df.withColumn(\"response\", from_json(\"response\", response_schema))\n",
    "\n",
    "  # Drop the existing Delta table if it exists to prevent schema conflicts\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS model_inference_table\")\n",
    "\n",
    "  # Save the DataFrame as a Delta table in the specified schema and catalog\n",
    "  inferences_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"model_inference_table\")\n",
    "\n",
    "  print(\"Inference table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3c30fa-67e9-4672-941a-5fbf9a8f3b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_init\n",
    "def create_model_logs(self):\n",
    "\n",
    "  from pyspark.sql import functions as F\n",
    "  import json\n",
    "  import pandas as pd\n",
    "  from pyspark.sql.dataframe import DataFrame\n",
    "  import pyspark.sql.types as T\n",
    "  from pyspark.sql.types import TimestampType, StructType, StructField, StringType, IntegerType, LongType, DoubleType, ArrayType, MapType\n",
    "  # Define a UDF to handle JSON unpacking\n",
    "  def convert_to_record_json(json_str: str) -> str:\n",
    "      \"\"\"\n",
    "      Converts records from different accepted JSON formats into a common, record-oriented\n",
    "      DataFrame format which can be parsed by the PySpark function `from_json`.\n",
    "      \n",
    "      :param json_str: The JSON string containing the request or response payload.\n",
    "      :return: A JSON string containing the converted payload in a record-oriented format.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Attempt to parse the JSON string\n",
    "          request = json.loads(json_str)\n",
    "      except json.JSONDecodeError:\n",
    "          # If parsing fails, return the original string\n",
    "          return json_str\n",
    "\n",
    "      output = []\n",
    "      if isinstance(request, dict):\n",
    "          # Handle different JSON formats and convert to a common format\n",
    "          if \"dataframe_records\" in request:\n",
    "              output.extend(request[\"dataframe_records\"])\n",
    "          elif \"dataframe_split\" in request:\n",
    "              dataframe_split = request[\"dataframe_split\"]\n",
    "              output.extend([dict(zip(dataframe_split[\"columns\"], values)) for values in dataframe_split[\"data\"]])\n",
    "          elif \"instances\" in request:\n",
    "              output.extend(request[\"instances\"])\n",
    "          elif \"inputs\" in request:\n",
    "              output.extend([dict(zip(request[\"inputs\"], values)) for values in zip(*request[\"inputs\"].values())])\n",
    "          elif \"predictions\" in request:\n",
    "              output.extend([{'predictions': prediction} for prediction in request[\"predictions\"]])\n",
    "          return json.dumps(output)\n",
    "      else:\n",
    "          # If the format is unsupported, return the original string\n",
    "          return json_str\n",
    "\n",
    "  @F.pandas_udf(T.StringType())\n",
    "  def json_consolidation_udf(json_strs: pd.Series) -> pd.Series:\n",
    "      \"\"\"\n",
    "      A UDF to apply the JSON conversion function to every request/response.\n",
    "      \n",
    "      :param json_strs: A Pandas Series containing JSON strings.\n",
    "      :return: A Pandas Series with the converted JSON strings.\n",
    "      \"\"\"\n",
    "      return json_strs.apply(convert_to_record_json)\n",
    "\n",
    "  def process_requests(requests_raw: DataFrame) -> DataFrame:\n",
    "      \"\"\"\n",
    "      Processes a stream of raw requests and:\n",
    "          - Unpacks JSON payloads for requests\n",
    "          - Extracts relevant features as scalar values (first element of each array)\n",
    "          - Converts Unix epoch millisecond timestamps to Spark TimestampType\n",
    "      \"\"\"\n",
    "      # Calculate the current timestamp in seconds\n",
    "      current_ts = int(spark.sql(\"SELECT unix_timestamp(current_timestamp())\").collect()[0][0])\n",
    "\n",
    "      # Define the start timestamp for 30 days ago\n",
    "      start_ts = current_ts - 30 * 24 * 60 * 60  # 30 days in seconds\n",
    "\n",
    "      # Dynamically calculate the min and max values of timestamp_ms\n",
    "      min_max = requests_raw.agg(\n",
    "          F.min(\"timestamp_ms\").alias(\"min_ts\"),\n",
    "          F.max(\"timestamp_ms\").alias(\"max_ts\")\n",
    "      ).collect()[0]\n",
    "\n",
    "      min_ts = min_max[\"min_ts\"] / 1000  # Convert from milliseconds to seconds\n",
    "      max_ts = min_max[\"max_ts\"] / 1000  # Convert from milliseconds to seconds\n",
    "\n",
    "      # Transform timestamp_ms to span the last month\n",
    "      requests_timestamped = requests_raw.withColumn(\n",
    "          'timestamp', \n",
    "          (start_ts + ((F.col(\"timestamp_ms\") / 1000 - min_ts) / (max_ts - min_ts)) * (current_ts - start_ts)).cast(TimestampType())\n",
    "      ).drop(\"timestamp_ms\")\n",
    "\n",
    "      # Unpack JSON for the 'request' column only, since 'response' is already structured\n",
    "      requests_unpacked = requests_timestamped \\\n",
    "          .withColumn(\"request\", json_consolidation_udf(F.col(\"request\"))) \\\n",
    "          .withColumn('request', F.from_json(F.col(\"request\"), F.schema_of_json(\n",
    "              '[{\"HighBP\": 1.0, \"HighChol\": 0.0, \"CholCheck\": 1.0, \"BMI\": 26.0, \"Smoker\": 0.0, \"Stroke\": 0.0, \"HeartDiseaseorAttack\": 0.0, \"PhysActivity\": 1.0, \"Fruits\": 0.0, \"Veggies\": 1.0, \"HvyAlcoholConsump\": 0.0, \"AnyHealthcare\": 1.0, \"NoDocbcCost\": 0.0, \"GenHlth\": 3.0, \"MentHlth\": 5.0, \"PhysHlth\": 30.0, \"DiffWalk\": 0.0, \"Sex\": 1.0, \"Age\": 4.0, \"Education\": 6.0, \"Income\": 8.0,\"id\": 1}]'))) \n",
    "\n",
    "      # Extract feature columns as scalar values (first element of each array)\n",
    "      feature_columns = [\"HighBP\", \"HighChol\", \"CholCheck\", \"BMI\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\",\n",
    "                        \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\", \"NoDocbcCost\",\n",
    "                        \"GenHlth\", \"MentHlth\", \"PhysHlth\", \"DiffWalk\", \"Sex\", \"Age\", \"Education\", \"Income\", \"id\"]\n",
    "\n",
    "      for col_name in feature_columns:\n",
    "          # Extract the first element of each array for all feature columns\n",
    "          requests_unpacked = requests_unpacked.withColumn(col_name, F.col(f\"request.{col_name}\")[0])\n",
    "\n",
    "      # Extract predictions from the 'response' column without using from_json\n",
    "      requests_unpacked = requests_unpacked.withColumn(\"Diabetes_binary\", F.col(\"response.predictions\")[0])\n",
    "\n",
    "      # Drop unnecessary columns\n",
    "      requests_cleaned = requests_unpacked.drop(\"request\", \"response\")\n",
    "\n",
    "      # Add a placeholder 'model_id' column\n",
    "      final_df = requests_cleaned.withColumn(\"model_id\", F.lit(0).cast(T.IntegerType()))\n",
    "\n",
    "      return final_df\n",
    "  inference_df = spark.read.format('delta').table('model_inference_table')\n",
    "\n",
    "  # Apply the function to the inference DataFrame\n",
    "  model_logs_df = process_requests(inference_df.where(\"status_code = 200\"))\n",
    "\n",
    "  diabetes_df_pd = spark.read.format('delta').table('diabetes').toPandas()\n",
    "\n",
    "  # Rename the column in the pandas DataFrame and convert it to a Spark DataFrame\n",
    "  label_pd_df = diabetes_df_pd.rename(columns={'Diabetes_binary': 'labeled_data'})\n",
    "  label_pd_df = spark.createDataFrame(label_pd_df)\n",
    "\n",
    "  # Perform the join operation\n",
    "  model_logs_df_labeled = model_logs_df.join(\n",
    "      label_pd_df.select(\"id\", \"labeled_data\"), \n",
    "      on=[\"id\"], \n",
    "      how=\"left\"\n",
    "  ).drop('id')\n",
    "\n",
    "  spark.sql(\"DROP TABLE IF EXISTS model_logs\")\n",
    "\n",
    "  model_logs_df_labeled.write.mode(\"overwrite\").saveAsTable(\"model_logs\")\n",
    "\n",
    "  #enable change data feed for efficient execution and incremental updates to our model_logs table\n",
    "  spark.sql(f'ALTER TABLE model_logs SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\n",
    "\n",
    "\n",
    "  print(\"Model logs table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8c6c49-6067-46b4-bfd5-372970f2c1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize DBAcademyHelper\n",
    "DA = DBAcademyHelper() \n",
    "DA.init()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-3.Lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
