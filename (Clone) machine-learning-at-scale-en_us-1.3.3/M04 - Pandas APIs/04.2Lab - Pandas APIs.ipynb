{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "652cac0a-1bea-47c2-a6e3-c6c346f9253a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce4ee584-440c-45f0-b940-65d97c20ed59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Pandas APIs\n",
    "\n",
    "In this lab, you will explore how to integrate Pandas with Spark by comparing performance, converting data between different DataFrame types, applying Pandas UDFs, and training group-specific models with Pandas Function APIs. The dataset you'll be working with is the **Airbnb dataset**, which contains listings with features such as price, neighborhood, and property details.\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "_By the end of this lab, you will be able to:_\n",
    "1. **Task 1:** Comparing Performance of Pandas, Spark, and Pandas API DataFrames  \n",
    "2. **Task 2:** Converting Between DataFrames  \n",
    "3. **Task 3:** Applying Pandas UDF to Spark DataFrames\n",
    "4. **Task 4:** Training Group-Specific Models with Pandas Function API\n",
    "5. **Task 5:** Group-Specific Inference Using Pandas Function API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f06fa1-50c1-43da-b3f3-5e760fbebe7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec23457b-f076-4ba9-9f65-ef80352da8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80fd0560-318b-407f-bc78-12c1bb9ff760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a5a5df-a4b4-42ec-a508-5db27b569339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas pyspark koalas\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "695829e5-1dae-4b3f-ba8b-a4d73bee6b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the Lab, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1db3355-42f2-4f00-8bca-e82abcd98fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cf63dc-b46a-4320-b513-d0feaffa414e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this lab, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca560341-053d-4def-9b69-f7b74991310f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.covid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "410e30b5-6a49-423b-a782-e9b76ec24084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Loading the Dataset\n",
    "In this lab, you will load the **COVID-19 dataset**, which contains time-series data of daily confirmed cases, deaths, recoveries, and other metrics. You will first load the data into a Spark DataFrame and then convert it into Pandas and Pandas API DataFrames for further analysis.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Load the dataset into a Spark DataFrame**  \n",
    "   - Use the `spark.read.csv()` method to load the dataset from a CSV file. Ensure that the `header=True` and `inferSchema=True` options are set to correctly read the column names and data types.\n",
    "\n",
    "2. **Explore the dataset**  \n",
    "   - Use the `display()` function to view the first few rows of the loaded dataset and understand the structure of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b44a370c-a7f1-41e0-84ce-5d5f23eadf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "## Load the original COVID-19 dataset into a Spark DataFrame from the specified path\n",
    "data_path = f\"{DA.paths.datasets.covid}/coronavirusdataset/Time.csv\"\n",
    "covid_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "## Display the first few rows of the original dataset to inspect the structure and data\n",
    "display(covid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3abc0f6-8cb6-427d-8442-a8c6245004d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Additional Resources:**\n",
    "- [Databricks Documentation: Reading Data with Spark](https://docs.databricks.com/data/data-sources/read-csv.html) – Learn more about how to efficiently read data into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9154fee3-5d34-4705-b11f-aae1c6e61524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Comparing Performance of Pandas, Spark, and Pandas API DataFrames\n",
    "\n",
    "In this task, you will compare the performance of **Pandas**, **Spark**, and **Pandas API on Spark** DataFrames by calculating the mean of numeric columns. You will measure the time taken by each method to highlight the differences in performance between single-node and distributed environments.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Calculate the mean using a Pandas DataFrame**  \n",
    "   - Convert the Spark DataFrame to a Pandas DataFrame.\n",
    "   - Measure the time taken to calculate the mean for each numeric column.\n",
    "\n",
    "2. **Calculate the mean using a Spark DataFrame**  \n",
    "   - Use Spark’s distributed DataFrame to calculate the mean of numeric columns.\n",
    "   - Measure the time taken to calculate the mean for each numeric column.\n",
    "\n",
    "3. **Calculate the mean using Pandas API on Spark DataFrame (Koalas)**  \n",
    "   - Convert the Spark DataFrame to a Pandas API on Spark DataFrame.\n",
    "   - Measure the time taken to calculate the mean for each numeric column.\n",
    "\n",
    "By comparing the time taken, you'll see the performance differences between the three approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a905f82c-56b7-4679-82ee-07d10ee6bb0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.1 Data Preparation\n",
    "\n",
    "In this part of the task, you will prepare the data by selecting numeric columns and converting the Spark DataFrame into **Pandas** and **Pandas API on Spark** DataFrames.\n",
    "\n",
    "**Steps:**\n",
    "1. **Select the numeric columns** from the dataset.\n",
    "2. **Convert** the Spark DataFrame into a **Pandas DataFrame** and a **Pandas API on Spark DataFrame** for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec80021-217e-4603-b7f3-4b47ba2a0e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "## Select numeric columns for averaging\n",
    "numeric_columns = <FILL_IN>\n",
    "## Convert Spark DataFrame to Pandas DataFrame\n",
    "covid_pandas_df = <FILL_IN>\n",
    "\n",
    "## Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "covid_pandas_on_spark_df = <FILL_IN>\n",
    "\n",
    "## Print the number of rows in the test dataframe\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdd29b58-a686-4550-958c-020dfc3abaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import time\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "## Select numeric columns for averaging\n",
    "numeric_columns = [col for col in covid_df.columns if covid_df.schema[col].dataType in [DoubleType(), IntegerType()]]\n",
    "\n",
    "## Convert Spark DataFrame to Pandas DataFrame\n",
    "covid_pandas_df = covid_df.toPandas()\n",
    "\n",
    "## Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "covid_pandas_on_spark_df = ps.DataFrame(covid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a465112d-74cd-43e7-8483-2ee1d0dabddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Task 1.2 Comparing Performance Based on Time\n",
    "\n",
    "In this part, you will measure the time taken by **Pandas**, **Spark**, and **Pandas API on Spark** to calculate the mean of numeric columns and compare their performance. This comparison will help you understand the computational efficiency of single-node and distributed environments.\n",
    "\n",
    "**Steps:**\n",
    "1. **Measure the time** for calculating the mean of numeric columns using **Pandas** on a single node.\n",
    "2. **Measure the time** for calculating the mean of numeric columns using **Spark** on a distributed architecture.\n",
    "3. **Measure the time** for calculating the mean of numeric columns using **Pandas API on Spark**, which uses Pandas-like syntax but benefits from Spark’s distributed capabilities.\n",
    "\n",
    "  - **Expected Outcome:** \n",
    "    - The mean values of the numeric columns will be displayed along with the time taken for each approach.\n",
    "    - You will observe differences in computational time between single-node processing (Pandas) and distributed processing (Spark and Pandas API on Spark).\n",
    "\n",
    "  - **Key Insights:**\n",
    "    - **Pandas:** Suitable for smaller datasets but can become slow or memory-intensive with larger datasets.\n",
    "    - **Spark:** Handles large datasets efficiently using distributed processing across multiple nodes.\n",
    "    - **Pandas API on Spark:** Provides the familiar Pandas syntax while utilizing Spark's distributed processing power, making it a good balance between usability and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c73066-920a-4b46-acb3-9e343f7e553c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "start_time = time.time()\n",
    "## Measure time for Pandas DataFrame\n",
    "pandas_mean = <FILL_IN>\n",
    "pandas_time = <FILL_IN>\n",
    "\n",
    "## Measure time for Spark DataFrame\n",
    "spark_mean = <FILL_IN>\n",
    "spark_time = <FILL_IN>\n",
    "\n",
    "## Measure time for Pandas API on Spark DataFrame\n",
    "pandas_on_spark_mean = <FILL_IN>\n",
    "pandas_on_spark_time = <FILL_IN>\n",
    "\n",
    "## Display all the times together\n",
    "print(f\"Pandas DataFrame mean calculated in: {<FILL_IN>} seconds\")\n",
    "print(f\"Spark DataFrame mean calculated in: {<FILL_IN>} seconds\")\n",
    "print(f\"Pandas API on Spark DataFrame mean calculated in: {<FILL_IN>} seconds\")\n",
    "\n",
    "## Now, display all the means together\n",
    "print(\"\\nPandas DataFrame Mean:\")\n",
    "<FILL_IN>\n",
    "\n",
    "print(\"Spark DataFrame Mean:\")\n",
    "<FILL_IN>\n",
    "\n",
    "print(\"Pandas API on Spark DataFrame Mean:\")\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f4abe0-eb5f-45f8-92fc-ea0ea13066e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "import time\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "start_time = time.time()\n",
    "## Measure time for Pandas DataFrame\n",
    "pandas_mean = covid_pandas_df[numeric_columns].mean()\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "## Measure time for Spark DataFrame\n",
    "spark_mean = covid_df.select([avg(col).alias(f\"avg_{col}\") for col in numeric_columns])\n",
    "spark_time = time.time() - start_time\n",
    "\n",
    "## Measure time for Pandas API on Spark DataFrame\n",
    "pandas_on_spark_mean = covid_pandas_on_spark_df[numeric_columns].mean()\n",
    "pandas_on_spark_time = time.time() - start_time\n",
    "\n",
    "## Display all the times together\n",
    "print(f\"Pandas DataFrame mean calculated in: {pandas_time:.4f} seconds\")\n",
    "print(f\"Spark DataFrame mean calculated in: {spark_time:.4f} seconds\")\n",
    "print(f\"Pandas API on Spark DataFrame mean calculated in: {pandas_on_spark_time:.4f} seconds\")\n",
    "\n",
    "## Now, display all the means together\n",
    "print(\"\\nPandas DataFrame Mean:\")\n",
    "display(pandas_mean)\n",
    "\n",
    "print(\"Spark DataFrame Mean:\")\n",
    "display(spark_mean)\n",
    "\n",
    "print(\"Pandas API on Spark DataFrame Mean:\")\n",
    "display(pandas_on_spark_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663e7e2a-93c0-40ce-b04e-55ce9c260ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploration Task:\n",
    "\n",
    "Now that you’ve seen how to calculate the mean across different frameworks, try exploring other statistical operations such as `median()`, `variance()`, or `correlation()`. How do these calculations scale when using Pandas, Spark, and Pandas API on Spark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b17883ed-c9ce-4ec2-ab42-56ce71c34661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example: Calculate median using Pandas API on Spark\n",
    "pandas_on_spark_median = covid_pandas_on_spark_df.median()\n",
    "## Display the median values calculated using Pandas API on Spark\n",
    "display(<FILL_IN>)\n",
    "\n",
    "## Try calculating variance\n",
    "pandas_on_spark_var = <FILL_IN>.var()\n",
    "## Display the variance values calculated using Pandas API on Spark\n",
    "display(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed928a51-f336-4377-ba32-579532fbec13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Example: Calculate median using Pandas API on Spark\n",
    "pandas_on_spark_median = covid_pandas_on_spark_df.median()\n",
    "## Display the median values calculated using Pandas API on Spark\n",
    "display(pandas_on_spark_median)\n",
    "\n",
    "## Try calculating variance\n",
    "pandas_on_spark_var = covid_pandas_on_spark_df.var()\n",
    "## Display the variance values calculated using Pandas API on Spark\n",
    "display(pandas_on_spark_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e76c3eb-1aa7-49cc-8473-688528ebeede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key Learning Takeaways:**\n",
    "\n",
    "- **Pandas:** Great for small datasets, but struggles with larger ones due to single-node operation.\n",
    "- **Spark:** Handles large datasets efficiently through distributed processing across multiple nodes.\n",
    "- **Pandas API on Spark (Koalas):** Provides the ease of using Pandas-like syntax but operates on Spark’s distributed environment, making it both scalable and user-friendly.\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- [Pandas API on Spark (Koalas) Documentation](https://koalas.readthedocs.io/en/latest/)\n",
    "- [Optimizing Spark Performance](https://spark.apache.org/docs/latest/sql-performance-tuning.html) for large datasets in distributed environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d96d18-e74b-44a1-93d4-48a336c01572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Converting Between DataFrames\n",
    "\n",
    "In this task, you will learn how to convert between different types of DataFrames: **Spark DataFrame**, **Pandas API on Spark (Koalas) DataFrame**, and **Pandas DataFrame**. Being able to seamlessly switch between these types allows you to leverage the strengths of each framework for specific tasks.\n",
    "\n",
    "**Why is this important?**\n",
    "- **Spark DataFrame**: Ideal for large datasets and distributed computing, particularly when using Spark’s built-in functions and scalability.\n",
    "- **Pandas API on Spark (Koalas)**: Provides the simplicity and familiarity of Pandas but operates in Spark’s distributed environment, offering the best of both worlds.\n",
    "- **Pandas DataFrame**: Perfect for small datasets or when you need to use Pandas-specific functions that are not available in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a235f2f-bf18-4881-90af-6ea1062def44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.1 Convert Spark DataFrame to Pandas API on Spark DataFrame\n",
    "\n",
    "You will start by converting a **Spark DataFrame** into a **Pandas API on Spark DataFrame** (Koalas). This allows you to perform Pandas-like operations but with the scalability of Spark’s distributed environment.\n",
    "\n",
    "**Steps:**\n",
    "- **Step 1:** Use the `.to_pandas_on_spark()` method to convert the Spark DataFrame into a Pandas API on Spark DataFrame.\n",
    "- **Expected Outcome:** You can now apply Pandas-like functions on this DataFrame while benefiting from Spark's distributed processing.\n",
    "\n",
    "\n",
    "**Exploration Tip:** Try performing some common Pandas operations like `describe()`, `groupby()`, or `sum()` on this new Pandas API DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aece53a1-f204-4630-a70e-64476ecaa597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert Spark DataFrame to Pandas API on Spark DataFrame (Koalas)\n",
    "pandas_on_spark_df = covid_df.<FILL_IN>\n",
    "## Display the Pandas API on Spark DataFrame\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41a9b1b5-2deb-444e-a50c-3c0214db094b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Convert Spark DataFrame to Pandas API on Spark DataFrame (Koalas)\n",
    "pandas_on_spark_df = covid_df.to_pandas_on_spark()\n",
    "## Display the Pandas API on Spark DataFrame\n",
    "display(pandas_on_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9e8071-3174-429f-b328-041a8fa80eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.2 Convert Pandas API on Spark DataFrame Back to Spark DataFrame\n",
    "\n",
    "You may want to convert a **Pandas API on Spark DataFrame** back to a **Spark DataFrame** when you need to use Spark-specific functions that are unavailable in Pandas.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** Use the `.to_spark()` method to convert the Pandas API on Spark DataFrame back into a Spark DataFrame.\n",
    "\n",
    "**Expected Outcome:** The DataFrame is now a Spark DataFrame again, and you can use Spark’s distributed computing and functions like `select()`, `groupBy()`, or `filter()` to manipulate large datasets efficiently.\n",
    "\n",
    "**Exploration Tip:** Try using Spark’s built-in functions, such as `groupBy()` and `agg()`, after converting back to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfa9bfe2-937b-456c-938a-925fc148151e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert Pandas API on Spark DataFrame back to Spark DataFrame\n",
    "spark_df = pandas_on_spark_df.<FILL_IN>\n",
    "## Display the Spark DataFrame\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e0f17e-d063-415f-88be-c996dd213749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Convert Pandas API on Spark DataFrame back to Spark DataFrame\n",
    "spark_df = pandas_on_spark_df.to_spark()\n",
    "## Display the Spark DataFrame\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55dfdd7-85a3-4171-a9ea-235be3c74f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.3 Convert Pandas API on Spark DataFrame to Pandas DataFrame\n",
    "\n",
    "In some cases, you might want to work with a smaller dataset locally, or use Pandas-specific functions that aren’t available in Pandas API on Spark. In such cases, you can convert the **Pandas API on Spark DataFrame** to a **Pandas DataFrame**.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** Use the `.to_pandas()` method to convert the Pandas API on Spark DataFrame into a Pandas DataFrame.\n",
    "\n",
    "**Expected Outcome:** The DataFrame is now a regular Pandas DataFrame, allowing you to use all Pandas functionalities, but it will be constrained to single-node operation.\n",
    "\n",
    "**Exploration Tip:** After converting to a Pandas DataFrame, try performing some advanced Pandas functions such as `pivot_table()`, `corr()`, or `rolling()` to explore how Pandas can handle data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf35a72-ca67-4b2b-a7f2-182c59aaef6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert Pandas API on Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = pandas_on_spark_df.<FILL_IN>\n",
    "## Display the Pandas DataFrame\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c59752-71be-4c32-8e59-6b0a4e86e6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Convert Pandas API on Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = pandas_on_spark_df.to_pandas()\n",
    "## Display the Pandas DataFrame\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4c5814-f373-4370-bbbf-457d966422d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key Takeaways:**\n",
    "- **Conversion Flexibility:** Switching between DataFrame types (Spark, Pandas API on Spark, and Pandas) allows you to take advantage of each framework’s strengths. \n",
    "- **Scaling Operations:** Pandas API on Spark provides a familiar Pandas-like interface with the scalability of Spark, making it easier to switch between smaller and larger datasets seamlessly.\n",
    "- **Local vs Distributed:** Use Pandas DataFrame for smaller, local data processing, while Pandas API on Spark and Spark DataFrames are better suited for distributed, large-scale data processing.\n",
    "\n",
    "**Further Exploration:**\n",
    "- [Databricks Documentation on Pandas API on Spark](https://docs.databricks.com/aws/en/pandas/pandas-on-spark)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3073de-eb44-4f14-ac75-80e78a053112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Applying Pandas UDF to a Spark DataFrame  \n",
    "In this task, you will use a pre-trained **RandomForest** model from **Scikit-learn** and apply it to a Spark DataFrame using a **Pandas UDF**. Pandas UDFs allow you to apply custom Python functions to Spark DataFrames in a distributed manner, combining the flexibility of Python with the scalability of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ecd4f8-23ac-429d-8c9c-09f2890f1dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.1 Training a RandomForest Model  \n",
    "You’ll start by training a simple **RandomForest** model using **Scikit-learn** on a **local Pandas DataFrame**. The goal is to predict the number of confirmed COVID-19 cases based on other features in the dataset.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** Convert the Spark DataFrame into a Pandas DataFrame using `.toPandas()`.\n",
    "- **Step 2:** Train a **RandomForestRegressor** model using the **confirmed** cases as the target variable (`y`) and the other columns as features (`X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd023457-56da-4ead-8350-5fe436bb0061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Convert Spark DataFrame to Pandas DataFrame for local model training\n",
    "covid_pandas_df = <FILL_IN>\n",
    "\n",
    "## Define features (X) and target variable (y)\n",
    "X = <FILL_IN>  # Features\n",
    "y = <FILL_IN>  # Target variable: confirmed cases\n",
    "\n",
    "## Train the RandomForest model\n",
    "model = RandomForestRegressor()\n",
    "## Fit the model to the data\n",
    "model.<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a011ca98-8260-4af4-92de-9939ebf56313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Convert Spark DataFrame to Pandas DataFrame for local model training\n",
    "covid_pandas_df = covid_df.toPandas()\n",
    "\n",
    "## Define features (X) and target variable (y)\n",
    "X = covid_pandas_df.drop(columns=[\"confirmed\", \"deceased\", \"released\", \"date\"])  # Features\n",
    "y = covid_pandas_df[\"confirmed\"]  # Target variable: confirmed cases\n",
    "\n",
    "## Train the RandomForest model\n",
    "model = RandomForestRegressor()\n",
    "## Fit the model to the data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8517bda2-e146-4029-aebc-1433955cb790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.2 Define and Apply a Pandas UDF  \n",
    "Next, you define a **Pandas UDF** to apply the trained **RandomForest** model to our **Spark DataFrame**. The UDF will take multiple columns as input and output predictions for each row.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** Define a Pandas UDF using the `@pandas_udf` decorator, specifying that the return type is a **double** (which represents the predicted confirmed cases).\n",
    "- **Step 2:** Use **`pd.concat`** to combine all input columns into a single DataFrame that the model can process for prediction.\n",
    "\n",
    "**Exploration Tip:** Try adjusting the output type of the UDF if you want to predict a different type of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0103fca-1750-4cb9-80ec-c7ff403a7782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "## Define a Pandas UDF to apply the trained RandomForest model\n",
    "@pandas_udf(\"double\")  # Output type of the UDF is a double (for predicted confirmed cases)\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    ## Combine all input columns into a single DataFrame for model prediction\n",
    "    features = <FILL_IN>\n",
    "    return pd.Series(model.<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc76885-a7a8-4809-aec4-299a671a364d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "## Define a Pandas UDF to apply the trained RandomForest model\n",
    "@pandas_udf(\"double\")  # Output type of the UDF is a double (for predicted confirmed cases)\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    ## Combine all input columns into a single DataFrame for model prediction\n",
    "    features = pd.concat(cols, axis=1)\n",
    "    return pd.Series(model.predict(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c2b6b1-9c6c-428c-a371-efb92c2ec31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3 Applying the Pandas UDF to the Spark DataFrame  \n",
    "Now that you’ve defined the UDF, it’s time to apply it to the Spark DataFrame to generate predictions for each row.\n",
    "\n",
    "**Steps;**\n",
    "\n",
    "- **Step 1:** Use the `.withColumn()` method to apply the **Pandas UDF** to the DataFrame and create a new column, **`prediction`**, that contains the predicted confirmed cases.\n",
    "- **Step 2:** Exclude the target column (`confirmed`) and other irrelevant columns (`deceased`, `released`, `date`) from the input features.\n",
    "- **Exploration Tip:** Check how well the predictions align with the actual values by comparing the `prediction` column with the `confirmed` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38236252-23f5-479e-bb8f-1ec6b80f11d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "## Define the features used for prediction, excluding the target columns \"confirmed\", \"deceased\", \"released\", and \"date\"\n",
    "feature_names = [col for col in covid_df.columns if col not in [\"confirmed\", \"deceased\", \"released\", \"date\"]]\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    ## Combine input columns into a DataFrame for model prediction\n",
    "    features = <FILL_IN>\n",
    "    features.columns = feature_names  # Set the correct feature names\n",
    "    \n",
    "    ## Return predictions from the trained model\n",
    "    return pd.Series(model.predict(features))\n",
    "\n",
    "## Apply the Pandas UDF to the Spark DataFrame for predictions, excluding the target columns\n",
    "prediction_df = covid_df.select(<FILL_IN>).withColumn(\n",
    "    \"prediction\", \n",
    "    predict_udf(*[<FILL_IN>])\n",
    ")\n",
    "\n",
    "## Display the DataFrame with predictions\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e83d3c6-411d-4f2b-b2e5-006528dad96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "## Define the features used for prediction, excluding the target columns \"confirmed\", \"deceased\", \"released\", and \"date\"\n",
    "feature_names = [col for col in covid_df.columns if col not in [\"confirmed\", \"deceased\", \"released\", \"date\"]]\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    ## Combine input columns into a DataFrame for model prediction\n",
    "    features = pd.concat(cols, axis=1)\n",
    "    features.columns = feature_names  # Set the correct feature names\n",
    "    \n",
    "    ## Return predictions from the trained model\n",
    "    return pd.Series(model.predict(features))\n",
    "\n",
    "## Apply the Pandas UDF to the Spark DataFrame for predictions, excluding the target columns\n",
    "prediction_df = covid_df.select([col for col in feature_names]).withColumn(\n",
    "    \"prediction\", \n",
    "    predict_udf(*[covid_df[col] for col in feature_names])\n",
    ")\n",
    "\n",
    "## Display the DataFrame with predictions\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b23b9ea3-180f-464b-8275-c2b2809be01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploration Task:\n",
    "\n",
    "**Try Training Other Models:**\n",
    "\n",
    "- After training a **RandomForest** model, experiment with other models such as **LinearRegression** or **GradientBoostingRegressor** from Scikit-learn. \n",
    "- Train these models on the same dataset, then define and apply the same Pandas UDF pattern for predictions.\n",
    "\n",
    "**Evaluate Model Performance:**\n",
    "\n",
    "- Add additional code to calculate metrics such as **Mean Squared Error (MSE)** or **R² score** to evaluate the performance of your models.\n",
    "- Compare the performance of different models (e.g., RandomForest vs. LinearRegression vs. GradientBoosting) to determine which model performs best on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca287df6-8d3f-4b23-968b-1f19df061a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Train a LinearRegression model\n",
    "lr_model = LinearRegression()\n",
    "## Fit the model to the data\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "## Define a Pandas UDF for Linear Regression predictions\n",
    "@pandas_udf(\"double\")\n",
    "def lr_predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    features = <FILL_IN>\n",
    "    return pd.Series(lr_model.<FILL_IN>)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "## Calculate MSE and R² for the RandomForest model predictions\n",
    "rf_predictions = model.predict(X)\n",
    "mse_rf = mean_squared_error(y, rf_predictions)\n",
    "r2_rf = r2_score(y, rf_predictions)\n",
    "\n",
    "print(f\"RandomForest MSE: {<FILL_IN>}, R²: {<FILL_IN>}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860b3b25-d5d3-4c6c-80a0-56c168eb6f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Example:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Train a LinearRegression model\n",
    "lr_model = LinearRegression()\n",
    "# Fit the model to the data\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "## Define a Pandas UDF for Linear Regression predictions\n",
    "@pandas_udf(\"double\")\n",
    "def lr_predict_udf(*cols: pd.Series) -> pd.Series:\n",
    "    features = pd.concat(cols, axis=1)\n",
    "    return pd.Series(lr_model.predict(features))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "## Calculate MSE and R² for the RandomForest model predictions\n",
    "rf_predictions = model.predict(X)\n",
    "mse_rf = mean_squared_error(y, rf_predictions)\n",
    "r2_rf = r2_score(y, rf_predictions)\n",
    "\n",
    "print(f\"RandomForest MSE: {mse_rf}, R²: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de9f750-d244-40ff-a9a4-b75b22b8fa3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Training Group-Specific Models with Pandas Function API\n",
    "\n",
    "In this task, you will learn how to train group-specific models for different dates using **Pandas Function APIs**. We'll also log each trained model with **MLflow** for tracking and comparison. This technique allows for creating separate machine learning models for each group (in this case, dates) and analyzing performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d01254c8-23fc-4433-a517-79ce1cb22630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 4.1 Define the Group-Specific Model Training Function  \n",
    "Your will start by defining a function that:  \n",
    "\n",
    "- **Trains a model** for each group of data.\n",
    "- **Logs the model** with MLflow for tracking.\n",
    "- **Returns model metadata**, such as the path to the saved model, Mean Squared Error (MSE), and the number of records used for training.\n",
    "\n",
    "This function will be applied to each group defined by the `date` column.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** The `train_group_model` function extracts the group label (in this case, the **date**) and defines the features (`X_group`) and target variable (`y_group`).\n",
    "- **Step 2:** A **RandomForestRegressor** model is trained on the subset of data for the specific date.\n",
    "- **Step 3:** The model is logged using **MLflow** to save it for future reference and evaluation.\n",
    "- **Step 4:** The function calculates the **Mean Squared Error (MSE)** for the group and returns relevant details, including the model's path in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "169cfab4-6b28-49d7-b8d9-050c6e1a6897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "## Define a pandas function to train a group-specific model and log each model with MLflow\n",
    "def train_group_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Extract the group label (in this case, the date)\n",
    "    date = df_pandas['date'].iloc[0]\n",
    "    \n",
    "    ## Define features (X) and target variable (y)\n",
    "    X_group = df_pandas.drop(columns=[\"confirmed\", \"deceased\", \"released\", \"date\"])\n",
    "    y_group = df_pandas[\"confirmed\"]\n",
    "    \n",
    "    ## Train a RandomForest model for the group\n",
    "    model = RandomForestRegressor(<FILL_IN>)\n",
    "    # Fit the model to the data\n",
    "    model.<FILL_IN>\n",
    "    \n",
    "    ## Log the trained model using MLflow\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.sklearn.log_model(<FILL_IN>)\n",
    "        model_uri = mlflow.get_artifact_uri(\"random_forest_model\")\n",
    "    \n",
    "    ## Calculate Mean Squared Error (MSE) for the group\n",
    "    predictions = model.predict(X_group)\n",
    "    mse = <FILL_IN>\n",
    "\n",
    "    ## Return a DataFrame containing group information and model performance\n",
    "    return pd.DataFrame({\n",
    "        <FILL_IN>\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e42654-aa4c-4807-9c8b-c0351e6576a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "## Define a pandas function to train a group-specific model and log each model with MLflow\n",
    "def train_group_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Extract the group label (in this case, the date)\n",
    "    date = df_pandas['date'].iloc[0]\n",
    "    \n",
    "    ## Define features (X) and target variable (y)\n",
    "    X_group = df_pandas.drop(columns=[\"confirmed\", \"deceased\", \"released\", \"date\"])\n",
    "    y_group = df_pandas[\"confirmed\"]\n",
    "    \n",
    "    ## Train a RandomForest model for the group\n",
    "    model = RandomForestRegressor(n_estimators=1, random_state=42)\n",
    "    model.fit(X_group, y_group)\n",
    "    \n",
    "    ## Log the trained model using MLflow\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "        model_uri = mlflow.get_artifact_uri(\"random_forest_model\")\n",
    "    \n",
    "    ## Calculate Mean Squared Error (MSE) for the group\n",
    "    predictions = model.predict(X_group)\n",
    "    mse = mean_squared_error(y_group, predictions)\n",
    "\n",
    "    ## Return a DataFrame containing group information and model performance\n",
    "    return pd.DataFrame({\n",
    "        \"date\": [str(date)],   # Group label (date)\n",
    "        \"model_path\": [str(model_uri)],  # Path where the model is stored\n",
    "        \"mse\": [float(mse)],  # Model performance metric\n",
    "        \"n_used\": [int(len(df_pandas))]  # Number of records used in training\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a92e5ab6-fe64-4a7c-80ca-e97d3b619709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 4.2 Apply the Group-Specific Function Using Pandas API  \n",
    "You will now apply the `train_group_model` function to each group using the **Pandas Function API**. This will allow us to train a separate model for each date in the dataset.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** You define the schema of the output DataFrame, which includes the group label (`date`), the **MLflow model path**, the **MSE**, and the number of records used in training.\n",
    "- **Step 2:** You apply the `train_group_model` function using **`applyInPandas`** to each group defined by the `date` column. \n",
    "- **Step 3:** The resulting DataFrame, `result_df`, will display the **model path**, the calculated **MSE**, and how many records were used in training each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b21769-f012-4bd9-b205-1bc26d951502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert the date column to a numerical value using only the month starting with January\n",
    "from pyspark.sql.functions import month\n",
    "covid_df = covid_df.withColumn('date', <FILL_IN>)\n",
    "\n",
    "## Define the schema for the output DataFrame\n",
    "schema = StructType([\n",
    "    <FILL_IN>\n",
    "])\n",
    "\n",
    "## Apply the group-specific model training function using 'applyInPandas'\n",
    "result_df = covid_df.groupby(\"date\").applyInPandas(train_group_model, schema=schema)\n",
    "\n",
    "## Display the result DataFrame showing the model path, MSE, and number of records for each group\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51d5178-57d1-44a9-83f4-aa2285728fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "\n",
    "## Convert the date column to a numerical value using only the month starting with January\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "covid_df = covid_df.withColumn('date', month('date'))\n",
    "\n",
    "## Define the schema for the output DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"model_path\", StringType(), True),\n",
    "    StructField(\"mse\", DoubleType(), True),\n",
    "    StructField(\"n_used\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "## Apply the group-specific model training function using 'applyInPandas'\n",
    "result_df = covid_df.groupby(\"date\").applyInPandas(train_group_model, schema=schema)\n",
    "\n",
    "## Display the result DataFrame showing the model path, MSE, and number of records for each group\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16d79f02-b376-4bd7-ae27-de01165edf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploration Task:\n",
    "\n",
    "**1. Experiment with Other Models:**\n",
    "   - Try replacing the **RandomForest** model with other **Scikit-learn** models such as **LinearRegression** or **GradientBoostingRegressor**.\n",
    "   - Observe how different models perform when applied to each group (date). Compare their performance metrics (such as **MSE**) and check whether one model performs better for certain groups.\n",
    "\n",
    "**2. Evaluate Model Performance:**\n",
    "   - Use additional evaluation metrics, such as the **R² score** or **Mean Absolute Error (MAE)**. Log these metrics in MLflow along with the models.\n",
    "   - Compare performance metrics across different dates or groups to track which model performs best for each group.\n",
    "\n",
    "**3. Test with Different Groupings:**\n",
    "   - Instead of grouping by `date`, try changing the grouping criterion. For example, group by features such as **zipcode** or **property_type** (if available in your dataset).\n",
    "   - This will help you understand how different group-specific models perform based on different segmentation criteria.\n",
    "\n",
    "**Further Exploration:**\n",
    "\n",
    "- **MLflow Documentation:** Learn more about tracking models and metrics using MLflow [MLflow Tracking Documentation](https://mlflow.org/docs/latest/tracking.html).\n",
    "- **Pandas Function API Documentation:** Explore how to use Pandas Function API in PySpark for scalable group-specific operations [Pandas Function API in PySpark](https://docs.databricks.com/aws/en/pandas/pandas-function-apis).\n",
    "\n",
    "This exploration step will help users understand the benefits of group-specific modeling and how different models and evaluation metrics behave across distinct segments of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9f3c13-6853-407f-a278-14e7fd03a6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Group-Specific Inference Using Pandas Function API  \n",
    "In this task, you will use the models trained in the previous task to perform group-specific predictions (inference) for each date. We’ll load the models from **MLflow**, apply them to the test data, and calculate the overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e244455-f50a-4155-b7d0-a685f5bd0967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 5.1 Define the Inference Function  \n",
    "You will create a function to:\n",
    "\n",
    "- **Load the saved model** for each date from **MLflow**.\n",
    "- Use the corresponding **feature columns** to make predictions for each group.\n",
    "- Return both the **predicted confirmed cases** and the **actual confirmed cases** for comparison.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** The function `apply_model` retrieves the **model path** for each date from the input DataFrame (`df_pandas`).\n",
    "- **Step 2:** The model is loaded from **MLflow** using the `mlflow.sklearn.load_model` method.\n",
    "- **Step 3:** The function selects the relevant feature columns (`test`, `negative`, and `released`) for inference.\n",
    "- **Step 4:** The model makes predictions based on the features, and the results (both predicted and actual confirmed cases) are returned in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff2558a-7773-478a-8c0d-2c89ad7d64f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Load the model path for this group (date) from the DataFrame\n",
    "    model_path = df_pandas[\"model_path\"].iloc[0]\n",
    "    \n",
    "    ## Load the model from MLflow\n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "    \n",
    "    ## Define the feature columns that were used during training\n",
    "    feature_columns = [\"test\", \"negative\", \"released\"]\n",
    "    X = df_pandas[feature_columns]\n",
    "    \n",
    "    ## Make predictions using the loaded model\n",
    "    predictions = <FILL_IN>\n",
    "    \n",
    "    ## Return a DataFrame containing both the predicted and actual confirmed cases\n",
    "    return pd.DataFrame({\n",
    "        <FILL_IN>\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35d5741-3d56-4ca0-a494-98745230ccdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "def apply_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Load the model path for this group (date) from the DataFrame\n",
    "    model_path = df_pandas[\"model_path\"].iloc[0]\n",
    "    \n",
    "    ## Load the model from MLflow\n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "    \n",
    "    ## Define the feature columns that were used during training\n",
    "    feature_columns = [\"time\", \"test\", \"negative\"]\n",
    "    X = df_pandas[feature_columns]\n",
    "    \n",
    "    ## Make predictions using the loaded model\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    ## Return a DataFrame containing both the predicted and actual confirmed cases\n",
    "    return pd.DataFrame({\n",
    "        \"prediction\": predictions,\n",
    "        \"actual_confirmed\": df_pandas[\"confirmed\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109df6bf-8b01-4495-bc5f-ec54e817f904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 5.2 Apply the Inference Function  \n",
    "You will apply the `apply_model` function using the **Pandas Function API** to perform inference for each group (date). This step will calculate predictions for the test data.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** You define the schema for the output of the `apply_model` function, specifying that it will contain **predicted** and **actual confirmed cases**.\n",
    "- **Step 2:** You join the result DataFrame (`result_df`, which contains the model paths) with the original **COVID-19** dataset to include the necessary features for inference.\n",
    "- **Step 3:** The `applyInPandas` function is used to apply the inference function across each group (`date`).\n",
    "- **Step 4:** The final DataFrame (`inference_df`) will display the **predicted values** alongside the **actual confirmed cases** for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40415824-3eb5-43ba-a5b9-5d9664c7a7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "\n",
    "## Define the schema to include the predicted and actual confirmed cases\n",
    "inference_schema = <FILL_IN>\n",
    "\n",
    "## Join the result DataFrame with the original dataset to ensure model_path and feature columns are included\n",
    "inference_df = result_df.join(covid_df, \"date\")\n",
    "\n",
    "## Apply the model using Pandas Function API, grouped by 'date'\n",
    "inference_df = inference_df.groupby(\"date\").applyInPandas(apply_model, schema=inference_schema)\n",
    "\n",
    "## Display the result DataFrame with predictions and actual confirmed cases\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf4d25b-5534-4070-82f8-98b1de4a7b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "## Define the schema to include the predicted and actual confirmed cases\n",
    "inference_schema = \"prediction double, actual_confirmed double\"\n",
    "\n",
    "## Join the result DataFrame with the original dataset to ensure model_path and feature columns are included\n",
    "inference_df = result_df.join(covid_df, \"date\")\n",
    "\n",
    "## Apply the model using Pandas Function API, grouped by 'date'\n",
    "inference_df = inference_df.groupby(\"date\").applyInPandas(apply_model, schema=inference_schema)\n",
    "\n",
    "## Display the result DataFrame with predictions and actual confirmed cases\n",
    "display(inference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ab78f0-4b8e-4b32-bf4b-cdd488562938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 5.3 Calculating Overall Accuracy  (Optional)\n",
    "In this step, you will calculate the overall accuracy of the predictions. We'll define accuracy as the percentage of predictions within **10%** of the actual confirmed cases.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Step 1:** You calculate the accuracy by checking if the **absolute difference** between the predicted and actual values is less than **10%** of the actual value.\n",
    "- **Step 2:** You compute the overall accuracy as the percentage of rows where the prediction falls within this range.\n",
    "- **Step 3:** The result is printed, showing the overall prediction accuracy as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b749a80-c389-4324-85a0-2e4b56ad210c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Calculate accuracy: percentage of predictions within 10% of the actual confirmed cases\n",
    "inference_df = inference_df.withColumn(\n",
    "    <FILL_IN>\n",
    ")\n",
    "\n",
    "## Calculate the overall accuracy\n",
    "overall_accuracy = inference_df.filter(\"accuracy = true\").count() / inference_df.count() * 100\n",
    "\n",
    "## Display the overall prediction accuracy\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767efea4-3df5-427d-9e68-d2837f3c3646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Calculate accuracy: percentage of predictions within 10% of the actual confirmed cases\n",
    "inference_df = inference_df.withColumn(\n",
    "    \"accuracy\", \n",
    "    (abs(inference_df[\"prediction\"] - inference_df[\"actual_confirmed\"]) / inference_df[\"actual_confirmed\"]) < 0.1\n",
    ")\n",
    "\n",
    "## Calculate the overall accuracy\n",
    "overall_accuracy = inference_df.filter(\"accuracy = true\").count() / inference_df.count() * 100\n",
    "\n",
    "## Display the overall prediction accuracy\n",
    "print(f\"Overall prediction accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7569ba09-b68e-4ece-9c7b-412e866c8a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploration Task:\n",
    "\n",
    "**1. Experiment with Model Inference:**\n",
    "   - Try using the models trained with different Scikit-learn algorithms (such as **LinearRegression** or **GradientBoostingRegressor**) and apply the inference function again. Compare the accuracy of these models with the original **RandomForest** model.\n",
    "\n",
    "**2. Modify Accuracy Threshold:**\n",
    "   - Adjust the accuracy threshold from **10%** to another value (e.g., **5%** or **20%**) and observe how the overall prediction accuracy changes. This will give you a sense of how sensitive the models are to accuracy thresholds.\n",
    "\n",
    "**3. Compare Predictions Across Dates:**\n",
    "   - Group the results by other columns (such as **region** or **age group**, if available) and observe how the models perform in predicting confirmed cases across different regions or demographic groups.\n",
    "\n",
    "**Further Exploration:**\n",
    "\n",
    "- **MLflow Model Inference:** Learn more about using **MLflow** to track and load models for inference in a production environment [MLflow Model Inference Documentation](https://mlflow.org/docs/latest/deployment/deploy-model-locally/).\n",
    "- **Pandas Function API in PySpark:** Explore how to use **Pandas Function APIs** in PySpark for distributed and scalable operations [Pandas Function API Documentation](https://docs.databricks.com/aws/en/pandas/pandas-function-apis).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9591a414-2dd5-4bac-934b-153cdef0b6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Conclusion\n",
    "In this lab, you explored the performance and versatility of Pandas, Spark, and Pandas API DataFrames for data processing. You learned how to convert between DataFrame types, apply Pandas UDFs for distributed inference, and train group-specific models using Pandas Function APIs. Finally, you performed group-specific inference with models logged in MLflow, demonstrating how to scale machine learning workflows efficiently using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ead397-1631-437b-9719-58447ddd809a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04.2Lab - Pandas APIs",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
