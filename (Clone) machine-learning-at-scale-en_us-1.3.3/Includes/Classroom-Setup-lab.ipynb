{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba23250-3a6e-49f5-b3c5-e6aa1234b3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24aa452b-690d-4c7c-846a-1ca64b820e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1fcbd8-bf14-4cdd-91e6-85e59e9afd12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a large California Housing dataset for demonstration purposes\n",
    "@DBAcademyHelper.add_init\n",
    "def create_large_california_housing_table(self):\n",
    "    spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "    spark.sql(f\"USE SCHEMA {DA.schema_name}\")\n",
    "    \n",
    "    # Load the original California Housing dataset\n",
    "    data_path = f\"{DA.paths.datasets.california_housing}/data\"\n",
    "    df = spark.read.format(\"delta\").load(data_path)\n",
    "\n",
    "    # Function to create a larger dataset for demonstration purposes\n",
    "    def generate_large_housing_dataset(df, num_copies=100):\n",
    "        pandas_df = df.toPandas()  # Convert to Pandas for duplication\n",
    "        large_df = pd.concat([pandas_df.sample(frac=1).reset_index(drop=True)] * num_copies, ignore_index=True)  # Duplicate and shuffle\n",
    "        return large_df\n",
    "\n",
    "    # Convert back to Spark DataFrame and save as Delta table\n",
    "    large_housing_df = spark.createDataFrame(generate_large_housing_dataset(df, num_copies=100))\n",
    "    output_delta_table = f\"{DA.paths.working_dir}/v01/large_california_housing_delta\"\n",
    "    large_housing_df.write.format(\"delta\").mode(\"overwrite\").save(output_delta_table)\n",
    "\n",
    "    print(f\"Created large Delta table at {output_delta_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0f45e6-7793-4f1f-b3c1-06be2bcb9624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize DBAcademyHelper\n",
    "DA = DBAcademyHelper() \n",
    "DA.init()                                           # Performs basic initialization including creating schemas and catalogs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
