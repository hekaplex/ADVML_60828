{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35cd167-4755-4190-aa24-8ac9031f0c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd84be0-1ac9-4710-80c3-9c57a0b3443f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo: HPO with Ray Tune\n",
    "\n",
    "In this demo, you will learn how to use **Ray Tune** — a powerful hyperparameter optimization framework — to tune machine learning models in **Databricks**. \n",
    "\n",
    "We will demonstrate how to implement the Ray Tune framework using a **Random Forest Regressor** from Scikit-Learn, covering:\n",
    "- **Defining search spaces**\n",
    "- **Creating objective functions**\n",
    "- **Optimizing hyperparameters** \n",
    "\n",
    "\n",
    "Additionally, we will track and log the results using **MLflow**, enabling efficient management and monitoring of the tuning process.\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this demo, you will be able to:\n",
    "- Define an **objective function** specific to Ray Tune.\n",
    "- Set up **Optuna-style search spaces** within Ray Tune.\n",
    "- Configure **Ray Tune's Tuner and compute resources**.\n",
    "- Optimize hyperparameters using **parallel execution**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2d388f-14ad-44a7-b8cb-9d54476cadc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Tuning with Ray Tune and a Single-Machine Model\n",
    "In this part, we will use **Ray for distributed hyperparameter optimization** while training a **Scikit-Learn** model.\n",
    "\n",
    "### How This Works:\n",
    "- **Data is converted from a Spark DataFrame to Pandas** to enable single-machine model training.\n",
    "- **Ray Tune runs on a single machine** but distributes trial execution across *multiple* CPU threads to speed up hyperparameter tuning.\n",
    "- **MLflow tracks the experiment**, logging the best hyperparameters and model performance.\n",
    "\n",
    "This approach allows us to use **Ray for distributed hyperparameter search**, while **training the model on a single node** to take advantage of Scikit-Learn’s efficient implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5352c8bc-f458-4fde-9015-2e07b4dbeb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6610aff8-0064-4603-ae6d-1cd8129482a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need a classic cluster running one of the following Databricks runtime(s): **16.4.x-cpu-ml-scala2.12**. **Do NOT use serverless compute to run this notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8b1184-eda2-43e6-b791-f8f54b215fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U optuna optuna-integration mlflow\n",
    "%pip install --upgrade ray[tune]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4bc9ccf-bc21-41ca-9abb-a36495ae5453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a178a-1fe7-4018-8489-3d78af0a94c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02.1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d711e888-1a1a-46ec-ac17-aa89a4607d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5abc6fb-f10a-402b-a646-2cfc096354bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets.wine_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e91432-d0ad-4cdf-9866-a61e75114860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure a Ray Cluster\n",
    "Let's begin by setting up a single-machine (driver-only) Ray cluster by defining the following:\n",
    "- 3 CPU cores allocated for the head node, leaving 1 core for Spark\n",
    "> The Vocareum environment allocates 4 CPUs per user for this demonstration. \n",
    "- 1 worker node\n",
    "- 4 CPUs per worker\n",
    "\n",
    "Additionally, we will initialize the Ray cluster on spark with the above configurations with `ray.init()` and defining the environment variable `RAY_ADDRESS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e321ebff-5853-4e82-9c2c-6064acc42379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b872ecd-f20a-4650-9fe4-e7cefbb0dddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# Attempt to shut down any existing Ray cluster\n",
    "try:\n",
    "    shutdown_ray_cluster()\n",
    "    print(\"Existing Ray cluster shutdown successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: No active Ray cluster to shut down. Details: {e}\")\n",
    "\n",
    "# Set up configurations for a single-machine (driver-only) Ray cluster\n",
    "num_cpus_head_node = 3  # Use 3 CPU cores, leaving 1 for Spark\n",
    "num_worker_nodes = 1  # Single-node setup (driver only)\n",
    "num_cpu_cores_per_worker = 4  # Unused since there's only one worker\n",
    "\n",
    "# Initialize the Ray cluster on Spark\n",
    "ray_conf = setup_ray_cluster(\n",
    "    min_worker_nodes=num_worker_nodes,  \n",
    "    max_worker_nodes=num_worker_nodes,\n",
    "    num_cpus_head_node=num_cpus_head_node,  \n",
    "    num_gpus_head_node=0  # No GPU usage\n",
    ")\n",
    "\n",
    "# Initialize Ray with the configured settings\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(f\"Ray initialized with address: {ray_conf[0]}\")\n",
    "\n",
    "# Set Ray address for Spark integration\n",
    "os.environ['RAY_ADDRESS'] = ray_conf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227e7ec3-c19c-4a38-979d-54442cfbb40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Preparation for Distributed Optuna with Single-Machine Training\n",
    "\n",
    "In this task, before we define the objective function, we need to convert our Spark DataFrame to a Pandas DataFrame. This will allow us to split the data into training and test sets and standardize the features. These steps are essential for conducting single-node model training using **Scikit-learn**.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. **Convert the Spark DataFrame** into a Pandas DataFrame for single-node processing.\n",
    "2. **Split the dataset** into training and test sets using **Scikit-learn**'s `train_test_split` method.\n",
    "3. **Standardize the features** to ensure the model performs optimally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797ad435-e8f9-4583-8370-1dcbbd7d197d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(train_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37344921-b8b8-411d-ba88-c4dc7ade718a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for single-machine training\n",
    "train_pandas = train_df.toPandas()\n",
    "\n",
    "# Separate features and labels\n",
    "X = train_pandas[feature_columns]\n",
    "y = train_pandas[label_column]\n",
    "\n",
    "# Split the data into training and test sets using Scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Display the shapes of the training and test sets\n",
    "print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315c4d1b-6cd6-47ec-81c8-eb1346879a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the Ray Tune Objective Function and Search Space for Distributed Hyperparameter Search (Single-Machine Training)\n",
    "\n",
    "In this step, we will define the **objective function** for hyperparameter tuning using **Ray Tune**. \n",
    "\n",
    "This function will:\n",
    "- Suggest hyperparameters dynamically.\n",
    "- Train and evaluate a `RandomForest` model using **Scikit-learn**.\n",
    "- Utilize **cross-validation** for performance evaluation.\n",
    "\n",
    "#### **Key Differences from the Previous Demonstration**\n",
    "- Unlike the previous approach, we now integrate **cross-validation** for model evaluation.\n",
    "- Cross-validation is feasible in **single-node** training but can be **resource-intensive** in a distributed setup.\n",
    "\n",
    "---\n",
    "\n",
    "### **Instructions**\n",
    "1. **Define the Hyperparameter Search Space**  \n",
    "   - Use **Ray Tune's API** for defining search spaces.  \n",
    "   - [Tune Search Space API](https://docs.ray.io/en/latest/tune/api/search_space.html)\n",
    "\n",
    "2. **Configure the Search Algorithm**  \n",
    "   - Use **Ray Tune’s Search Algorithms** for efficient exploration.  \n",
    "   - [Tune Search Algorithms](https://docs.ray.io/en/latest/tune/api/suggestion.html)\n",
    "\n",
    "3. **Implement the Objective Function**  \n",
    "   - Train a **RandomForest model** using **Scikit-learn**.  \n",
    "   - Optimize hyperparameters within the function.\n",
    "\n",
    "4. **Set Up & Execute the Tuning Process**  \n",
    "   - Use **Ray Tune Execution (`tune.Tuner`)** to configure and run the tuning process.  \n",
    "   - Apply key configurations like:\n",
    "     - `TuneConfig`\n",
    "     - `RunConfig`\n",
    "     - `CheckpointConfig`\n",
    "     - `FailureConfig`  \n",
    "   - [Tune Execution (`tune.Tuner`)](https://docs.ray.io/en/latest/tune/api/execution.html)\n",
    "   - [`ray.tune.with_parameters`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.with_parameters.html)\n",
    "\n",
    "5. **Evaluate Model Performance**  \n",
    "   - Use **cross-validation** and return **negative RMSE** (to be minimized).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8c12a7-952a-4f00-bbe6-02c979189536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Define the hyperparameter search space for RandomForest tuning\n",
    "search_space = {\n",
    "    \"n_estimators\": tune.randint(50, 300),  # Number of trees in the forest (wider range)\n",
    "    \"max_depth\": tune.randint(3, 30)  # Depth of the trees (realistic range)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d80385-5919-45e4-b11e-c3203e95c731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.types.utils import _infer_schema\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from ray import tune\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback, setup_mlflow\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Retrieve Databricks MLflow credentials\n",
    "mlflow_db_creds = get_databricks_env_vars(\"databricks\")\n",
    "\n",
    "if not mlflow_db_creds:\n",
    "    raise ValueError(\"Databricks MLflow credentials could not be retrieved.\")\n",
    "\n",
    "# Set up MLflow experiment\n",
    "# MLflow Experiment Setup\n",
    "experiment_name_ray = os.path.join(\n",
    "    os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()),\n",
    "    \"02b - Model Tuning with Ray\"\n",
    ")\n",
    "mlflow.set_experiment(experiment_name_ray)\n",
    "experiment_id_ray = mlflow.get_experiment_by_name(experiment_name_ray).experiment_id\n",
    "\n",
    "# Define Optuna search algorithm\n",
    "searcher = OptunaSearch(metric=\"rmse\", mode=\"min\")  # Minimize RMSE\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=3)  # Limit concurrent trials to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01a5e9c-f58d-4316-865b-de6d04d8d94b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "import ray\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective_ray_scikit(\n",
    "    config: Dict[str, Any],\n",
    "    parent_run_id: str,\n",
    "    X_train_in: pd.DataFrame,\n",
    "    y_train_in: pd.Series,\n",
    "    experiment_name_in: str,\n",
    "    mlflow_db_creds_in: Dict[str, str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Objective function for Ray Tune hyperparameter optimization using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config (Dict[str, Any]): Hyperparameter configuration from Ray Tune.\n",
    "        parent_run_id (str): MLflow parent run ID for nested tracking.\n",
    "        X_train_in (pd.DataFrame): Training features.\n",
    "        y_train_in (pd.Series): Training labels.\n",
    "        experiment_name_in (str): MLflow experiment name.\n",
    "        mlflow_db_creds_in (Dict[str, str]): Databricks MLflow credentials.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing RMSE (lower is better).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Update Databricks credentials for Ray (executors restart each run)\n",
    "        if mlflow_db_creds_in:\n",
    "            os.environ.update(mlflow_db_creds_in)\n",
    "\n",
    "        # Start nested MLflow run under the parent run\n",
    "        with mlflow.start_run(nested=True, experiment_id=experiment_id_ray, tags={\"mlflow.parentRunId\": parent_run_id}):\n",
    "            # Extract hyperparameters from Ray Tune config\n",
    "            n_estimators = config[\"n_estimators\"]\n",
    "            max_depth = config[\"max_depth\"]\n",
    "\n",
    "            # Initialize RandomForest Regressor\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators, \n",
    "                max_depth=max_depth, \n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Perform 3-fold cross-validation and compute RMSE\n",
    "            scores = cross_val_score(\n",
    "                model, X_train_in, y_train_in, \n",
    "                scoring='neg_root_mean_squared_error', \n",
    "                cv=3\n",
    "            )\n",
    "\n",
    "            mean_rmse = -scores.mean()  # Convert negative RMSE to positive\n",
    "\n",
    "            # Log hyperparameters and metrics in MLflow\n",
    "            mlflow.log_params(config)\n",
    "            mlflow.log_metric(\"RMSE\", mean_rmse)\n",
    "\n",
    "            return {\"rmse\": mean_rmse}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in objective function: {e}\")\n",
    "        return {\"rmse\": float(\"inf\")}  # Return a large RMSE in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6f5340-780d-4ee7-8be6-f327fdc80ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start the parent MLflow run\n",
    "with mlflow.start_run(run_name=\"ray_tune\", experiment_id=experiment_id_ray) as parent_run:\n",
    "    os.environ.update(mlflow_db_creds)  # Ensure Ray executors have credentials\n",
    "\n",
    "    # Set up and execute Ray Tune with the objective function and search space\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(\n",
    "            objective_ray_scikit,\n",
    "            parent_run_id=parent_run.info.run_id,\n",
    "            X_train_in=X_train,\n",
    "            y_train_in=y_train,\n",
    "            experiment_name_in=experiment_name_ray,\n",
    "            mlflow_db_creds_in=mlflow_db_creds\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            num_samples=10,\n",
    "            reuse_actors=True  # Keeps actors alive for efficiency\n",
    "        ),\n",
    "        param_space=search_space\n",
    "    )\n",
    "\n",
    "    # Run tuning and retrieve the best result\n",
    "    multinode_results = tuner.fit()\n",
    "    best_result = multinode_results.get_best_result(metric=\"rmse\", mode=\"min\", scope=\"last\")\n",
    "\n",
    "    if best_result is None:\n",
    "        raise ValueError(\"No best trial found. Ensure the tuning job ran successfully.\")\n",
    "\n",
    "    # Extract best trial details\n",
    "    best_trial_number = best_result.metrics.get(\"trial_id\", \"N/A\")  # Default if missing\n",
    "    best_model_params = best_result.config\n",
    "    best_model_params[\"random_state\"] = 42  # Ensures reproducibility\n",
    "    best_rmse = best_result.metrics[\"rmse\"]\n",
    "\n",
    "    # Train the best model using the best hyperparameters\n",
    "    best_model = RandomForestRegressor(**best_model_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Enable MLflow autologging (disable model logging to avoid conflicts)\n",
    "    mlflow.sklearn.autolog(log_input_examples=True, log_models=False, silent=True)\n",
    "\n",
    "    # Infer model output schema\n",
    "    try:\n",
    "        output_schema = _infer_schema(y_train)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "        output_schema = None\n",
    "\n",
    "    # Infer model signature\n",
    "    input_example = X_train[:3]  # Use a small subset as an example\n",
    "    signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "\n",
    "    # Set model name for MLflow registration\n",
    "    model_name = f\"{DA.catalog_name}.{DA.schema_name}.hpo_model_ray_tune_optuna\"\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Best Trial Number: {best_trial_number}\")\n",
    "    print(f\"Best Hyperparameters: {best_model_params}\")\n",
    "    print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "    # Log the best model to MLflow\n",
    "    with mlflow.start_run(run_name=\"best_trial_ray_scikit_results\", nested=True):\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=model_name\n",
    "        )\n",
    "        mlflow.log_params(best_model_params)\n",
    "        mlflow.log_metric(\"Best RMSE\", best_rmse)\n",
    "\n",
    "# Ensure MLflow run is properly closed\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ff043e-48a3-4fe0-87c4-a1926cfd3be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Shut down the ray cluster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d3b488c-7f5d-441a-a4f5-0d3c9ce818af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4bec4a1-23c0-4713-8fbb-a54b5337dbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this demo, we explored how to setup and execute model training with Ray Tune on a single-machine. Additionally, we walked through the core components needed to perform model training with the Ray framework such as defining a search space, building objective functions, and optimization of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182dede5-b2aa-4be4-b7d6-8896d9517178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02.2 - HPO with Ray Tune",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
